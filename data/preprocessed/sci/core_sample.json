{
    "title": [
        "Free-ordered CUG on Chemical Abstract Machine",
        "Free-ordered CUG on Chemical Abstract Machine",
        "Free-ordered CUG on Chemical Abstract Machine",
        "Free-ordered CUG on Chemical Abstract Machine",
        "Free-ordered CUG on Chemical Abstract Machine",
        "Free-ordered CUG on Chemical Abstract Machine",
        "A Centering Approach to Pronouns",
        "A Centering Approach to Pronouns",
        "A Centering Approach to Pronouns",
        "A Centering Approach to Pronouns",
        "A Centering Approach to Pronouns",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Dual-Coding Theory and Connectionist Lexical Selection",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "Implementation and evaluation of a German HMM for POS disambiguation",
        "The Semantics of Motion",
        "The Semantics of Motion",
        "The Semantics of Motion",
        "The Semantics of Motion",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Rapid Development of Morphological Descriptions for Full Language Processing Systems",
        "Temporal Relations : Reference or Discourse Coherence ?",
        "Temporal Relations : Reference or Discourse Coherence ?",
        "Temporal Relations : Reference or Discourse Coherence ?",
        "Temporal Relations : Reference or Discourse Coherence ?",
        "An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process",
        "An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process",
        "An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "A State-Transition Grammar for Data-Oriented Parsing",
        "Typed Feature Structures as Descriptions",
        "Typed Feature Structures as Descriptions",
        "Typed Feature Structures as Descriptions",
        "Typed Feature Structures as Descriptions",
        "Typed Feature Structures as Descriptions",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Semantics of Complex Sentences in Japanese",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "Off-line Optimization for Earley-style HPSG Processing",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "An Integrated Heuristic Scheme for Partial Parse Evaluation",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Algorithms for Analysing the Temporal Structure of Discourse",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Distributional Clustering of English Words",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Incorporating `` Unconscious Reanalysis '' into an Incremental , Monotonic Parser",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Evaluating Discourse Processing Algorithms",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Stochastic HPSG",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Analysis of Japanese Compound Nouns using Collocational Information",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Splitting the Reference Time : Temporal Anaphora and Quantification in DRT",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Common Topics and Coherent Situations : Interpreting Ellipsis in the Context of Discourse Inference",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Planning Argumentative Texts",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Improving Language Models by Clustering Training Sentences",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Qualitative and Quantitative Models of Speech Translation",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Parsing with Principles and Probabilities",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Bottom-Up Earley Deduction",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Creating a tagset , lexicon and guesser for a French tagger",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Grammar Specialization through Entropy Thresholds",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "Multilingual Sentence Categorization according to Language",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "On Learning More Appropriate Selectional Restrictions",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "A Robust Parser Based on Syntactic Information",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Automated Tone Transcription",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Syntactic-Head-Driven Generation",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "Ellipsis and Quantification : A Substitutional Approach",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "FOCUS ON \" ONLY \" AND \" NOT \"",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Cooperative Error Handling and Shallow Processing",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "Segmenting Speech without a Lexicon : the Roles of Phonotactics and Speech Source",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "LHIP : Extended DCGs for Configurable Robust Parsing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Discourse Obligations in Dialogue Processing",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "Abstract Generation Based on Rhetorical Structure Extraction",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "The Semantics of Resource Sharing in Lexical-Functional Grammar",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "A Tractable Extension of Linear Indexed Grammars",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Does Baum-Welch Re-estimation Help Taggers ?",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Distributional Part-of-Speech Tagging",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Lexical Functions and Machine Translation",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Collaboration on Reference to Objects that are not Mutually Known",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "Default Handling in Incremental Generation",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "A fast partial parse of natural language sentences using a connectionist method",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Corpus Statistics Meet the Noun Compound : Some Empirical Results",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Magic for Filter Optimization in Dynamic Bottom-up Processing",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Cues and control in Expert-Client Dialogues",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "Bayes -ian Grammar Induction for Language Modeling",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Incremental Interpretation of Categorial Grammar",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Response Generation in Collaborative Negotiation",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Redundancy in Collaborative Dialogue",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Countability and Number in Japanese-to-English Machine Translation",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Learning Dependencies between Case Frame Slots",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Clustering Words with the MDL Principle",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Unsupervised Learning of Word-Category Guessing Rules",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Similarity between Words Computed by Spreading Activation on an  English Dictionary",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "Tagset Design and Inflected Languages",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "The intersection of Finite State Automata and Definite Clause Grammars",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Statistical Decision-Tree Models for Parsing",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Discourse and Deliberation : Testing a Collaborative Strategy",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Parsing for Semidirectional Lambek Grammar is NP-Complete",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Non-Constituent Coordination : Theory and Practice",
        "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "An Efficient Generation Algorithm for Lexicalist MT",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "Disambiguating Noun Groupings with Respect to WordNet Senses",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "A Morphographemic Model for Error Correction in Nonconcatenative Strings",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities",
        "Similarity-Based Estimation of Word Cooccurrence Probabilities"
    ],
    "abstract": [
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " We propose a paradigm for concurrent natural language generation .  In order to represent grammar rules distributively , we adopt categorial unification grammar ( CUG ) where each category owns its functional type .  We augment typed lambda calculus with several new combinators , to make the order of  - conversions free for partial / local processing .  The concurrent calculus is modeled with Chemical Abstract Machine .  We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination . ",
        " In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .  As described in Grosz et al. 1986 , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting .  We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns .  The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application . ",
        " In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .  As described in Grosz et al. 1986 , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting .  We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns .  The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application . ",
        " In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .  As described in Grosz et al. 1986 , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting .  We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns .  The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application . ",
        " In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .  As described in Grosz et al. 1986 , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting .  We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns .  The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application . ",
        " In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .  As described in Grosz et al. 1986 , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting .  We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns .  The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " We introduce the bilingual dual-coding theory as a model for bilingual mental representation .  Based on this model , lexical selection neural networks are implemented for a connectionist transfer project in machine translation . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " A German language model for the Xerox HMM tagger is presented .  This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .  The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .  Finally , the model 's error types are described .  I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German . ",
        " In this paper we present a semantic study of motion complexes ( ie. of a motion verb followed by a spatial preposition ) .  We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs , on the one hand , and of the spatial prepositions , on the other hand .  Then we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components . ",
        " In this paper we present a semantic study of motion complexes ( ie. of a motion verb followed by a spatial preposition ) .  We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs , on the one hand , and of the spatial prepositions , on the other hand .  Then we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components . ",
        " In this paper we present a semantic study of motion complexes ( ie. of a motion verb followed by a spatial preposition ) .  We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs , on the one hand , and of the spatial prepositions , on the other hand .  Then we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components . ",
        " In this paper we present a semantic study of motion complexes ( ie. of a motion verb followed by a spatial preposition ) .  We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs , on the one hand , and of the spatial prepositions , on the other hand .  Then we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " I describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system .  The compiler is optimized for a class of languages including many or most European ones , and for rapid development and debugging of descriptions of new languages .  The key design decision is to compose morphophonological and morphosyntactic information , but not the lexicon , when compiling the description .  This results in typical compilation times of about a minute , and has allowed a reasonably full , feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system . ",
        " The temporal relations that hold between events described by successive utterances are often left implicit or underspecified .  We address the role of two phenomena with respect to the recovery of these relations :  the referential properties of tense , and  the role of temporal constraints imposed by coherence relations .  We account for several facets of the identification of temporal relations through an integration of these . ",
        " The temporal relations that hold between events described by successive utterances are often left implicit or underspecified .  We address the role of two phenomena with respect to the recovery of these relations :  the referential properties of tense , and  the role of temporal constraints imposed by coherence relations .  We account for several facets of the identification of temporal relations through an integration of these . ",
        " The temporal relations that hold between events described by successive utterances are often left implicit or underspecified .  We address the role of two phenomena with respect to the recovery of these relations :  the referential properties of tense , and  the role of temporal constraints imposed by coherence relations .  We account for several facets of the identification of temporal relations through an integration of these . ",
        " The temporal relations that hold between events described by successive utterances are often left implicit or underspecified .  We address the role of two phenomena with respect to the recovery of these relations :  the referential properties of tense , and  the role of temporal constraints imposed by coherence relations .  We account for several facets of the identification of temporal relations through an integration of these . ",
        " Both anaphora resolution and prepositional phrase ( PP ) attachment are the most frequent ambiguities in natural language processing .  Several methods have been proposed to deal with each phenomenon separately , however none of proposed systems has considered the way of dealing both phenomena .  We tackle this issue here , proposing an algorithm to co-ordinate the treatment of these two problems efficiently , i.e. , the aim is also to exploit at each step all the results that each component can provide . ",
        " Both anaphora resolution and prepositional phrase ( PP ) attachment are the most frequent ambiguities in natural language processing .  Several methods have been proposed to deal with each phenomenon separately , however none of proposed systems has considered the way of dealing both phenomena .  We tackle this issue here , proposing an algorithm to co-ordinate the treatment of these two problems efficiently , i.e. , the aim is also to exploit at each step all the results that each component can provide . ",
        " Both anaphora resolution and prepositional phrase ( PP ) attachment are the most frequent ambiguities in natural language processing .  Several methods have been proposed to deal with each phenomenon separately , however none of proposed systems has considered the way of dealing both phenomena .  We tackle this issue here , proposing an algorithm to co-ordinate the treatment of these two problems efficiently , i.e. , the aim is also to exploit at each step all the results that each component can provide . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus .  Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream , and combine them into a small number of functions , with the parameters weighted on basis of how useful they are for discriminating text genres .  An application to information retrieval is discussed . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .  It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts . ",
        " A description is an entity that can be interpreted as true or false of an object , and using feature structures as descriptions accrues several computational benefits .  In this paper , I create an explicit interpretation of a typed feature structure used as a description , define the notion of a satisfiable feature structure , and create a simple and effective algorithm to decide if a feature structure is satisfiable . ",
        " A description is an entity that can be interpreted as true or false of an object , and using feature structures as descriptions accrues several computational benefits .  In this paper , I create an explicit interpretation of a typed feature structure used as a description , define the notion of a satisfiable feature structure , and create a simple and effective algorithm to decide if a feature structure is satisfiable . ",
        " A description is an entity that can be interpreted as true or false of an object , and using feature structures as descriptions accrues several computational benefits .  In this paper , I create an explicit interpretation of a typed feature structure used as a description , define the notion of a satisfiable feature structure , and create a simple and effective algorithm to decide if a feature structure is satisfiable . ",
        " A description is an entity that can be interpreted as true or false of an object , and using feature structures as descriptions accrues several computational benefits .  In this paper , I create an explicit interpretation of a typed feature structure used as a description , define the notion of a satisfiable feature structure , and create a simple and effective algorithm to decide if a feature structure is satisfiable . ",
        " A description is an entity that can be interpreted as true or false of an object , and using feature structures as descriptions accrues several computational benefits .  In this paper , I create an explicit interpretation of a typed feature structure used as a description , define the notion of a satisfiable feature structure , and create a simple and effective algorithm to decide if a feature structure is satisfiable . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively .  However if there can be relations between every pair of semantic roles , the amount of computation to identify the relations that hold in the given sentence is extremely large .  In this paper , for semantics of Japanese complex sentence , we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses .  By these new roles constraints on the relations among semantic / pragmatic roles are known to be almost local within subordinate or main clause .  In other words , as for the semantics of the whole complex sentence , the only role we should deal with is a motivated . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " A novel approach to HPSG based natural language processing is described that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced Earley-style processor .  This way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of HPSG generation .  Extensive testing with a large HPSG grammar revealed some important constraints on the form of the grammar . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " GLR* is a recently developed robust version of the Generalized LR Parser Tomita 1986 , that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .  On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .  This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .  We describe the heuristic measures used and their combination scheme .  Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense , aspect , temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure .  It is part of a discourse grammar implemented in Carpenter 's ALE formalism .  The method for building up the temporal structure of the discourse combines constraints and preferences : we use constraints to reduce the number of possible structures , exploiting the HPSG type hierarchy and unification for this purpose ; and we apply preferences to choose between the remaining options using a temporal centering mechanism .  We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal / rhetorical structure until higher-level information can be used to disambiguate . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .  Deterministic annealing is used to find lowest distortion sets of clusters .  As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .  Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " This paper describes the author 's implementation of a parser aimed at reproducing , in a computationally explicit system , the constraints of a particular psycholinguistic model Gorrell in press .  In Gorrell 's model , `` unconscious '' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation , but there is no discussion as to how the parser decides which relations to add .  We model this decision as a search for a node in the tree at which an explicitly defined parsing operation , tree-lowering may be applied .  With reference to English and Japanese processing data , we show the importance of this search for empirical adequacy of the psycholinguistic model . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .  We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues .  We present the quantitative results of hand-simulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .  We illustrate the general difficulties encountered with quantitative evaluation .  These are problems with :  allowing for underlying assumptions ,  determining how to handle underspecifications , and  evaluating the contribution of false positives and error chaining . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag .  We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .  We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains .  In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus .  An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters .  The accuracy of this method is about 80 % . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .  The analysis in Partee 1984 of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .  This problem has been previously analyzed in de Swart 1991 as an instance of the proportion problem , and given a solution from a Generalized Quantifier approach .  By using a careful distinction between the different notions of reference time , based on Kamp and Reyle 1993 , we propose a solution to this problem , within the framework of DRT .  We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " It is claimed that a variety of facts concerning ellipsis , event reference , and interclausal coherence can be explained by two features of the linguistic form in question :  whether the form leaves behind an empty constituent in the syntax , and  whether the form is anaphoric in the semantics .  It is proposed that these features interact with one of two types of discourse inference , namely Common Topic inference and Coherent Situation inference .  The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " This paper presents PROVERB a text planner for argumentative texts .  PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way .  The former splits the task of presenting a particular proof into subtasks of presenting subproofs .  The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms , using a wide-coverage grammar .  The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .  I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .  This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .  It also offers a reasonably automatic means to gather evidence on whether a more complex , context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .  As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .  These results are consistent with other findings for such models , suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper compares a qualitative reasoning model of translation with a quantitative statistical model .  We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .  The quantitative language and translation models are based on relations between lexical heads of phrases .  Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " This paper is an attempt to bring together two approaches to language analysis .  The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise .  Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We propose a bottom-up variant of Earley deduction .  Bottom-up deduction is preferable to top-down deduction because it allows incremental processing ( even for head-driven grammars ) , it is data-driven , no subsumption check is needed , and preference values attached to lexical items can be used to guide best-first search .  We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " We earlier described two taggers for French , a statistical one and a constraint-based one .  The two taggers have the same tokeniser and morphological analyser .  In this paper , we describe aspects of this work concerned with the definition of the tagset , the building of the lexicon , derived from an existing two-level morphological analyser , and the definition of a lexical transducer for guessing unknown words . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees .  This allows very much faster parsing and gives a lower error rate , at the price of a small loss in coverage .  Previously , it has been necessary to specify the tree-cutting criteria ( or operationality criteria ) manually ; here they are derived automatically from the training set and the desired coverage of the specialized grammar .  This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " Issues in sentence categorization according to language is fundamental for NLP , especially in document processing .  In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser .  The major difficulties in sentence categorization are convergence and textual errors .  Convergence since dealing with short entries involve discarding languages from few clues .  Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR .  We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency .  The implementation is fast , small , robust and textual errors tolerant .  Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences .  The resolution power is based on grammatical words ( not the most common words ) and alphabet .  Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected .  The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained .  We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance .  Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .  It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .  Evaluation measures for the Selectional Restrictions learning task are discussed .  Finally , an experimental evaluation of these variations is reported . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " An extragrammatical sentence is what a normal parser fails to analyze .  It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .  A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .  We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .  Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .  To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .  The experimental result shows 68 % - 77 % accuracy in error recovery . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " In this paper I report on an investigation into the problem of assigning tones to pitch contours .  The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .  Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .  Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .  Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .  This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .  Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .  These programs can be adapted to other tone languages by adjusting the F  prediction function . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990 .  This is the case for the semantic analysis rules of certain constraint-based semantic representations , e.g. Underspecified Discourse Representation Structures ( UDRSs ) Frank and Reyle 1992 .  Since head-driven generation in general has its merits , we simply return to a syntactic definition of ` head ' and demonstrate the feasibility of syntactic-head-driven generation .  In addition to its generality , a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of ( syntactic ) heads , for which only ad-hoc solutions existed , so far . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " The paper describes a substitutional approach to ellipsis resolution giving comparable results to Dalrymple et al. 1991 , but without the need for order-sensitive interleaving of quantifier scoping and ellipsis resolution .  It is argued that the order-independence results from viewing semantic interpretation as building a description of a semantic composition , instead of the more common view of interpretation as actually performing the composition . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " Krifka 1993 has suggested that focus should be seen as a means of providing material for a range of semantic and pragmatic functions to work on , rather than as a specific semantic or pragmatic function itself .  The current paper describes an implementation of this general idea , and applies it to the interpretation of only and not . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora .  The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items .  Some experimental results about the performance of the method are provided . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " This paper is concerned with the detection and correction of sub-sentential English text errors .  Previous spelling programs , unless restricted to a very small set of words , have operated as post-processors .  And to date , grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse , assuming a complete sentence .  Work described below is aimed at evaluating the effectiveness of shallow ( sub-sentential ) processing and the feasibility of cooperative error checking , through building and testing appropriately an error-processing system .  A system under construction is outlined which incorporates morphological checks ( using new two-level error rules ) over a directed letter graph , tag positional trigrams and partial parsing .  Intended testing is discussed . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .  Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .  Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .  The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .  The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .  Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We present LHIP , a system for incremental grammar development using an extended DCG formalism .  The system uses a robust island-based parsing method controlled by user-defined performance thresholds . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .  In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction .  The system first extracts the rhetorical structure , the compound of the rhetorical relations between sentences , and then cuts out less important parts in the extracted structure to generate an abstract of the desired length .  Evaluation of the generated abstract showed that it contains at maximum 74 % of the most important sentences of the original text .  The system is now utilized as a text browser for a prototypical interactive document retrieval system . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " We argue that the resource sharing that is commonly manifest in semantic accounts of coordination is instead appropriately handled in terms of structure-sharing in LFG f-structures .  We provide an extension to the previous account of LFG semantics Dalrymple et al. 1993b according to which dependencies between f-structures are viewed as resources ; as a result a one-to-one correspondence between uses of f-structures and meanings is maintained .  The resulting system is sufficiently restricted in cases where other approaches overgenerate ; the very property of resource-sensitivity for which resource sharing appears to be problematic actually provides explanatory advantages over systems that more freely replicate resources during derivation . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " Vijay-Shanker and Weir 1993 show that Linear Indexed Grammars ( LIG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing .  This paper describes a formalism that is more powerful than LIG , but which can also be processed in polynomial time using similar techniques .  The formalism , which we refer to as Partially Linear PATR ( PLPATR ) manipulates feature structures rather than stacks . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " In part of speech tagging by Hidden Markov Model , a statistical model is used to assign grammatical categories to words in a text .  Early work in the field relied on a corpus which had been tagged by a human annotator to train the model .  More recently , Cutting et al. 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities , by using Baum-Welch re-estimation to automatically refine the model .  In this paper , I report two experiments designed to determine how much manual training information is needed .  The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy .  The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation .  In two of the patterns , the re-estimation ultimately reduces the accuracy of the tagging rather than improving it .  The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus ( if any ) and the corpus to be tagged .  Heuristics for deciding how to use re-estimation in an effective manner are given .  The conclusions are broadly in agreement with those of Merialdo 1994 , but give greater detail about the contributions of different parts of the model . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper presents an algorithm for tagging words whose part-of-speech properties are unknown .  Unlike previous work , the algorithm categorizes word tokens in context instead of word types .  The algorithm is evaluated on the Brown Corpus . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " This paper discusses the lexicographical concept of lexical functions Mel'cuk and Zolkovsky 1984  and their potential exploitation in the development of a machine translation lexicon designed to handle collocations .  We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents , and therefore suggest themselves as some kind of language-independent semantic primitives from which translation strategies can be developed . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " In conversation , a person sometimes has to refer to an object that is not previously known to the other participant .  We present a plan-based model of how agents collaborate on reference of this sort .  In making a reference , an agent uses the most salient attributes of the referent .  In understanding a reference , an agent determines his confidence in its adequacy as a means of identifying the referent .  To collaborate , the agents use judgment , suggestion , and elaboration moves to refashion an inadequate referring expression . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " Natural language generation must work with insufficient input .  Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input .  The paper aims to escape from such dead-end situations by making assumptions .  We discuss global aspects of default handling .  Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .  This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .  It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .  The function of the network is briefly explained , and results are given . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " A variety of statistical methods for noun compound analysis are implemented and compared .  The results support two main conclusions .  First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy .  Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .  The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .  Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We conducted an empirical analysis into the relation between control and discourse structure .  We applied control criteria to four dialogues and identified 3 levels of discourse structure .  We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .  Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " We describe a corpus-based induction algorithm for probabilistic context-free grammars .  The algorithm employs a greedy heuristic search within a Bayes -ian framework , and a post-pass using the Inside-Outside algorithm .  We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .  In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .  The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics .  This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences .  The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances .  The algorithm applies equally to simple and complex utterances and sequences of utterances . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .  The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .  The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .  It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process .  In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs .  This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution .  Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification .  Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker .  A speaker cannot simply assume that a proposal or an assertion will be accepted .  However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection .  Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution .  In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance .  The model  requires a theory of mutual belief that supports mutual beliefs of various strengths ;  explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and  contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases .  Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data .  In particular , we propose a method of learning dependencies between case frame slots .  We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots .  We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables .  Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice .  To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data .  In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task .  Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .  We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .  We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .  We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .  Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " Words unknown to the lexicon present a substantial problem to part-of-speech tagging .  In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .  Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .  The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions .  The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) .  However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .  The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .  Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .  The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " Why should computers interpret language incrementally ?  In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .  However , possible computational applications have received less attention .  In this paper we consider various potential applications , in particular graphical interaction and dialogue .  We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .  Finally , we tease apart the relationship between dynamic semantics and incremental interpretation . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .  In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .  Some problems associated with tagging unknown words in inflected languages are briefly considered . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Bernard Lang defines parsing as the calculation of the intersection of a FSA ( the input ) and a CFG .  Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) .  Furthermore , certain techniques for robust parsing can be modelled as finite state transducers .  In this paper we investigate how we can generalize this approach for unification grammars .  In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG .  It is shown that existing parsing algorithms can be easily extended for FSA inputs .  However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) .  Furthermore we discuss approaches to cope with the problem . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .  In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .  This work is based on the following premises :  grammars are too complex and detailed to develop manually for most interesting domains ;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and  existing n-gram modeling techniques are inadequate for parsing models .  In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .  Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " A discourse strategy is a strategy for communicating with another agent .  Designing effective dialogue systems requires designing agents that can choose among discourse strategies .  We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation .  The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional .  In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .  SDL grammars are able to generate each context-free language and more than that .  We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs .  This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing .  Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars . ",
        " Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence .  These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification .  However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation .  This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs . ",
        " Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence .  These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification .  However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation .  This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs . ",
        " Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence .  These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification .  However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation .  This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs . ",
        " Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence .  These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification .  However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation .  This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs . ",
        " Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence .  These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification .  However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation .  This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions .  However , the Shake-and-Bake generation algorithm of Whitelock 1992 is NP-complete .  We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve .  However , for many tasks , one is interested in relationships among word senses , not words .  This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms .  Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels .  The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism .  Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples .  The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to Damerau errors .  A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another .  We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns .  The application of the control rules lets us derive domain-independent discourse structures .  The derived structures indicate that initiative plays a role in the structuring of discourse .  In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets .  This distribution indicates that some control segments are hierarchically related to others .  The analysis suggests that discourse participants often mutually agree to a change of topic .  We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types .  These differences can be explained in terms of collaborative planning principles . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . ",
        " In many applications of natural language processing it is necessary to determine the likelihood of a given word combination .  For example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely .  Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus .  However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus .  In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words .  We describe a probabilistic word association model based on distributional word similarity , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz 's back-off model .  The similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . "
    ],
    "text": [
        "parallel distributed computation expected main stream information processing conventional generation rule composition given outside rule control behavior symbol object assembling hierarchical tree structure example linguistic object word phrase applied called grammar rule form grammatical structure rational semantic representation strict controller process kind formalization contradicts partial distributed processing required parallel architecture future order represent grammar rule adopt categorial grammar attach local grammar rule word phrase aim paper propose paradigm enables partial local generation decomposition reorganization tentative local structure following section introduce extended calculus introduce cham model reinterpret model term natural language processing model membrane interaction model example japanese causative sentence requires drastic change domination case discus future model",
        "cug categorial unification grammar uszkoreit advantageous compared phrase structure grammar parallel architecture regard category functional type represent grammar rule mean need given grammar rule rule reside word phrase section regard category polymorphic type consider type calculus section denote category dag directed acyclic graph patr grammar shieber use greek letter type schema type constant use type variable use object type type type purpose type inference infer type object set object type known presuppose type variable unified unifier use set type known object important rule follows rule corresponds conversion ordinary calculus hindley seldin subsection introduce combinators enable change order conversion proposed steedman kind type change dowty ordinary calculus requires strict order conversion concurrent model kind strict order hindrance contingent conversion required combinator change order variable follows requirement exchange order conversion following case required compose following typed object case need concatenate applicable help following b combinator variable shifted scope concatenate applicable fig repeated use b combinators problematic consider implementing actual system termination processing guaranteed modeled process partial decomposition abstraction argument order term abstraction occurs process fall loop order avoid assume unification cost compound term subtree decomposed element longer distance abstracted regard sentence structure grammatical sum unification cost smaller introduce heuristic cost tojo considering parallelism syntactic case semantic role follows represents unifier dag syntactic case semantic role constant larger",
        "subsection introduce combinators enable change order conversion proposed steedman kind type change dowty ordinary calculus requires strict order conversion concurrent model kind strict order hindrance contingent conversion required combinator change order variable follows requirement exchange order conversion following case required compose following typed object case need concatenate applicable help following b combinator variable shifted scope concatenate applicable fig",
        "repeated use b combinators problematic consider implementing actual system termination processing guaranteed modeled process partial decomposition abstraction argument order term abstraction occurs process fall loop order avoid assume unification cost compound term subtree decomposed element longer distance abstracted regard sentence structure grammatical sum unification cost smaller introduce heuristic cost tojo considering parallelism syntactic case semantic role follows represents unifier dag syntactic case semantic role constant larger",
        "chemical machine cham short berry boudol paradigm concurrent calculus paper mention principle natural language processing regard cham model assume process natural language recognition follows linguistic object recognized thrown solution cham act molecule auxiliary verb introduces membrane membrane scope case role domination verb search molecule noun phrase necessary satisfy verb case role frame membrane occasion multiple verb exist sentence conflict verb dominates noun phrase case membrane interact exchange molecule use membrane membrane contains molecule denote supporting relation interpreted inclusion relation case membrane interact contact notation floating molecule concatenated molecule porous membrane concatenation molecule represented typed lambda variable membrane contains composite structure surplus valence regard membrane surplus valence follows apply notion actual problem sentence generation",
        "japanese language causative change voice realized agglutination auxiliary verb tail current verb auxiliary verb ordinary verb dominate case agglutination change syntax gunji scope operation auxiliary verb operated verb sentence order illustrate role change alternation agent main verb table short tip japanese lexicon example sentence dag lexical item fig dag fig represents verb yomu read requires role reader object read optional role counter agent hears reader read figure mean word recognized general world verb yomu introduced special membrane subworld dag mean polymorphic type lexical item parser construct partial tree structure recognizing word head word recognized form complete sentence noun concatenated read sentential representation subworld record unification contain cost original type necessary backtracked meaning binding transitive let recapitulate occurred membrane lexical item set organized sentence singleton problematic final word aseru causative arrives dag representation fig dag fig requires sentential form category argument addition subcategorizes item category agent subsentence process fig process fig b combinators ordinary type inference second membrane requires agent role variable record bit agent comparison unifies nominative case agent role cost equivalent case linguistic heuristic solve problem case agent nominative sentence co agent dative sentence arrived remains bound read variable type process depicted fig",
        "approach discourse structure developed sidner grosz discourse exhibit global local coherence view key element local coherence centering system rule constraint govern relationship discourse linguistic choice discourse participant choice grammatical function syntactic structure type referring expression proper noun definite indefinite description reflexive personal pronoun etc pronominalization particular serf focus attention talked inappropriate use failure use pronoun cause communication fluent instance take hearer process pronominalized noun phrase focus take process non pronominalized noun phrase focus guindon grosz centering model based following assumption discourse segment consists sequence utterance utterance associated list looking center consisting discourse entity realized realized linguistic expression utterance entity list corresponds likelihood primary focus subsequent discourse entity list preferred center center entity time looking center backward center confirmation entity introduced discourse realized preceding utterance distinct type transition utterance typology transition based factor center attention entity coincides preferred center transition type appear figure transition describe utterance linked coherent local segment discourse speaker number proposition express simple way express proposition given entity continuing introducing related entity retaining shifting center new entity figure way signal intention shift claim speaker behave orderly fashion algorithm expects kind behavior successful depend recency parallelism grammatical function interaction centering global focusing mechanism factor intentional structure semantic selectional restriction verb tense aspect modality intonation pitch accent topic research transition specific focus movement described sidner extension propose make specific grosz corresponds sidner discourse focus potential focus formal system constraint rule centering interpreted grosz follows constraintsthere element realized ranked element realized element realized pronoun preferred retaining preferred shifting evident constraint ranking item forward center list crucial rank item obliqueness grammatical relation subcategorized function main verb subject object object followed subcategorized function adjunct capture idea grosz subjecthood contributes priority item list aware ranking coincides surface constituent order english interest examine data language",
        "freer constituent order german determine influence constituent order centering grammatical function held constant addition language provide identifiable topic function japanese suggest topic take precedence subject hpsg system us centering algorithm pronoun binding called pragmatic processor interacts module called semantics processor computes representation intrasentential anaphoric relation thing semantics processor access information surface syntactic structure utterance provides pragmatic processor representation include set reference marker reference marker contraindexed expression co specify marker carry information agreement grammatical function pronominal reference marker unique index displayed figure form pollard semantic representation co specifier non pronominal reference marker surface string index indefinites generated",
        "constraint proposed grosz fail certain example like following read pronoun destressed example characterized multiple ambiguous pronoun fact final utterance achieves shift figure shift inevitable constraint state equal realized subject friedman constraint rule grosz fail choice co specification possibility pronoun transition shift coherent way shift item examined order characterize transition pair anchor grosz shift occurs successive definition shifting consider equal status important case determining retaining chontinuing distinction propose following extension handle additional case containing multiple ambiguous pronoun extended rule kind shift transition ranked state shifting represents coherent way shift preferred ranking shifting figure extension enables bind final utterance example figure friedman appendix illustrates application algorithm figure proposed extension grosz theory property sharing constraint attempt enforce parallellism entity successive utterance considers property subj ident extension subject pronoun prefer subject antecedent non subject pronoun prefer non subject antecedent structural parallelism consequence ordering list grammatical function preference continuing retaining constraint suggested grosz succeed case invoking independent structural parallelism constraint distinction continuing retaining kameyama fails consider example reproduce figure accounted continuing retaining distinction utterance example interpretation consistent centering rule constraint rule interpretation figure preferred figure",
        "goal current algorithm design conceptual clarity efficiency hope structure provided allow easy addition constraint preference simple change control structure algorithm proposed continuing retaining anchor shifting one avoiding precomputation possible anchor state realization contribute entity true case specified semantic description consistent interpretation need enumerate possible interpretation constructing possible associated semantic theory allows specified interpretation hold entity realized utterance view referring house reference door gotten inference representation house proposed anchor constructed possibility having infinite number potential utterance finite length question preference ordering transition constraint example contains single pronoun retention informant preference shifting centering algorithm chooses continuation figure informant strong preference co specification unstressed avoid ambiguity stressing pronoun respect phonological environment computational system understanding need acknowledge ambiguity computational system generation try plan retention signal impending shift retention shift preferred continuation course local approach described provide necessary information interpreting pronoun constraint imposed world knowledge pragmatic semantics phonology interesting question concerning centering algorithm centering algorithm interact inferencing mechanism choice proposed anchor ranking database query system answer incorporated discourse model centering interact treatment definite indefinite quantifier exploring idea extension centering approach modeling reference local discourse",
        "goal current algorithm design conceptual clarity efficiency hope structure provided allow easy addition constraint preference simple change control structure algorithm proposed continuing retaining anchor shifting one avoiding precomputation possible anchor state realization contribute entity true case specified semantic description consistent interpretation need enumerate possible interpretation constructing possible associated semantic theory allows specified interpretation hold entity realized utterance view referring house reference door gotten inference representation house proposed anchor constructed possibility having infinite number potential utterance finite length question preference ordering transition constraint example contains single pronoun retention informant preference shifting centering algorithm chooses continuation figure informant strong preference co specification unstressed avoid ambiguity stressing pronoun respect phonological environment computational system understanding need acknowledge ambiguity computational system generation try plan retention signal impending shift retention shift preferred continuation",
        "psycholinguistic knowledge helpful believe constructing artificial language processing system machine translation advantage understanding ofhow language represented human mind representation mapped language representation mapping acquired human bilingual coding theory paivio answer question depicts verbal representation different language separate connected logogen system characterizes translation process activation connection logogen system attribute acquisition representation unspecified statistical process explored information theoretical neural network gorin levinson acquire verbal association coding theory provides learnable lexical selection sub system connectionist transfer project machine translation",
        "known debate psycholinguistics concerning bilingual mental representation independence position assumes bilingual memory represented independent storage retrieval system interdependence position hypothesizes information language exists common memory store cross language transfer cross language priming provided evidence hypothesis groot na lambert coding theory explains coexistence independent interdependent phenomenon separate connected structure general coding theory hypothesizes human represents language dual system verbal system imagery system element verbal system logogens word language element imagery system called imagens connected logogens verbal system referential connection verbal system interconnected associative connection bilingual coding theory proposes architecture common imagery system connected verbal system verbal system interconnected associative connection figure language association rich diverse language association involve translation equivalent term experienced interconnection system explain interdependent functional behavior hand different characteristic language language association account independent functional behavior structural assumption coding theory proposes parallel set processing assumption connection related imagens logogens called referential processing object imaging word prototypical example associative connection logogens called associative processing translation example associative processing language",
        "lexical selection task choosing target language word reflect meaning corresponding source language word play important role machine translation pustejovsky nirenburg common lexical selection practice involves intermediate representation disambiguates source language word entity intermediate representation map entity target lexical entry intermediate representation lexical concept structure dorr interlingua nirenberg engineering approach requires great effort designing representation mapping rule effort statistical lexical selection target language word selected posterior probability given source language word target language lexical entry selected single source language word correct selection identified language model target language brown approach learnable accuracy low reason use structural information language subsection propose information theoretical network based bilingual coding theory lexical selection theoretical network neural network formalism capable association layer representation association obtained according network experience information theoretical network layer unit layer represents element input output training pattern logogen word different layer connected weight connection unit layer unit layer assigned mutual information element represented unitseach layer contains bias unit activated weight connection bias unit layer unit layer isboth information theoretical network propagation network compute posterior probability association task gorin levinson robinson information theoretical network isomorphic interconnected verbal system coding theory information theoretical network following advantage learns network learn single pas gradient decent adaptive adapt new experience adding new data training sample modifying association according changed statistic network plausible tried map source language f structure target language f structure connectionist transfer project wang waibel sub task finding target sub structure phrasal category corresponding source structure finding head target structure second sub task problem lexical selection implemented propagation network replaced propagation network lexical selection information theoretical network simulating associative process coding theory network layer unit source target language lexical item represented unit input output layer network constructed phrasal category network work following way target language f structure generated transfer system know phrasal category corresponding source language f structure network perform sub task activates lexical selection network phrasal category input unit correspond head source language f structure sub structure connection layer output unit activated lexical item corresponds active output unit selected",
        "head target f structure following example illustrates system selects head anmelden german xcomp sub structure transfer fromtosince structure network find sub structure xcomp target structure corresponding input structure eqn activates lexical selection network input unit register conference propagating activation associative connection unit anmelden active output anmelden chosen head xcomp sub structure domain work conference registration telephony conversation lexicon task contained english german word english german f structure pair available research task osterholtz separate set sentential f structure test generalization performance system testing data collected independent task jain sentential f structure pair german sub structure extracted labeled english counterpart english counterpart head immediate sub structure head serve input sample association german f structure head output association example association sample drawn f structure network training sample network created way accuracy system information theoretical network lexical selection lower propagation network training data generalization performance unseen input information theoretical network learn propagation network reduced number free parameter information theoretical network",
        "lexical selection task choosing target language word reflect meaning corresponding source language word play important role machine translation pustejovsky nirenburg common lexical selection practice involves intermediate representation disambiguates source language word entity intermediate representation map entity target lexical entry intermediate representation lexical concept structure dorr interlingua nirenberg engineering approach requires great effort designing representation mapping rule effort statistical lexical selection target language word selected posterior probability given source language word target language lexical entry selected single source language word correct selection identified language model target language brown approach learnable accuracy low reason use structural information language subsection propose information theoretical network based bilingual coding theory lexical selection",
        "information theoretical network neural network formalism capable association layer representation association obtained according network experience information theoretical network layer unit layer represents element input output training pattern logogen word different layer connected weight connection unit layer unit layer assigned mutual information element represented unitseach layer contains bias unit activated weight connection bias unit layer unit layer isboth information theoretical network propagation network compute posterior probability association task gorin levinson robinson information theoretical network isomorphic interconnected verbal system coding theory information theoretical network following advantage learns network learn single pas gradient decent adaptive adapt new experience adding new data training sample modifying association according changed statistic network plausible",
        "tried map source language f structure target language f structure connectionist transfer project wang waibel sub task finding target sub structure phrasal category corresponding source structure finding head target structure second sub task problem lexical selection implemented propagation network replaced propagation network lexical selection information theoretical network simulating associative process coding theory network layer unit source target language lexical item represented unit input output layer network constructed phrasal category network work following way target language f structure generated transfer system know phrasal category corresponding source language f structure network perform sub task activates lexical selection network phrasal category input unit correspond head source language f structure sub structure connection layer output unit activated lexical item corresponds active output unit selected head target f structure following example illustrates system selects head anmelden german xcomp sub structure transfer fromtosince structure network find sub structure xcomp target structure corresponding input structure eqn activates lexical selection network input unit register conference propagating activation associative connection unit anmelden active output anmelden chosen head xcomp sub structure",
        "domain work conference registration telephony conversation lexicon task contained english german word english german f structure pair available research task osterholtz separate set sentential f structure test generalization performance system testing data collected independent task jain sentential f structure pair german sub structure extracted labeled english counterpart english counterpart head immediate sub structure head serve input sample association german f structure head output association example association sample drawn f structure network training sample network created way accuracy system information theoretical network lexical selection lower propagation network training data generalization performance unseen input information theoretical network learn propagation network reduced number free parameter information theoretical network",
        "late speech po disambiguation hidden markov model hmm widespread method tagging text fact little work employing technology disambiguation german text wothke schmid kempe work author feldweg feldweg framework project corpus based development lexical knowledge base elwis produced straightforward implementation viterbi algorithm employing hmm parameter obtained pre tagged text corpus original tag set redefined making tagged corpus train likely tagger obsolete current project adapting bilingual dictionary online comprehension assistance compass lre need arose po disambiguator facilitate context sensitive dictionary look system compass project make ample use xerox technology core look engine po disambiguation language german obvious thing develop german language model xerox tagger following section describes implementation new model section result obtained new model compared result previous model analysis type disambiguation involved model presented section model error type analysed section conclusion drawn section",
        "version xerox tagger implementation described dd tagger version kupiec wilkens version differs current version xerox tagger described cutting dd tagger accommodates lexicon class guesser form external finite state transducer new language model tagger involves supplying definition tag set hmm lexicon listing word form equivalence class list po tag assigned word form class guesser assigns equivalence class word covered lexicon set initial transition bias set initial symbol bias large text training hmm reference text assigned po tag tokenizer recognizes word free text following paragraph describe component detail tag set implementation smaller version tag set developed university stuttgart tbingen referred elwis tag set thielen thielen schiller consists total po tag tag punctuation special tag truncated word tag elwis tag set given table guideline use individual tag available thielen sailer word form lexicon give set po tag assigned word set consist unambiguous word tag referred word equivalence class lexicon implementation derived lingsoft gertwol morphological analyzer us finite state transducer mapping morpho syntactic label generated gertwol elwis tag set lexicon realized finite state transducer dd tagger mapping rule developed generate lexicon elwis tag german lexical database centre lexical information celex guesser xerox tagger assign potential po tag unknown word according surface analysis word form addition common practice mapping po tag according word suffix implementation make use case initial letter word significant po assignment german class guesser take care po assignment abbreviation special symbol sequence language external material text class guesser lexicon finite state transducer model trained set initial transition bias including positive negative constraint tag sequence model trained initial bias performance resulting model increase appropriate initial bias bias model consisted specification plausible successor tag tag tag set constructed refined series subsequent training evaluation run symbol bias additional set bias define preference tag assignment given particular equivalence class symbol bias defined evaluation training run reflect bias equivalence class class guesser majority symbol bias added correct misguided bias chosen training process text training hmm selected german data contained eci multilingual cd rom eci",
        "word sample summer issue german newspaper frankfurter rundschau reference text taken frankfurter rundschau overlap training text reference text total running word tagged checked current version implementation us straightforward tokenizer accepting line token text pretokenized external tokenizer written lex best result implementation obtained running iteration training word training text total transition symbol bias configuration training parameter resulting hmm assigned incorrect tag run reference text compared assigned tag",
        "main advantage xerox tagger compared earlier implementation hmm tagger trained untagged text performance resulting hmm poor initial bias help training process find suitable parameter comparison evaluation procedure evaluate implementation hmm tagger described preceding section repeated initial bias result poor performance resulting hmm error rate initial bias help train model subtle task requires sound knowledge tag set target language model aiming requires feel initial bias modified given number training iteration frustrating linguistic knowledge create initial bias get optimized trained subsequent iteration training overcome disadvantage hybrid technology developed combine free text training method parameter estimation pre tagged text setting initial transition symbol bias replaced frequency tag sequence tag instantiation small pre tagged corpus counted frequency taken approximation model probability smoothed small number training iteration order gained german language model hybrid technology extension xerox tagger developed university stuttgart facilitate initialization hmm value obtained pre tagged corpus schmid kempe parameter obtained counting transition symbol frequency tagged word corpus taken newspaper stuttgarter zeitung available university stuttgart initial parameter adjusted single training iteration xerox baum welch implementation parameter estimation text training reference evaluation one implementation described section resulting hmm error rate superior performance model confirms result presented briscoe merialdo elworthy english obtained initial value transition output probability small number training iteration lead better result generated bias end scale parameter production hmm po disambiguator extraction parameter basis larger pre tagged text corpus baum welch estimation involved implementation described earlier work author feldweg feldweg model error rate depending coverage underlying lexicon reported result comparable implementation described paper tag set employed differ extent",
        "preceding section evaluation model quantitative measured percent mismatch output generated hmm tag assigned manual tagging error rate appropriate measure performance hmm given particular reference text say little disambiguation tagger ambiguity type involved disambiguation process difficulty disambiguation quantified ambiguity rate number possible tag assignment divided number word given text test text evaluate german model described section ambiguity rate lexicon provides average tag word text effort compare german model reported language english french tagged text analysed text contained word tagged english resp language model xerox tagger tag set annotate english text modified version brown tag set consisting total tag ambiguity rate text french text tag set described chanod tapanainen different tag ambiguity rate term ambiguity rate english french german text comparable order compare type ambiguity resolved different language model hmm tagger relative frequency table class computed text frequent equivalence class language listed table relative frequency table viewed term major word class noun verb adjective adverb closed class form following predominant ambiguity class english distinguished noun verb share offer plan adjective noun public high closed class noun adjective noun verb return field closed class adverb closed class adjective adverb french major ambiguity type noun verb affaire bout place adjective noun demi moyen responsable adjective verb appliqu devenu fabriqu closed class adjective numeral element frequent ambiguity type german belong major word class exception closed class adjective numeral einen einer verb adjective fehlgeschlagen bekannt reflecting subtle distinction german tag set vpp adjd participle modifier non attributive adjective comparison frequent ambiguity type show significant difference german model hand english french model german effort going subclassification major word class english french good deal disambiguation work devoted separate major word class",
        "difference ambiguity type model effect type error produced german model error affect assignment word subclass major word class show common error produced german model entry sorted decreasing frequency relative total number mismatch manually tagged text column give relative frequenciy second column list tag chosen german hmm tagger second column number following slash indicates number element equivalence class model choose missing number indicates choice lexicon column correct tag chosen human tagger common accumulated error confusion proper noun common noun result fact proper noun common noun capitalized german fourth line table represents special case error hmm model held responsible ambiguity indicated lexicon model choose occurs common noun proper noun die gehobene mittelklasse plaziert renault mrz den safrane expect add tag proper noun noun lexicon second frequent error type involves confusion infinitive pers pl finite present tense form homograph german hard disambiguate narrow context difficulty distinguishing non attributive adjectival usage participle ist geladen participle proper hat den wagen geladen mentioned preceding section addition number form finite verb gehrt source error remaining error misassignments closed class including known error long distance phenomenon resulting confusion relative pronoun demonstrative pronoun article sentence die einmal die buchproduktion erfaten texte doch der wollte nicht da falle auf",
        "despite hypothesis free word order german lead poor performance low order hmm tagger compared language english overall result german line comparable implementation english better argued disadvantage free word order hmm tagger compensated richer morphology additional disambiguation cue having upper lower case initial letter distinguish po membership hinders recognition proper noun common type error responsible model mistake important notice type disambiguation carried tagger german different disambiguation work english french english french fair number disambiguation involve separating major po class verb noun adjective work performed german model involves disambiguation subclass main category finite infinitive verb noun proper noun different sub category pronoun etc finding consequence compass project po disambiguation employed mean disambiguating word sens facilitate precise dictionary look technique help confine word sens english french little help word sense disambiguation german german model useful project tagged reference corpus required lexicographic work order adapt existing bilingual dictionary tagger annotate word german corpus contained eci multilingual corpus cd rom",
        "like thank helmut schmid university stuttgart providing extension parameter initialization xerox tagger jean pierre chanod lauri kartunnen rank xerox research laboratory grenoble making available french english tagged text lexicon like acknowledge valuable advice tracy holloway king steven abney commented earlier version paper work supported ministry science research land baden wrttemberg project corpus based development lexical knowledge base elwis commission european community lre project adapting bilingual dictionary online assistance compass lre",
        "natural language provide type lexical item describe motion entity respect location motion verb run enter spatial preposition verb location transitive cross town spatial preposition intransitive town case interesting french motion verb intransitive interaction motion verb spatial preposition give detailed information way human beeings represent spatiotemporal aspect motion describe motion fact choose verb preposition syntactic structure reveals mental cognitive representation claim natural language considered trace representation possible systematic detailled linguistic study light way spatiotemporal property represented basic concept representation lie present linguistic investigation french motion verb spatial preposition basic concept found address compositional semantics motion complex motion verb followed spatial preposition complexity refinement linguistic study presented justified required compositional level order capture different behaviour compositional process exist french language compare english language draw conclusion benefit approach",
        "following gruber jackendoff boon approach motion verb term localist semantical role label linguistic study french intransitive motion verb asher sablayrolles realized allowed definition ontology location basic concept location concrete place room house street position part location position room posture way position standing sitting lying help ontology realized typology intransitive motion verb distinguish category basis kind location refer location col verb entrer enter sortir denote change location enter place place different spatial relation location motion position verb voyager travel courir run denote change position travel run global location verb behave denote change position occur voyager travel example voyager sur place travel place verb change position cop verb denote possible change position courir run example courir sur place run place verb inertial change position icops verb posture coptu verb asseoir sit baisser bend denote change relation part entity following focus col verb change location verb rich spatiotemporal information disposal exhaustive list french col verb realized systematic fine linguistic study verb looking col verb french order extract intrinsic spatiotemporal property semantic property characterized restructuration space induced called reference location lref talmy lref suggested col verb initial location partir leave sortir path passer traverser pas final location arriver arrive entrer enter motion verb sortir suggest location gone space induced lref characterized author literature spatial system consisting inside outside lref propose refine structure new concept required distinguish minimal pair sortir partir leave entrer enter atterir land concept limit proximity distinguishing proximity sortir requires lref partir addition force mobile lref external zone contact required verb atterir final location lref contrast entrer outside proximity lref contrast approcher approach defined structuration space based zone inside external zone contact outside proximity structuration close way jackendoff landau encode space induced reference object introduced static spatial preposition come distinction examining different linguistic material conclude language structure space way sort lexical item motion verb dynamic static spatial preposition examine allowed classify",
        "linguistic study typology col verb spatial preposition realized verb considered adjunct atemporal form context hand preposition considered context methodology discussed borillo sablayrolles allowed extract intrinsic semantics lexical item natural language verb preposition sentence developped formal calculus asher sablayrolles based typology computes compositional way spatiotemporal property motion complex semantic property verb preposition reason space detail formalism intend present talk semantics motion complex simple addition semantics constituent contrary result complex interaction property case interaction appear new property belong verb preposition new property result interaction verb preposition consider example following sortir dans jardin garden verb sortir suggests initial location preposition dans mean translated positional preposition denotes static spatial relation location jardin garden final location motion final information contained verb preposition information result interaction verb sortir preposition dans combination item behave english final information brought preposition directional preposition particular combination create new information show neccesity account language specific behaviour natural language understanding system natural language machine translation formalize axiom non monotonic order logic behaviour possible kind verb preposition association french language use non monotonic logic order represent defeasible generic rule order encode default lexical entry axiom based lexical semantics col verb spatial preposition account syntactic structure sentence supposed x bar syntax internal subject essential link exist level discours sentence previous following sentence text link called discourse relation basic concept text structured asher",
        "study result presented cover lexical semantics discourse structure strong interaction end lexical information disambiguate structure discours discourse relation disambiguate lexical entry shown asher sablayrolles work based systematic detailed linguistic study lead complex computation calculating spatiotemporal semantics motion complex seen level detail complexity necessary want understand formalize compute right spatiotemporal semantics motion complex continue investigation direction compare result similar work course realization basquian language michel aurnague japanese language junichi saito use result presented refining notion structuration space zone distinguish sub class traditional known class aspectual study",
        "paradigm level morphology koskenniemi popular handling word formation phenomenon variety language original formulation extended allow morphotactic constraint expressed feature specification trost alshawi koskenniemi perspicuous device continuation class automatic compilation rule notation convenient rule writer finite state automaton developed allowing efficient analysis synthesis word form automaton derived rule trost involve composition lexicon karttunen trade run time efficiency factor important rapid accurate system development perspicuity notation ease debugging speed compilation size output independence morphological lexical component compilation compose level rule set set affix allowed combination andthe lexicon kaplan kay exposition mathematical basis type compilation appropriate rapid development acceptable run time performance depends nature language described number base form lexicon position dimensional space defined example english inflectional morphology simple dimension small lexicon known advance manageable size entire task morphological analysis carried compile time producing list analysed word form need looked run time network traversed need provide powerful mechanism level morphology simpler device affix stripping alshawi listing inflected form preferable agglutinative language korean finnish turkish kwon karttunen koskenniemi oflazer dimension large creating exhaustive word list question lexicon trivial network sense language tend exhibit non concatenative morphophonological phenomenon vowel harmony continuation class mechanism suffice describe allowed affix sequence surface level european language inflecting type occupy region space difficulty complex yield simpler technique work english phonological orthographic change involved affixation complex dimension large feature mechanism needed handle varied interrelated morphosyntactic phenomenon umlaut trost case number gender different morphological paradigm hand different affix possibility combination word dimension manageable paper describes representation associated compiler intended level morphological description written form inflecting language system described component core language engine cle alshawi general purpose language analyser generator implemented prolog support built lexicon access large external lexical database context efficient word analysis generation run time important ensuring morphology mechanism expressive easy debug allows quick compilation need integrated processing level particular specify relation morphosyntactic morphophonological rule lexical entry convenience developer mean",
        "feature equation assumed lexicon specified morphology rule compiled wish add test lexical entry recompiling rule necessary deal unknown word run time example querying large external lexical database attempting spelling correction alshawi analysis generation word form required time speed need time spent morphology small compared sentential contextual processing parameter language complex morphology syntax interface number affix combination task lexicon known compile time bidirectional processing need ease development optimize run time efficiency dictate design morphology compiler described paper spelling rule possible affix combination item lexicon item composed compilation phase french polish english inflectional morphology developed aspect mechanism allow phenomenon language handled",
        "formalism spelling rule dimension syntactic variant ruessink pulman rule formrules optional obligatory lexical string formmeaning surface lexical target correspond left right context feature specification satisfied vertical bar separate part string match letter correspondence surface lexical string entire word licensed partitioning partition pair corresponding surface lexical target licensed rule partition break obligatory rule partition break obligatory rule surface target match including feature specification feature rule list equation allowed finite set value feature prespecified atomic boolean expression surface lexical string character class single character represented single digit string item classname class list multiple occurrence single rule match character given application show french spelling rule developed system rule simplified make obligatory lexical realised surface followed morpheme boundary feature cdouble appropriate value default rule copy character surface lexical level boundary rule deletes boundary marker optional rule permit following realization cher expensive followed feminine gender suffix chre shown figure obligatory nature change fact orthographic feature restriction root cher cdouble consistent rule alternative realisation involving use default rule position ruled flavour level morphology target part rule need consist single character class occurrence contain surface target obviates need null character surface surface target length specified practice good strategy lexical target character definition obligatory rule block application rule lexical target different length example section clarifies point allowed sequence morpheme syntactic semantic property morpheme word derived combining specified morphosyntactic production rule dimension lexical entry affix dimension root dimension described alshawi production rule referred morphology rule appear production rule root assigned complex feature valued category including creation logical form passed constituent rule sharing variable feature augmented production rule device cle syntactico semantic description natural way express morphotactic information finite state device continuation class trost matiasek related approach syntactic semantic production rule deriving feminine singular french adjective suffixation given detail omitted figure case feature shared inflected word root logical form",
        "word shown adj deriv rule differing feature gender shown argument agr macro expands category form complete word affixable stem specified listing morphological rule terminal morpheme appropriate analysis constructed example present pseudo affix syntactic semantic information attached sense affix form regular person singular spelling rule reference present device allowing category logical form irregular word built production rule regular word",
        "formalism spelling rule dimension syntactic variant ruessink pulman rule formrules optional obligatory lexical string formmeaning surface lexical target correspond left right context feature specification satisfied vertical bar separate part string match letter correspondence surface lexical string entire word licensed partitioning partition pair corresponding surface lexical target licensed rule partition break obligatory rule partition break obligatory rule surface target match including feature specification feature rule list equation allowed finite set value feature prespecified atomic boolean expression surface lexical string character class single character represented single digit string item classname class list multiple occurrence single rule match character given application show french spelling rule developed system rule simplified make obligatory lexical realised surface followed morpheme boundary feature cdouble appropriate value default rule copy character surface lexical level boundary rule deletes boundary marker optional rule permit following realization cher expensive followed feminine gender suffix chre shown figure obligatory nature change fact orthographic feature restriction root cher cdouble consistent rule alternative realisation involving use default rule position ruled flavour level morphology target part rule need consist single character class occurrence contain surface target obviates need null character surface surface target length specified practice good strategy lexical target character definition obligatory rule block application rule lexical target different length example section clarifies point",
        "allowed sequence morpheme syntactic semantic property morpheme word derived combining specified morphosyntactic production rule dimension lexical entry affix dimension root dimension described alshawi production rule referred morphology rule appear production rule root assigned complex feature valued category including creation logical form passed constituent rule sharing variable feature augmented production rule device cle syntactico semantic description natural way express morphotactic information finite state device continuation class trost matiasek related approach syntactic semantic production rule deriving feminine singular french adjective suffixation given detail omitted figure case feature shared inflected word root logical form word shown adj deriv rule differing feature gender shown argument agr macro expands category form complete word affixable stem specified listing morphological rule terminal morpheme appropriate analysis constructed example present pseudo affix syntactic semantic information attached sense affix form regular person singular spelling rule reference present device allowing category logical form irregular word built production rule regular word",
        "rule lexical entry cle compiled form allows normal prolog unification category matching run time compiled form analysis generation indexed feature major category assigned unique position compiled prolog term feature finite value set specified compiled vector form allows boolean expression involving negation conjunction disjunction conjoined unification mellish alshawi compilation morphological information motivated nature task language handled discussed section expect number affix combination lexicon known advance interaction complex purpose morphological processing derive syntactic semantic analysis word vice purpose quick compilation required run time speed need moderate individual spell rule straightforward feature specification compiled positional boolean format character occurrence character class converted boolean vector left context reversed abramson efficiency possible analyse word compiled rule section long time wide range choice rule available point need check stage obligatory rule broken following approach legal sequence morpheme produced nondeterministic application production rule section selecting affix keeping root morpheme unspecified explained lexicon undetermined stage example english sequence ing produced asterisk representing unspecified root sequence associated restriction orthographic feature undergoes analysis compiled spelling rule section surface sequence root lexical sequence uninstantiated applied style abramson taking advantage prolog unification mechanism instantiate surface string corresponding affix place spelling constraint start end surface lexical form root process result set spelling pattern distinct application spelling rule affix sequence suggested production rule spelling pattern consists specified surface lexical root character sequence specified surface lexical affix sequence orthographic feature constraint associated spelling rule affix pair syntactic category specification derived production rule category root form inflected form pattern indexed according surface analysis lexical generation affix character involve run time inflected word analysed stage succeed number time including possible surface affix character word locating spelling pattern index matching remaining character word surface spelling pattern shared variable instantiating character lexical provide possible root spelling checking orthographic feature constraint root finding lexical entry root range mechanism including lookup system",
        "lexicon querying external lexical database attempting guess entry undefined word andunifying root lexical entry root category spelling pattern variable sharing category pattern creating specified category inflected form parsing generation process work reverse starting index lexical affix character arise spelling rule application fact compile time lexical surface form root length known possible hypothesize sensible length compile separate spelling pattern lead time pattern produced necessary instantiation surface string unspecified root represented complex redundant way variable instantiated single character beginning root variable instantiated list character continuation represent end root continuation time reversed root structure matched kleene star default spelling rule generality minimal redundancy constrained match default rule value required spelling rule character target lexical string n k beginning right context including boundary symbol lexical string rule match right context match required value calculated reference left context rule rule compilation spelling pattern lead run time analysis chre given derived specified rule sequence variable matching figure absence lexical string root correct treatment obligatory rule problem compilation obligatory rule specifies lexical realised surface certain contextual feature condition hold partitioning realised allowed condition unsatisfied use boolean vector feature character possible constrain partitioning unifying complement condition applicable obligatory rule preventing rule applying english simple inflectional spelling change work language including french lead excessive number spelling pattern obligatory rule non trivial context feature specification reason complement unification carried compile time spelling pattern augmented fact certain condition certain obligatory rule need checked certain part partitioning instantiated slows run time performance little speed acceptable compilation process entire rule set take minute thorough description french inflectional morphology running sparcstation time speed adequate nlp reflect fact system implemented prolog syntactico semantic analysis sentence morpheme sequence acceptability judgment produced french",
        "word rule set core lexicon average word second mean spelling analysis word leading mean morphological analysis reduction root suggested spelling analysis exist combine affix produced result cached subsequent attempt analyse word time running word faster analysis spelling possible analysis sought separation lexical morphological representation timing unaffected core lexicon size advantage taken prolog built indexing time important computation time rule set embodying comprehensive treatment french inflectional morphology developed person month english spelling rule set adapted ritchie day polish rule set development swedish planned near future",
        "compilation individual spell rule straightforward feature specification compiled positional boolean format character occurrence character class converted boolean vector left context reversed abramson efficiency possible analyse word compiled rule section long time wide range choice rule available point need check stage obligatory rule broken following approach legal sequence morpheme produced nondeterministic application production rule section selecting affix keeping root morpheme unspecified explained lexicon undetermined stage example english sequence ing produced asterisk representing unspecified root sequence associated restriction orthographic feature undergoes analysis compiled spelling rule section surface sequence root lexical sequence uninstantiated applied style abramson taking advantage prolog unification mechanism instantiate surface string corresponding affix place spelling constraint start end surface lexical form root process result set spelling pattern distinct application spelling rule affix sequence suggested production rule spelling pattern consists specified surface lexical root character sequence specified surface lexical affix sequence orthographic feature constraint associated spelling rule affix pair syntactic category specification derived production rule category root form inflected form pattern indexed according surface analysis lexical generation affix character involve run time inflected word analysed stage succeed number time including possible surface affix character word locating spelling pattern index matching remaining character word surface spelling pattern shared variable instantiating character lexical provide possible root spelling checking orthographic feature constraint root finding lexical entry root range mechanism including lookup system lexicon querying external lexical database attempting guess entry undefined word andunifying root lexical entry root category spelling pattern variable sharing category pattern creating specified category inflected form parsing generation process work reverse starting index lexical affix character",
        "complication arise spelling rule application fact compile time lexical surface form root length known possible hypothesize sensible length compile separate spelling pattern lead time pattern produced necessary instantiation surface string unspecified root represented complex redundant way variable instantiated single character beginning root variable instantiated list character continuation represent end root continuation time reversed root structure matched kleene star default spelling rule generality minimal redundancy constrained match default rule value required spelling rule character target lexical string n k beginning right context including boundary symbol lexical string rule match right context match required value calculated reference left context rule rule compilation spelling pattern lead run time analysis chre given derived specified rule sequence variable matching figure",
        "absence lexical string root correct treatment obligatory rule problem compilation obligatory rule specifies lexical realised surface certain contextual feature condition hold partitioning realised allowed condition unsatisfied use boolean vector feature character possible constrain partitioning unifying complement condition applicable obligatory rule preventing rule applying english simple inflectional spelling change work language including french lead excessive number spelling pattern obligatory rule non trivial context feature specification reason complement unification carried compile time spelling pattern augmented fact certain condition certain obligatory rule need checked certain part partitioning instantiated slows run time performance little speed acceptable",
        "compilation process entire rule set take minute thorough description french inflectional morphology running sparcstation time speed adequate nlp reflect fact system implemented prolog syntactico semantic analysis sentence morpheme sequence acceptability judgment produced french word rule set core lexicon average word second mean spelling analysis word leading mean morphological analysis reduction root suggested spelling analysis exist combine affix produced result cached subsequent attempt analyse word time running word faster analysis spelling possible analysis sought separation lexical morphological representation timing unaffected core lexicon size advantage taken prolog built indexing time important computation time rule set embodying comprehensive treatment french inflectional morphology developed person month english spelling rule set adapted ritchie day polish rule set development swedish planned near future",
        "clarify use formalism operation mechanism examine example obligatory spelling change french involve letter example masculine adjective noun ending eau feminine counterpart ending elle beau nice belle chameau camel chamelle final feminizing affix seen inducing obligatory spelling change obvious spelling rule allows change rule incorrect realization beau beaue shown figure affect partitioning lexical level form single partition following pair rule lexical target character achieve desired effect change rule partition figure change rule necessary surface target contain character blocking effect apply semantics obligatoriness lexical target context taken specified surface target length obligatory partition reverse constraint lexical target apply control application rule particular lexical item applicability deduced spelling example polish noun stem final syllable vowel inflected form accent dropped nominative plural krj style kroje forest bory combat boje exception zbj bandit zbje french verb infinitive end eler grave accent person singular future modeler model modlera double appeler appellera phenomenon handled providing obligatory rule case letter change constraining applicability rule feature making feature clash root change occur polish case partitioning given figure possible one change rule apply chngo feature unspecified value zbj rule prevented applying feature clash default rule apply",
        "obligatory spelling change french involve letter example masculine adjective noun ending eau feminine counterpart ending elle beau nice belle chameau camel chamelle final feminizing affix seen inducing obligatory spelling change obvious spelling rule allows change rule incorrect realization beau beaue shown figure affect partitioning lexical level form single partition following pair rule lexical target character achieve desired effect change rule partition figure change rule necessary surface target contain character blocking effect apply semantics obligatoriness lexical target context taken specified surface target length obligatory partition reverse constraint lexical target apply",
        "feature control application rule particular lexical item applicability deduced spelling example polish noun stem final syllable vowel inflected form accent dropped nominative plural krj style kroje forest bory combat boje exception zbj bandit zbje french verb infinitive end eler grave accent person singular future modeler model modlera double appeler appellera phenomenon handled providing obligatory rule case letter change constraining applicability rule feature making feature clash root change occur polish case partitioning given figure possible one change rule apply chngo feature unspecified value zbj rule prevented applying feature clash default rule apply",
        "debugging tool help checking operation spelling rule eitherin conjunction constraint oron case user ask inflection root licensed spelling rule production rule lexicon cher output ismeaning cher adjp adjective combine suffix listed produce inflected form shown useful checking undergeneration possible view spelling pattern production rule tree produce form chre trace simplified figure spelling pattern referred depicted different form figure notation denotes set possible consonant represented variable occurs right hand rule indicating selection occurrence rule tree single application rule adjp adjp fem describes feminine form adjective root taken masculine form root infl line feature differ root inflected form line show share pointed spelling pattern describes feminine form noun case spelling rule applied rule compilation specified surface lexical character sequence lexical morphotactic constraint existed constraint case rule apply constraint broken shown lexical sequence cher example output follows indicates user cher given lexical entry consistent constraint cdouble analysis valid second",
        "rule formalism compiler described work european language complex orthographic change range possible affix combination compilation run time efficiency acceptable use rule containing complex feature augmented category allows morphotactic behaviour non segmental spelling constraint specified perspicuous linguist leading rapid development description adequate nlp kind non linear effect common semitic language vowel consonant pattern interpolated word kay kiraz treated mechanism described proved possible define representation allowed part inflected word corresponding root separated part expressing inflection modified version current system basis efficient lookup spelling pattern current system allow possible lexical root calculated language handled current mechanism specification provided affix combination occur real text backup mechanism provided attempted slower complete direct application rule rarer case interaction morphological analysis spelling correction carter oflazer bowden fruitful area work root spelling pattern affix combination pointing created analysis reduces instance affix stripping amenable technique outlined carter work discrimination net root form required augmented spelling pattern creation flexibility resulting composing lexicon spelling rule lost",
        "tense interpretation received attention linguistics partee hinrichs nerbonne inter alia natural language processing webber kameyama lascarides asher inter alia researcher partee hinrichs nerbonne webber sought explain temporal relation induced tense treating anaphoric drawing reichenbach separation event speech reference time reichenbach account forward progression time induced successive simple past tense narrative treat simple past referring time evoked previous tense instance hinrich hinrichs proposal accomplishment achievement introduce new reference point ordered time event ensuring consecutive accomplishment achievement discourse ordered temporal sequence hand lascarides asher view temporal relation resolved product reasoning coherence relation holding utterance argue treating simple complex tense anaphoric unnecessary approach parallel treatment pronoun resolution espoused hobbs pronoun modeled free variable bound product coherence resolution temporal centering framework kameyama integrates aspect approach pattern treating tense anaphoric argue aspect analysis necessary account recovery temporal relation demonstrate approach address following example passage taken lascarides asher understood narrative indicating spilling subsequent slipping understood second clause explaining indicating reverse temporal ordering hold address related question arises treating simple past anaphoric treatment hinrichs explain forward progression time example explained sentence felicitous sentence predict clash temporal relation sentence simple past induce forward progression time conjunction indicates reverse temporal ordering second question arises assuming temporal relation recovered reasoning coherence relation use simple past passage felicitous past perfect passage explanation interpretation case indicated explained passage understood explanation passage case relationship need inferred present analysis section account fact section",
        "postulate rule characterizing referential nature tense role discourse relation constraining temporal relation clause rule governing tense main verb tense referential creating new temporal entity constraint imposed type past present future relation discourse reference time instance main verb tense introduces new temporal entity constraint simple tense speech time simple tense anaphoric auxiliary complex tense anaphoric identifying existing temporal entity indefinite main verb tense ordered respect tense specify implicit temporal relation described event claim relation refined constraint imposed coherence relation operative clause describe coherence relation relevant example paper temporal constraint narration relation characterized series event displaying movement time passage lascarides asher capture ordering constraint imposed narration coherence relation narration b parallel relation relates utterance share common topic relation impose constraint temporal relation event provided tense instance passage uttered response question inducing parallel relation narration temporal ordering sentence implied explanation relation denotes cause effect relationship reversed clause ordering sentence second event constrained preceding explanation b summarize analysis claim tense operates indefinite reference respect resolved discourse reference time temporal relation specified refined product establishing coherence relationship extant clause narration relation",
        "analyze example presented section repeated approach implicit ordering time evoked simple past passage result understanding narration passage auxiliary refers event time slipping past tense spill creates temporal entity constrained precede time necessitates coherence relation consistent temporal order case explanation passage time evoked simple past ordered explanation relation indicated resulting backward progression time passage tense coherence relation order time backward progression problem noted section treating simple past anaphoric account forward progression time passage expect existence explanation relation passage cause temporal clash fact passage felicitous clash temporal relation predicted account use simple past imply specific ordering narration relation order time progression passage explanation relation order backward progression passage parallel relation specify ordering potential context passage given section second problem noted section temporal relation recovered reasoning coherence relation use simple past passage felicitous past perfect passage explanation interpretation asks passage understood explanation passage case relationship need inferred hypothesize hearer assume speaker engaging narration absence specific cue contrary use past perfect passage cue implies reversed temporal ordering use explicit conjunction indicating coherence relation narration passage cue passage understood explanation semantic ground hearer assumes narration relation cued advantage approach lascarides asher lascarides asher note incoherence examplein arguing past perfect treated anaphoric theory analyse distinction simple past pluperfect term different relation reference time event time term event connection fail explain acceptable awkward lascarides asher pg show coherence relation need utilized account temporal relation bear issue past perfect anaphoric incoherence example predicted account virtue fact coherence relation corresponds narration reverse temporal ordering addressing example lascarides asher specify special rule connection changing tense cct law stipulates sentence containing simple past followed sentence containing past perfect related subset possible coherence relation subset contains relation predicted possible account treating past perfect anaphoric one constrain temporal order event displaying backward progression time advantage adopting rule comment law stipulated account fact concerning possible tense combination explain explanation",
        "relation inferred passage passage lascarides asher stipulate causal slipping law stating spilling cause slipping requires cct law satisfied constraint imposed require second clause contain past perfect simple past explain use simple past coherent explanation relationship indicated sentence explain cct satisfied causal law supporting similar example infer unsignaled explanation relation discussion example pg lascarides asher account explain past perfect stand discourse opened consider stating sentence isolation usage infelicitous dependency salient time introduced captured lascarides asher account sentence containing past perfect treated equivalent containing simple past hand sentence simple past felicitous standing opening discourse introducing asymmetry account treating simple past anaphoric evoked time fact explained account given",
        "method proposed deal anaphora resolution prepositional phrase attachment phenomenon literature abundant pps frazier fodor hobbs wilks huang anaphora carter reinhart sidner method considered way dealing phenomenon concrete system propose paper algorithm deal phenomenon analyser anaphora module pertains recent method us set resolution rule based focusing approach sidner rule applied conceptual representation output set candidate antecedent pps unattached preposition involve unfilled role conceptual structure cs expressed frame based language zarri disambiguation procedure aim filling role attachment rule work accomplished context cobalt project lre dealing financial news detailed discussion procedure anaphora resolution attachment developed",
        "main principle algorithm algorithm applied text sentence sentence ambiguity previous sentence considered resolved anaphora procedure skip resolution given anaphor anaphor preceded unattached preposition resolution rule role parameter unattached preposition resolution anaphor postponed second phase anaphora resolution proposed procedure based successive call anaphora module attachment module output set cs represent intermediate result exchanged module operate turn aim fill unfilled role cs anaphora unattached pps summarize algorithm apply anaphora module attachment procedure anaphora left unresolved apply anaphora module unattached pps apply attachment procedure pps anaphor treated order module called based efficiency deduced statistical data performed cobalt corpus main case faced algorithm anaphor occurs given preposition sentence resolution depend preposition attached cataphors rare case anaphora module applied attachment procedure example show resolution anaphoric pronoun performed starting attached anaphor occurs unattached preposition intra sentential anaphor referring entity sentence resolution depend previous prepositional phrase case resolution anaphora postponed anaphora module according principle stated anaphor included particular case attachment rule need semantic information object pronoun semantic information available attachment rule applied anaphoric pronoun resolved determine semantic class refer attachment procedure applied sequence contains pps anaphor object length cycle",
        "pronoun resolved anaphora resolution module preceded unattached pps resolution skipped attachment procedure called determine attachment object comprises anaphoric pronoun case preceded attachment pps skipped anaphora module called resolve anaphoric pronoun possible example previous pps attached anaphor attachment procedure called pps module called time redundancy processing algorithm considered splitting anaphora resolution attachment procedure phase repetition procedure",
        "different type text thing differing genre different type varying quality vary parameter relevant general information retrieval problem matching reader need text variation text retrieval context problem areidentifying genre andchoosing criterion cluster text genre predictable precision recall confused issue identifying topic choosing criterion discriminate topic orthogonal genre dependent variation variation relates content topic dimension co variance certain topic occur certain genre text certain genre treat certain topic topic occur genre interest biber studied text variation parameter found text considered vary dimension study cluster feature according covariance find underlying dimension biber wish find method identifying computable parameter classify unseen text general class small set smaller biber dimension explained simple term user information retrieval application aim set text selected sort crude semantic analysis performed information retrieval system partition genre text type display variation possible dimension",
        "start feature similar investigated biber concentrate easy compute assuming speech tagger cutting church person pronoun occurrence rate opposed general hedge biber biber feature available advent proficient analysis program instance complete surface syntactic parsing performed categorization voutilainen tapanainen use discriminant analysis technique descriptive statistic analysis take set precategorized individual data variation number parameter work set discriminant function distinguishes group function predict category membership new individual based parameter score tatsuoka mustonen",
        "data brown corpus english text sample uniform length categorized category seen table ran discriminant analysis text corpus different feature seen table spss system statistical data analysis feature complete discriminant analysis spss discriminant function extracted data analysis linear combination parameter categorize set category function need determined content able plot category dimensional plane want ease exposition use significant function case category function necessary determining category item function classified case misclassified case shown table figure function extracted case classified case misclassified case seen table figure miscellaneous problematic category grouping different informative text single problematic subsubset text subset non fiction text labeled learned humanity misclassified miscellaneous function extracted case classified case misclassified case shown table distinguish different type fiction expensive term error fiction subcategories collapsed category error rate categorization improve shown revised total record table learned humanity subcategory problematic item classified misclassified religion belle lettres",
        "important note experiment claim genre fact differ sort technique determine parameter use given set use test set disjoint training set claim function method extract data useful discus method categorizes set text given set category given set parameter error rate climb number category tested corpus category chosen defined instance distinguishing different type fiction formal stylistic criterion kind attempt fiction type defined term content statistical technique factor analysis discover category biber problem derived category sense real meaning supported data difficult explain unenthusiastic layman aim use technique retrieval tool criterion studied second higher order statistic respective parameter parameter vary certain text type skewed distribution difficult determine standard method support automatic determination standard deviation skewness discrimination criterion investigation untried parameter step",
        "unrelated study genre study readability aim categorize text according suitability assumed set assumed reader wealth formul compute readability combine computed text measure average sampled average sentence length combined computed word length incidence word specified easy word list chall klare spite chall warning injudicious application writing task readability measurement come prescriptive metric good writing tool writer come disrepute text researcher small study confirms basic finding early readability study important factor one tested word length sentence length different derivative parameter long readability indexing scheme descriptive application work discriminate text type",
        "technique show practical promise territorial map shown figure useful tool displaying type particular text compared existing text technique demonstrated obvious application information retrieval picking interesting text content based method select large set easy manipulation browsing cutting specific application area unlikely text database accessed free form text consideration specific way text type useful domain field specific text typology envisioned application user employ cascade filter starting filtering topic continuing filter genre text type ending filter text quality tentative finer grained qualification",
        "intfilter project department computer system science computational linguistics psychology stockholm university present studying text usenet news conferencing system project present study text appear different type usenet news conference investigates classification criterion category experienced usenet news user report intfilter newsreader system project applies method described project us category query comment announcement faq categorizing parameter different type length measure form word content quote level percentage quoted text usenet news specific parameter",
        "recent year seen resurgence interest probabilistic technique automatic language analysis particular arisen distinct paradigm processing basis pre analyzed data taken data oriented parsing data oriented parsing dop model rule language experience form analyzed corpus constitute basis language processing space present justification adopting approach detail advantage offer main claim make effective language processing requires consideration structural statistical aspect language traditional competence grammar rely standard statistical technique n gram model attempt combine tradition produce performance grammar contain information structural possibility general language system detail actual language use language community approach entail corpus pre analyzed ie hand parsed question arises formalism lack competing competence grammar available reason expect grammar suited dop approach designed characterize nature linguistic competence performance section set property require performance grammar offer formalism attempt satisfy requirement",
        "given attempting construct formalism justice statistical structural aspect language feature wish maximize include following formalism use probabilistic processing technique having close correspondence simple probabilistic model markov process formalism grained responsive behaviour individual word n gram model suggests lexicalist approach karttunen rule encoded lexicon phrase structure rule introduce lexical item capable capturing linguistic intuition language user word formalism able characterize structural regularity language sophistication modern competence grammar real data formalism able characterize wide range syntactic structure found actual language use including excluded competence grammar belonging periphery language ungrammatical interpretable utterance analysis interpretation point close relation simple probabilistic model good place start search right branching finite state grammar class grammar rule form non terminal terminal tree simple structure table equivalent vertical alignment paper right probabilistic term finite state grammar corresponds order markov process given sequence state drawn finite set possible state probability particular state occurring depends identity previous state finite state grammar word associated transition category tree transition calculate probability string word parse represented string category state product probability transition ie addition satisfying criterion finite state grammar fulfills requirement formalism lexicalist definition rule introduces lexical item finite state grammar chosen criterion linguistic adequacy present insurmountable stumbling block simple formalism syntax reduced string category state hope capture basic hierarchical structure familiar tree structure linguistic expression non terminal viewed atomic category way line current theory category taken bundle feature feature value stack category hierarchical structure represented notation represent state basic category carrying category stack hierarchical structure sentence represented syntactic link non adjacent word impossible standard finite state grammar established passing category stack state intervening word formalism capture basic linguistic structure confirmed proof aho indexed grammar ie category supplemented stack unbounded length restricted right linear tree equivalent context free grammar perusal state transition associated individual word reveals obvious relationship type categorial grammar represent list category null arrive following transition corresponding categorial type ditransitive verb gave isdeterminers complement position determiner",
        "subject position type raised common noun fact intermediate constituent formed analysis closer parallel dependency syntax rightward pointing arrow allowed formalism presented notational variant lack intermediate constituent added benefit spurious ambiguity arise addition stack valued feature suffices capture basic hierarchical structure language additional feature deal syntactic relation example following example gpsg unbounded dependency captured slashed category represent slashed category lower case use notation category carrying feature topicalized sentence analysis space paper greater detail construction involving unbounded dependency complement control phenomenon captured similar way criterion remains satisfied width coverage formalism cope peripheral structure found real written spoken text stand formalism equivalent context free grammar problem dealing phenomenon discontinuous constituent non constituent coordination gapping extension formalism taking weak equivalence context free grammar natural general analysis present construction sketched pair sentence identical interpretation containing discontinuous noun phrase respective analysis transition differs corresponding word core variant dog respective transition noun introduce relative clause modifier difference discontinuous variant category taken stack time modifier placed stack assumed right linear indexed grammar rule disallowed indexed grammar allowing transition kind end formalism weak equivalence context free grammar course having allowed crossed dependency formalism disallow similar analysis discontinuity unacceptable english present problem dop information parsed corpus determines structure possible need rule transition rare corpus garbled speech transition rel met written spoken english analysis standard coordination shown typical transition gnawed transition introducing coordinated general transition category list category transition introducing coordination constituent coordination present problem phrase structure approach generalize schema obtained standard coordination allowing single category list category found suffice non constituent coordination analysis regular transition bone transition introducing coordination category stack non stack move formalism step indexed grammar final incarnation formalism state transition grammar title schema investigated characterize gapping construction noted indefinite centre embedding described expense unlimited growth length state contrast",
        "finite state grammar chosen criterion linguistic adequacy present insurmountable stumbling block simple formalism syntax reduced string category state hope capture basic hierarchical structure familiar tree structure linguistic expression non terminal viewed atomic category way line current theory category taken bundle feature feature value stack category hierarchical structure represented notation represent state basic category carrying category stack hierarchical structure sentence represented syntactic link non adjacent word impossible standard finite state grammar established passing category stack state intervening word formalism capture basic linguistic structure confirmed proof aho indexed grammar ie category supplemented stack unbounded length restricted right linear tree equivalent context free grammar perusal state transition associated individual word reveals obvious relationship type categorial grammar represent list category null arrive following transition corresponding categorial type ditransitive verb gave isdeterminers complement position determiner subject position type raised common noun fact intermediate constituent formed analysis closer parallel dependency syntax rightward pointing arrow allowed formalism presented notational variant lack intermediate constituent added benefit spurious ambiguity arise addition stack valued feature suffices capture basic hierarchical structure language additional feature deal syntactic relation example following example gpsg unbounded dependency captured slashed category represent slashed category lower case use notation category carrying feature topicalized sentence analysis space paper greater detail construction involving unbounded dependency complement control phenomenon captured similar way",
        "consider pair sentence identical interpretation containing discontinuous noun phrase respective analysis transition differs corresponding word core variant dog respective transition noun introduce relative clause modifier difference discontinuous variant category taken stack time modifier placed stack assumed right linear indexed grammar rule disallowed indexed grammar allowing transition kind end formalism weak equivalence context free grammar course having allowed crossed dependency formalism disallow similar analysis discontinuity unacceptable english present problem dop information parsed corpus determines structure possible need rule transition rare corpus garbled speech transition rel met written spoken english",
        "analysis standard coordination shown typical transition gnawed transition introducing coordinated general transition category list category transition introducing coordination constituent coordination present problem phrase structure approach generalize schema obtained standard coordination allowing single category list category found suffice non constituent coordination analysis regular transition bone transition introducing coordination category stack non stack move formalism step indexed grammar final incarnation formalism state transition grammar title schema investigated characterize gapping construction",
        "assuming corpus parsed state transition grammar information parse fresh text word type corpus collect transition occurs calculate probability distribution possible transition infinite number concrete token word dog example dog transition probability distribution find probable parse sentence find path word word maximizes product state transition order markov process simple minded approach implement way leaf desired probability distribution gappy huge data collected chance provide desired path sentence reasonable length slim process generalizing smoothing transition probability seen indispensable exhausting possible method smoothing following implementation described end paper element stack carried state state looking correspondence state transition categorial type previous transition dog factor feature passed state state instance example sentence generalized transition generalized single transition establish word paradigm ie class word occur similar transition probability distribution individual word smoothed blending paradigmatic distribution paradigm correspond great extent word class rule based grammar advantage retained system grained reflect idiosyncratic pattern individual word override paradigmatic information sufficient data available unknown system treated extreme example word lacking sufficient transition data given transition distribution blended open class word paradigm essential effective processing smoothing operation rise new problem example factoring item stack remove model disinclination long state inherent original corpus recapture discarded aspect language introduce model probabilistic penalty based state length penalty calculated according length state parsed corpus allow modelling restriction centre embedding allow processing phenomenon characterized example heavy np shift suppose corpus contained distinct transition word threw particle object greater cumulative negative effect longer state lead model giving sentence shifted higher probability strength n gram model capture certain lexical preference information example bigram model trained sufficient data probability bigram dog barked expected higher cat barked slice world knowledge model lack difficult small extension present model capture information",
        "exhausting possible method smoothing following implementation described end paper element stack carried state state looking correspondence state transition categorial type previous transition dog factor feature passed state state instance example sentence generalized transition generalized single transition establish word paradigm ie class word occur similar transition probability distribution individual word smoothed blending paradigmatic distribution paradigm correspond great extent word class rule based grammar advantage retained system grained reflect idiosyncratic pattern individual word override paradigmatic information sufficient data available unknown system treated extreme example word lacking sufficient transition data given transition distribution blended open class word paradigm",
        "essential effective processing smoothing operation rise new problem example factoring item stack remove model disinclination long state inherent original corpus recapture discarded aspect language introduce model probabilistic penalty based state length penalty calculated according length state parsed corpus allow modelling restriction centre embedding allow processing phenomenon characterized example heavy np shift suppose corpus contained distinct transition word threw particle object greater cumulative negative effect longer state lead model giving sentence shifted higher probability",
        "strength n gram model capture certain lexical preference information example bigram model trained sufficient data probability bigram dog barked expected higher cat barked slice world knowledge model lack difficult small extension present model capture information introducing additional feature containing lexical value head phrase shorthand representing subject slashed sentence added lexical head feature appear contrast n gram sentence cloud world knowledge containing bigram cat barked added structure model allows lexical preference captured head noun head verb world knowledge system reinforced stereotypical transition",
        "running word section brown corpus text hand parsed state transition grammar actual formalism fuller schematic given including additional feature case tense person number probability generalized way discussed previous section word chosen text section brown corpus fed parser alteration word input assigned orthographic class given appropriate transition calculated corpus parsed hand parse differing insignificant way model hope know parsed ie analysis differed hand parse non trivial way parsed ie transition necessary find parse path lacking generalizing transition result present modest borne mind data system work small smoothing transition probability optimal present target achieve level performance corpus extended hand correction parser output hand parsing scratch save certain drudgery help minimize error maintain consistency distant goal ascertain performance model improve parsing new text processing data hand correction par limit self improvement",
        "sentence word chosen text section brown corpus fed parser alteration word input assigned orthographic class given appropriate transition calculated corpus parsed hand parse differing insignificant way model hope know parsed ie analysis differed hand parse non trivial way parsed ie transition necessary find parse path lacking generalizing transition",
        "result present modest borne mind data system work small smoothing transition probability optimal present target achieve level performance corpus extended hand correction parser output hand parsing scratch save certain drudgery help minimize error maintain consistency distant goal ascertain performance model improve parsing new text processing data hand correction par limit self improvement",
        "describing object purpose linguist use feature structure description entity interpreted true false object example conventional interpretation description black true soot particle false snowflake use feature structure describe object demand feature structure interpreted true false object paper tailor semantics king suit typed feature structure carpenter create explicit interpretation typed feature structure description use interpretation define notion satisfiable feature structure feature structure algebra provides description expressive provided feature logic feature structure describe object profit large stock available computational technique represent test process feature structure paper demonstrate computational benefit marrying tractable syntax explicit semantics creating simple effective algorithm decide satisfiability feature structure goetz troll type resolution system implement semantics efficient refinement satisfiability algorithm present goetz gerdemann king gerdemann forthcoming",
        "signature provides symbol construct typed feature structure interpretation give symbol meaning signature iff sextuple set partial order set partial function cartesian product andfor defined defined andhenceforth work signature member state member type subsumption member specie member attribute appropriateness interpretation iff triple set total function toa total function set partial function defined defined andfor defined defined interpretation member object type denotes set object denotation specie partition assigns object unique specie denotation contains object object denotation specie iff encodes relationship denotation specie type object denotation type iff denotation type contains denotation type attribute denotes partial function object object assigns attribute partial function denotes encodes relationship denotation specie attribute defined denotation attribute act object denotation specie yield object denotation type undefined denotation attribute act object denotation specie defined denotation attribute act object denotation type yield object denotation type finite sequence attribute path write set path path interpretation function iff interpretation total function set partial function andfor functional composition ofi write path interpretation function feature structure iff quadruple finite subset finite partial function cartesian product total function andfor run run iff andfor defined andeach feature structure connected moore machine moore state input alphabet output alphabet true iff feature structure interpretation object run run defined defined anddefinition satisfiable feature structure iff feature structure interpretation object true",
        "abundance interpretation preclude effective algorithm decide feature structure satisfiable insert morphs feature structure object yield interpretation free characterisation satisfiable feature structure semi morph iff triple nonempty subset equivalence relation total function andfor defined anddefinition morph iff semi morph defined theneach morph moshier abstraction moshier connected typed carpenter moore machine state input alphabet output alphabet abstract iff morph interpretation object iff defined defined andfor iff defined interpretation object unique morph abstract write abstraction standard object iff quadruple morph andis equivalence class write set standard object write total function iff write total function set partial function defined iff interpretation write defined iff induction length equivalence class path abstraction isproposition morph interpretation object abstraction approximates iff feature structure morph run run anda feature structure approximates morph iff moshier abstraction feature structure subsumes carpenter morph interpretation object feature structure true iff approximates abstraction feature structure satisfiable iff approximates morph proposition",
        "theorem give interpretation free characterisation satisfiable feature structure characterisation admit effective algorithm decide feature structure satisfiable use theorem resolved feature structure yield general interpretation free characterisation satisfiable feature structure admits algorithm resolved feature structure iff feature structure total function andfor defined defined andeach resolved feature structure typed carpenter feature structure output alphabet resolvant iff resolved feature structure feature structure proposition interpretation object feature structure true iff resolvant true rational iff defined rational resolved feature structure satisfiable bijection ordinal semi morph andis morph approximates theorem satisfiable rational feature structure satisfiable iff resolvant proposition",
        "section use theorem given rational signature meet reasonable computational condition construct effective algorithm decide feature structure satisfiable computable iff countable finite effective function true false andfor effective function defined undefined proposition computable effective function feature structure list resolvants computable effective function finite list total function effective function finite set finite partial function cartesian product total function domain defined true false effective function finite set total function total function true false follows feature structure set set true true set output effective algorithm feature structure list resolvants rational computable effective function feature structure satisfiable true false theorem proposition goetz troll system goetz gerdemann king gerdemann forthcoming employ efficient refinement test satisfiability feature structure fact troll represents feature structure disjunction resolvants feature structure speaking resolvants feature structure underlying finite state automaton feature structure differ output function exploit property represent feature structure finite state automaton set output function troll unifier closed representation expensive troll us compilation run time",
        "aim formalize constraint needed develop parser based unification grammar called parser deal variety type sentence japanese parsing natural language understanding important necessary task parser process discourse japanese called anaphora resolution syntactic semantic pragmatic constraint involved resolve anaphora course omitted pronoun resolved instance suffix regarded clause conjunct subject omitted surface corefer subject sentence example iswhere subject refer sentential topic hanako example possible account interpretation following subject phrase pro term sell result controlled subject main subject term sentential topic hanako possible antecedent subject example complex sentence thing different following sentence hanako speaker sentence candidate antecedent following interpretation likely pro fact fact known japanese linguist sell takubo result anaphora resolution complex sentence promising candidate centering theory brennan walker apply centering theory sequence sentence discourse regard subordinate clause main clause segment discourse hanako marked regarded topic clause topic hanako strongest candidate backward center subordinate clause backward center subordinate clause hanako subject refers hanako way subordinate clause case dealt subject main clause known refer hanako result interpretation shown candidate property sharing thoery kameyama theory subject share subjecthood known refer hanako topic clause property sharing theory fails account intuitive interpretation shift attention microscopic speaking important semantics complex sentence formalized relation semantic role appear main clause subordinate clause glance constraint relation local term main subordinate clause word semantic role appear subordinate clause semantic role appear main clause constrained constraint complex sentence looking find constraint subordinate clause constraint main clause represented local constraint introducing new notion motivated characterized person reason act main clause describes motivated pragmatic role appear subordinate clause constraint subordinate clause stated identity relation motivated semantic pragmatic role appearing subordinate clause constraint",
        "local subordinate clause constraint main clause stated identity relation motivated come subordinate clause semantic role appearing main clause understanding main clause care semantic pragmatic role subordinate clause motivated sense constraint main clause treated local constraint main clause question represent semantics complex sentence feature structure called write constraint relation semantic pragmatic role feature structure formalism space limitation paper pursue constraint semantic feature structure",
        "pay attention general structure japanese utterance helpful represent semantics complex sentence japanese linguist proposed general structure japanese utterance mikami minami takubo gunji categorized clause class open semi open closed categorization indicates content clause interacts outside clause instance categorized degree possibility coreference pronoun subordinate clause nominal topic appear main clause mikami idea minami proposed level level correspond proposition sentence communication mood utterance take account hearer divided level level corresponds corresponds certain kind subject called objective subject proposed detailed structure starting predicate verb adjective object voice subject aspect tense modality topic mood added fulfilled sentence component end utterance gunji structure node daughter node complex sentence structure called cluase level complex sentence following shown fig fig sub clause conjunct mean subordinate clause conjunctive particle fig represents hierarchical structure word order complex sentence japanese structure gunji structure showing complex proposition subordinate clause conjunctive particle added deal complex sentence comment appearing sub clause structure comment appearing judgement comment defined practice level depth appearing comment comprehensible sentence",
        "section predicate category subordinate clause deal paper table category exists person affected situation described subordinate clause contrary category explicit affected person theory affected person play key role semantics complex sentence result general derive useful result category theory deal category paper moment explain nature called subjective predicate mentioned table short subjective predicate describes experiencer inner state known experiencer focus verbal suffix garu garu syntax present form root form gar inflection follows gar gar etc addition garu allophonic root form gat gat ta past form gat teiru progressive form derived gat form appear example talk semantics garu mean sign behave ing ohye palmer semantics explained proposal formalize garu semantics computational linguistics introduce new pragmatic role called observer notion observer share large pivot iida sell notion observer introduced garu narrower notion introduced role playing key role bridge semantic role subordinate clause semantic role main clause observer introduced garu known consequence nature subjective predicate following sentence subjective adjective followed verbal suffix garu experiencer subjective adjective speaker sentence thing introduced notion observer clear way deal semantic content sem soa state affair form situation semantics use semantic role agent patient experiencer argument role soa observer observes situation characterized soa know exists observer observed soa embedded observing situation turn embedded semantic content sense observed soa argument role observed confusion omit role observed typical schema sem type following use garu value relation feature meant rel english gross relation garu observe explain semantics clause consists subjective adjective garu ta garu category category form p garu past form p gat ta subjective adjective category table verb followed gar category table experiencer category exist observer experiencer observe experience sem feature p garu gat following mean token identical constraint token like written shown constraint satisfaction method developed researcher tsuda theory able implemented system theirs sentence finish garu gat ta important point arean introduced observer speaker consequentlythe experiencer speaker",
        "according hierachical structure japanese sentence shown fig essential hierarchical structure following sentence shown fig figure structure proposition replaced corresponding part sentence embedding structure corresponds hierarchy shown hierarchical structure fig grasp image relation hierarchical structure corresponding example complex sentence analyzed based hierarchical structure following result unification f subordinate clause main clause content syntactic feature head omitted english gross relation following sime close node samu feel cold key point semantics complex sentence role motivated appears corresponds content subordinate clause role motivated link content subordinate clause main clause motivated characterized following important indispensable semantics complex sentence speaking relation subordinate clause main clause look relation relation semantic pragmatic role appearing subordinate clause appearing main clause introduced role motivated give important clue relation rest paper effort concentrated motivated refers main concern semantic role sem subordinate clause motivated unified semantic role sem main clause motivated unified subsection propose constraint complex sentence categorize relation subordinate clause main clause based semantics divided type complex sentence important typical type table mean subordinate clause main clause table column sentence type second column indicates rough meaning relation subordinate clause main clause complex sentence column show japanese conjunctive particle represent type complex sentence row adjunct tutu nagara express event ocurring aspectual suffix mean perfective instance nagara regarded clause conjuncts interpreted minami deal type temporal adverbial clause describes event occurs event described main clause expect essential information relation semantic role appearing adverbial main clause type sentence focus type motivated play key role constraint table constraint semantic pragmatic role subordinate clause motivated show semantic role main clause unified motivated table column row constraint name second column show set sentence type constraint shown second row apply column table show predicate pattern subordinate clause column table show semantic category predicate main clause constraint written second row apply",
        "constraint table local subordinate clause side constraint role subordinate clause case subjective adjective garu constraint motivated experiencer hold type case auxiliary verb yaru kureru given case future problem table state case exists party motivated put experiencer state instance experiencer permitted motivated kind case thing complicated omit space table local main clause semantic role appeares righthand constraint defined main clause influence subordinate clause come role motivated mean constraint rest section example exemplify constraint type constraint applied know content subordinate main clause combination agent main clause observer situation described subordinate clause behaved feeling cold interpretation coincides native intuition following pair example motivateds subordinate clause constrained motivateds observer subordinate clause say case unified motivated hand observer interpretation comply constraint corefer interpretation expected constraint apply case know example constraint strong identify antecedent make safe interpretation disambiguation constraint useful inference commonsense knowledge special vocabulary kekkyoku case intransitive passive adversity passive known gunji exists person affected situation described passive sentence example sentence following semantic role affected person role wife dead affected intuitive interpretation expected constraint table table contrary case transitive passive affected context transitive passive form require role affected inherent adversity passive instance case person wallet stolen explicit regarded affected case having affected relational noun subject transitive passive person relation expressed relational noun thought affected situation mother father daughter son supervisor forth relational noun couple example sentence following retaliated retaliate certain relation henchman attacked instance bos henchman constraint table table apply affected attacking event described subordinate clause interpretation coincides native intuition sum constraint constraint satisfaction process based parsing word constraint satisfaction process subordinate clause analysis subordinate clause main clause",
        "according hierachical structure japanese sentence shown fig essential hierarchical structure following sentence shown fig figure structure proposition replaced corresponding part sentence embedding structure corresponds hierarchy shown hierarchical structure fig grasp image relation hierarchical structure corresponding example complex sentence analyzed based hierarchical structure following result unification f subordinate clause main clause content syntactic feature head omitted english gross relation following sime close node samu feel cold key point semantics complex sentence role motivated appears corresponds content subordinate clause role motivated link content subordinate clause main clause motivated characterized following important indispensable semantics complex sentence speaking relation subordinate clause main clause look relation relation semantic pragmatic role appearing subordinate clause appearing main clause introduced role motivated give important clue relation rest paper effort concentrated motivated refers main concern semantic role sem subordinate clause motivated unified semantic role sem main clause motivated unified",
        "subsection propose constraint complex sentence categorize relation subordinate clause main clause based semantics divided type complex sentence important typical type table mean subordinate clause main clause table column sentence type second column indicates rough meaning relation subordinate clause main clause complex sentence column show japanese conjunctive particle represent type complex sentence row adjunct tutu nagara express event ocurring aspectual suffix mean perfective instance nagara regarded clause conjuncts interpreted minami deal type temporal adverbial clause describes event occurs event described main clause expect essential information relation semantic role appearing adverbial main clause type sentence focus type motivated play key role constraint table constraint semantic pragmatic role subordinate clause motivated show semantic role main clause unified motivated table column row constraint name second column show set sentence type constraint shown second row apply column table show predicate pattern subordinate clause column table show semantic category predicate main clause constraint written second row apply constraint table local subordinate clause side constraint role subordinate clause case subjective adjective garu constraint motivated experiencer hold type case auxiliary verb yaru kureru given case future problem table state case exists party motivated put experiencer state instance experiencer permitted motivated kind case thing complicated omit space table local main clause semantic role appeares righthand constraint defined main clause influence subordinate clause come role motivated mean constraint rest section example exemplify constraint type constraint applied know content subordinate main clause combination agent main clause observer situation described subordinate clause behaved feeling cold interpretation coincides native intuition following pair example motivateds subordinate clause constrained motivateds observer subordinate clause say case unified motivated hand observer interpretation comply constraint corefer interpretation expected constraint",
        "apply case know example constraint strong identify antecedent make safe interpretation disambiguation constraint useful inference commonsense knowledge special vocabulary kekkyoku case intransitive passive adversity passive known gunji exists person affected situation described passive sentence example sentence following semantic role affected person role wife dead affected intuitive interpretation expected constraint table table contrary case transitive passive affected context transitive passive form require role affected inherent adversity passive instance case person wallet stolen explicit regarded affected case having affected relational noun subject transitive passive person relation expressed relational noun thought affected situation mother father daughter son supervisor forth relational noun couple example sentence following retaliated retaliate certain relation henchman attacked instance bos henchman constraint table table apply affected attacking event described subordinate clause interpretation coincides native intuition sum constraint constraint satisfaction process based parsing word constraint satisfaction process subordinate clause analysis subordinate clause main clause motivated value constrained subordinate clause",
        "relevant research jpsg developed gunji gunji studied icot working group focus pragmatic oriented jpsg japanese linguist enormous basic observation proposed linguistic theory phenomenon deal paper mikami kuno kuno ohye minami takubo teramura teramura saito course research based work observation ohye said garu subordinate clause subject main clause experiencer subordinate clause saito say cognizer corresponds observer introduced garu andif observer introduced subordinate clause responsible person appearing main clause identical observer linguistic phenomenon observation similar constraint propose new answer state semantics complex sentence relation semantic role use semantic pragmatic role grammatical role constraint constraint account anaphora sentence main clause passive agent experiencer subject following example intuitive reading following subject refers taro subject refers taro parent observer motivated subordinate clause formalize theory formalism detail omitted space limitation find constraint complex sentence local one localization constraint found introducing new pragmatic role observer motivated important efficiency based parsing localization make proposed constraint compositional one case embedded complex sentence identify referent motivated bridge subordinate clause main clause constraint proposed resolved computation confined clause case directional auxiliary verb yaru kureru left future problem implemented japanese language understanding system based theory state paper space limitation report detail implementation place near future",
        "bidirectionality grammar research topic natural language processing enjoying increasing attention strzalkowski clear theoretical practical advantage bidirectional grammar use appelt address topic describing novel approach hpsg pollard sag based language processing us line compiler prime declarative grammar generation parsing hand primed grammar advanced earley processor developed technique direction independent sense generation parsing hpsg grammar paper focus application developed technique context neglected area hpsg generation gave use earley algorithm generation algorithm use prediction step restrict feature instantiation predicted phrase lack goal directedness gerdemann showed modify restriction function information available completion step earley generation prediction problem generating subpart construction wrong order lead massive nondeterminacy nontermination overcame problem incorporating head driven strategy earley algorithm evaluating head construction dependent subpart suffers efficiency problem head construction missing displaced underspecified martinovic strzalkowski observed simple head reordering grammar rule insufficient restricting information available generation form grammar restricted unary binary rule essential argument approach eaa strzalkowski approach generation parsing logic grammar us line compilation invert parser oriented logic grammar inversion process consists automatic static reordering node grammar interchanging argument rule defined head based notion essential argument argument instantiated ensure efficient terminating execution node observe eaa infeasible demand investigation possible permutation grammar interchanging argument recursive procedure proposed strzalkowski fails guarantee input output grammar equivalent direct inversion approach dia minnen overcomes problem making reordering process goal directed developing reformulation technique allows successful treatment rule exhibit head recursion eaa dia presented approach inversion parser oriented grammar grammar suitable generation approach declarative grammar specification input produce generator parser oriented grammar dymetman paper adopt interesting perspective developed compiler line optimization phrase structure rule based typed feature structure grammar generalizes technique developed context dia advanced typed extension earley style generator gerdemann line compilation section produce grammar earley style generator section use line grammar optimization overcomes problem displaced head developed technique tested large hpsg grammar partial topicalization german hinrichs uncovered important constraint form phrase structure rule phrase structure rule grammar imposed compiler section",
        "shieber noted main shortcoming earley generation lack goal directedness result proliferation edge tackled shortcoming modifying restriction function information available completion step generator follows head driven strategy order avoid inefficient evaluation order head right hand grammar rule distinguished distinguished category scanned predicted resulting evaluation strategy similar head corner approach shieber gerdemann hinrichs press prediction follows main flow semantic information lexical pivot reached head dependent subpart construction built fashion mixture information flow crucial semantic information goal category integrated subcategorization information lexicon strict evaluation strategy suffers called head recursion generation analog left recursion parsing evaluation strategy fail rule irrespective order evaluation right hand category rule combining line optimization process mixed evaluation strategy refrain complete reformulation grammar example minnen improved typed extension gerdemann earley generator number technique reduce number edge created generation optimization helpful supply edge chart index backward index pointing state chart edge predicted index pointing state predicted edge matching backward index edge combined completion located indexing technique illustrated improves complex index gerdemann related oldt resolution tamaki sato edge resulted active edge prediction backward index edge identified forward index edge active edge result edge identical backward index case example step edge edge edge edge get predicted passive edge forward index order use passive edge completion active edge need consider edge index identical backward index second optimization creates table category prediction discussed gerdemann table avoid redundant prediction expensive subsumption test index lexical entry necessary obtain constant time lexical access optimization earley generator lead significant gain efficiency despite heuristic improvement problem goal directedness solved displaced head present principal goal directedness problem head driven generation approach shieber koenig gerdemann hinrichs press head refers construction head phonology construction head unspecified phonology guide generation phonological realization head construction play generation construction illustrate problem underspecified head pose consider sentence adopt argument composition analysis presented hinrichs nakazawa subcat list auxiliary verb",
        "instantiated lexicon instantiated combination verbal complement main verb phrase structure rule describes construction head driven generator generate head rule prescribes order generation complement head generator generates main verb subcat list main verb instantiates subcat list head generation deterministic procedure complement generated sequence generator generates complement main verb subcat list head contains restricting information guide deterministic generation generation generate test procedure complement generated random eliminated unification order evaluation complement rule influence efficiency generation efficient head driven generator order evaluation complement rule line reordering solve ordering problem discussed previous subsection unattractive interpreting grammar rule run time creates overhead determining optimal evaluation order impossible freezing overcome ordering problem unappealing goal freezing expensive demand procedural annotation declarative grammar specification presupposes grammar writer posse substantial computational processing expertise chose deal ordering problem line compilation optimize grammar generation additional provision dealing evaluation order earley generator",
        "improved typed extension gerdemann earley generator number technique reduce number edge created generation optimization helpful supply edge chart index backward index pointing state chart edge predicted index pointing state predicted edge matching backward index edge combined completion located indexing technique illustrated improves complex index gerdemann related oldt resolution tamaki sato edge resulted active edge prediction backward index edge identified forward index edge active edge result edge identical backward index case example step edge edge edge edge get predicted passive edge forward index order use passive edge completion active edge need consider edge index identical backward index second optimization creates table category prediction discussed gerdemann table avoid redundant prediction expensive subsumption test index lexical entry necessary obtain constant time lexical access optimization earley generator lead significant gain efficiency despite heuristic improvement problem goal directedness solved",
        "displaced head present principal goal directedness problem head driven generation approach shieber koenig gerdemann hinrichs press head refers construction head phonology construction head unspecified phonology guide generation phonological realization head construction play generation construction illustrate problem underspecified head pose consider sentence adopt argument composition analysis presented hinrichs nakazawa subcat list auxiliary verb instantiated lexicon instantiated combination verbal complement main verb phrase structure rule describes construction head driven generator generate head rule prescribes order generation complement head generator generates main verb subcat list main verb instantiates subcat list head generation deterministic procedure complement generated sequence generator generates complement main verb subcat list head contains restricting information guide deterministic generation generation generate test procedure complement generated random eliminated unification order evaluation complement rule influence efficiency generation efficient head driven generator order evaluation complement rule",
        "line grammar optimization based generalization dataflow analysis employed dia dataflow analysis typed feature structure grammar dataflow analysis take input specification path start category considered instantiated case generation mean user annotates path specifying logical form path subpaths bound use type hierarchy extension unification generalization operation path annotation preserved determine flow semantic information rule lexical entry grammar sharing determines dataflow rule grammar dataflow analysis determine relative efficiency particular evaluation order right hand category phrase structure rule computing maximal degree nondeterminacy introduced evaluation category maximal degree nondeterminacy introduced right hand category equal maximal number rule lexical entry category unifies given binding annotation optimal evaluation order right hand category found comparing maximal degree nondeterminacy introduced evaluation individual category degree nondeterminacy grammar allowed introduce degree nondeterminacy introduced evaluation right hand category rule exceeds admissible degree nondeterminacy ordering hand rejected degree nondeterminacy grammar allowed introduce set incremented optimal evaluation order rule grammar found compilation process illustrated basis phrase structure rule argument composition discussed limitation force recursive optimization rule defining right hand category considering defining lexical entry user annotated start category left hand phrase structure rule lead annotation path specifying logical form construction bound result structure sharing left hand rule auxiliary verb category value auxiliary verb treated bound addition path value maximal specific type appropriate feature specified example path cat considered bound basis annotated rule investigate lexical entry defining right hand category auxiliary verb category unified defining lexical entry preservation binding annotation following example lexical entry note subpaths path marked bound considered bound binding annotation lexical entry defining auxiliary verb determine lexical entry right hand category rule unifies maximal degree nondeterminacy case maximal degree nondeterminacy evaluation auxiliary verb introduces low logical form auxiliary verb considered instantiated mark path defining lexical entry instantiation deduced type hierarchy mimic evaluation auxiliary verb determine information common defining lexical entry taking generalization specific feature structure subsuming unify result original right hand category phrase structure rule generalization unification operation preserve binding annotation lead structure sharing",
        "annotation logical form verbal complement considered instantiated nonverbal complement instantiated subsequent investigation maximal degree nondeterminacy introduced evaluation complement permutation find logical form sentence restricts evaluation nonverbal complement evaluation verbal complement verified basis sample lexical entry main verb relative efficiency evaluation lead compiler chooseas optimal evaluation order phrase structure rule argument composition optimal evaluation order phrase structure rule need head dataflow analysis treat head complement includes head calculation optimal evaluation order rule evaluation head rule introduces nondeterminacy provides insufficient restricting information evaluation complement dataflow analysis select head category evaluated choose insteadas optimal evaluation order demonstrates important consequence dataflow analysis compile declarative grammar grammar optimized generation displaced head pose problem optimal evaluation order right hand rule determined head dataflow analysis ignores grammatical head identifies processing head processing complement second processing complement",
        "compilation process illustrated basis phrase structure rule argument composition discussed limitation force recursive optimization rule defining right hand category considering defining lexical entry user annotated start category left hand phrase structure rule lead annotation path specifying logical form construction bound result structure sharing left hand rule auxiliary verb category value auxiliary verb treated bound addition path value maximal specific type appropriate feature specified example path cat considered bound basis annotated rule investigate lexical entry defining right hand category auxiliary verb category unified defining lexical entry preservation binding annotation following example lexical entry note subpaths path marked bound considered bound binding annotation lexical entry defining auxiliary verb determine lexical entry right hand category rule unifies maximal degree nondeterminacy case maximal degree nondeterminacy evaluation auxiliary verb introduces low logical form auxiliary verb considered instantiated mark path defining lexical entry instantiation deduced type hierarchy mimic evaluation auxiliary verb determine information common defining lexical entry taking generalization specific feature structure subsuming unify result original right hand category phrase structure rule generalization unification operation preserve binding annotation lead structure sharing annotation logical form verbal complement considered instantiated nonverbal complement instantiated subsequent investigation maximal degree nondeterminacy introduced evaluation complement permutation find logical form sentence restricts evaluation nonverbal complement evaluation verbal complement verified basis sample lexical entry main verb relative efficiency evaluation lead compiler chooseas optimal evaluation order phrase structure rule argument composition",
        "optimal evaluation order phrase structure rule need head dataflow analysis treat head complement includes head calculation optimal evaluation order rule evaluation head rule introduces nondeterminacy provides insufficient restricting information evaluation complement dataflow analysis select head category evaluated choose insteadas optimal evaluation order demonstrates important consequence dataflow analysis compile declarative grammar grammar optimized generation displaced head pose problem optimal evaluation order right hand rule determined head dataflow analysis ignores grammatical head identifies processing head processing complement second processing complement",
        "earley generator described compiler line grammar optimization tested large hpsg grammar test grammar based implementation analysis partial topicalization german hinrichs troll system gerdemann king developed technique uncovered important constraint form phrase structure rule grammar imposed compiler compiler able find evaluation order earley generator sufficient restricting information generate subpart construction particular case complement displacement problem arises complement receives essential restricting information head construction extracted time provides essential restricting information complement stayed case represented figure page processing complement head displaced problematic case provides essential binding successful evaluation complement evaluated head evaluated possible evaluate example problematic complement displacement taken test grammar given figure page topicalized partial anna lieben receives restricting semantic information auxiliary verb evaluation provides essential binding direct object subject stayed mittelfeld auxiliary verb mutual dependency subconstituents different local tree lead unrestricted generation partial unrestricted generation subject mittelfeld handled problem partial execution pereira shieber filler head rule allows evaluation filler evaluation auxiliary verb subject head driven generator rely similar solution able find successful ordering local tree exist potential problem approach constitutes requirement phrase structure rule grammar need particular degree specificity generalization operation mimic evaluation best illustrated basis following schematic phrase structure rule underspecification head rule allows unify finite auxiliary finite ditransitive main verb combination underspecification complement allows rule argument composition construction discussed construction finite main verb saturated mean logical form nonverbal complement available evaluation complement tagged case argument composition evaluation finite verb case head rule ditransitive main verb result use generalization suffice mimic evaluation respective right hand category verbal category defining lexical entry instantiate logical form nonverbal argument dataflow analysis lead conclusion logical form nonverbal complement instantiated cause rejection possible evaluation order rule evaluation unrestricted nonverbal complement exceeds allowed maximal degree nondeterminacy grammar forced split schematic phrase structure rule specific rule optimization process important note",
        "compiler able find evaluation order earley generator sufficient restricting information generate subpart construction particular case complement displacement problem arises complement receives essential restricting information head construction extracted time provides essential restricting information complement stayed case represented figure page processing complement head displaced problematic case provides essential binding successful evaluation complement evaluated head evaluated possible evaluate example problematic complement displacement taken test grammar given figure page topicalized partial anna lieben receives restricting semantic information auxiliary verb evaluation provides essential binding direct object subject stayed mittelfeld auxiliary verb mutual dependency subconstituents different local tree lead unrestricted generation partial unrestricted generation subject mittelfeld handled problem partial execution pereira shieber filler head rule allows evaluation filler evaluation auxiliary verb subject head driven generator rely similar solution able find successful ordering local tree exist",
        "potential problem approach constitutes requirement phrase structure rule grammar need particular degree specificity generalization operation mimic evaluation best illustrated basis following schematic phrase structure rule underspecification head rule allows unify finite auxiliary finite ditransitive main verb combination underspecification complement allows rule argument composition construction discussed construction finite main verb saturated mean logical form nonverbal complement available evaluation complement tagged case argument composition evaluation finite verb case head rule ditransitive main verb result use generalization suffice mimic evaluation respective right hand category verbal category defining lexical entry instantiate logical form nonverbal argument dataflow analysis lead conclusion logical form nonverbal complement instantiated cause rejection possible evaluation order rule evaluation unrestricted nonverbal complement exceeds allowed maximal degree nondeterminacy grammar forced split schematic phrase structure rule specific rule optimization process important note consequence general limitation dataflow analysis mellish",
        "innovative approach hpsg processing described us line compiler prime declarative grammar generation parsing input primed grammar advanced earley processor line compiler extends technique developed context dia compiles typed feature structure grammar simple logic grammar approach allows efficient bidirectional processing similar generation parsing time shown combining line technique advanced earley style generator provides elegant solution general problem displaced head pose conventional head driven generation developed line compilation technique crucial use fundamental property hpsg formalism monostratal uniform treatment syntax semantics phonology support dataflow analysis provide information line compilation based compiler us type hierarchy determine path value minimal type appropriate feature bound equivalent kind minimal type untyped feature structure grammar constant similar fashion line optimization",
        "glr developed robust version generalized parser allows skipping unrecognizable part input sentence lavie tomita designed enhance parsability domain spontaneous speech input contain deviation grammar extra grammaticalities grammar coverage case complete input sentence covered grammar parser attempt find maximal subset input parsable case parse serve good approximation true parse sentence parser accommodates skipping word input string allowing shift operation performed inactive state node graph structured stack g input symbol inactive state equivalent skipping word input encountered parser reached inactive state current word shifted parser previous reduce operation remain valid word input skipped skipped word maintained symbol node represent parse sub tree guarantee runtime feasibility glr parser coupled beam search heuristic restricts skipping capability parser focus par maximal close maximal substring input efficiency parser increased enhanced process local ambiguity packing pruning ambiguous symbol node compared term word skipped case phrase skipped word phrase skipped word discarded favor complete parsed phrase operation reduces number par pursued parser",
        "generalized parser developed tomita extended original parsing algorithm case non lr language parsing table contain entry multiple parsing action algorithm us graph structured stack g order pursue parallel different parsing option arise result multiple entry parsing table second data structure us pointer track possible parse tree parsing input sharing common subtrees different par process local ambiguity packing allows parser pack sub par rooted non terminal single structure represents glr parser syntactic engine universal parser architecture developed cmu tomita architecture support grammatical specification lfg framework consists context free grammar rule augmented feature bundle associated non terminal rule structure computation specified implemented unification operation allows grammar constrain applicability context free rule result parsing input sentence consists parse tree computed feature structure associated non terminal root tree",
        "end process parsing sentence glr parser return set possible par corresponding grammatical subset word input sentence beam search heuristic ambiguity packing scheme set par maximal close maximal grammatical subset principle goal find maximal parsable subset input string parse case distinct maximal par consisting different subset word original sentence experience shown case ignoring additional input word result parse coherent developed evaluation heuristic combine different measure order select parse deemed best heuristic us set feature parse candidate evaluated compared use feature candidate parse ignored part original input sentence feature designed general grammar domain independent parse heuristic computes penalty score feature penalty different feature combined single score linear combination weight scheme adjustable optimized particular domain grammar parser selects parse ranked parse lowest overall score experimented following set evaluation feature number position skipped wordsthe number substituted wordsthe fragmentation parse statistical score disambiguated parse tree penalty scheme skipped word designed prefer par correspond fewer skipped word assigns penalty range word original sentence skipped scheme word skipped sentence receive higher penalty preference designed handle phenomenon false start common spontaneous speech glr parser capability handling common word substitution parser input string output speech recognition system input contains pre determined substituted word parser attempt continue original input word specified correct word number substituted word evaluation feature prefer analysis fewer substituted word grammar working allow single input sentence analyzed grammatical sentence fragment experiment indicated case fragmented analysis desirable use sum number fragment analysis additional feature augmented parser statistical disambiguation module use framework similar proposed briscoe carroll shift reduce action parsing table augmented probability probability performed set disambiguated par probability parse action induce statistical score alternative parse tree disambiguation use statistical score disambiguated parse additional evaluation feature par statistical score value converted confidence measure common parse tree receive lower penalty score following formula penalty score feature combined linear combination weight assigned feature determine way",
        "interact experiment fined tuned weight try optimize result training set data plan investigating possibility known optimization technique task utility parser glr depends semantic coherency parse result return parser designed succeed parsing input parsing success provide likely guarantee coherency believe task handled domain dependent semantic analyzer follow parser attempted handle problem simple filtering scheme filtering scheme task classify parse chosen parser category bad heuristic take account actual value parse combined penalty score measure relative length input sentence penalty score scheme precise threshold fine tuned try optimize classification result training set data",
        "experimented following set evaluation feature number position skipped wordsthe number substituted wordsthe fragmentation parse statistical score disambiguated parse tree penalty scheme skipped word designed prefer par correspond fewer skipped word assigns penalty range word original sentence skipped scheme word skipped sentence receive higher penalty preference designed handle phenomenon false start common spontaneous speech glr parser capability handling common word substitution parser input string output speech recognition system input contains pre determined substituted word parser attempt continue original input word specified correct word number substituted word evaluation feature prefer analysis fewer substituted word grammar working allow single input sentence analyzed grammatical sentence fragment experiment indicated case fragmented analysis desirable use sum number fragment analysis additional feature augmented parser statistical disambiguation module use framework similar proposed briscoe carroll shift reduce action parsing table augmented probability probability performed set disambiguated par probability parse action induce statistical score alternative parse tree disambiguation use statistical score disambiguated parse additional evaluation feature par statistical score value converted confidence measure common parse tree receive lower penalty score following formula penalty score feature combined linear combination weight assigned feature determine way interact experiment fined tuned weight try optimize result training set data plan investigating possibility known optimization technique task",
        "utility parser glr depends semantic coherency parse result return parser designed succeed parsing input parsing success provide likely guarantee coherency believe task handled domain dependent semantic analyzer follow parser attempted handle problem simple filtering scheme filtering scheme task classify parse chosen parser category bad heuristic take account actual value parse combined penalty score measure relative length input sentence penalty score scheme precise threshold fine tuned try optimize classification result training set data",
        "conducted new experiment test utility glr parser parse evaluation heuristic parsing speech recognized spontaneous speech atis domain modified existing partial coverage syntactic grammar grammar atis domain development set sentence resulting grammar rule translate parsing table state list common appearing substitution constructed development set correct par grammatical sentence train parse table statistic disambiguation parse evaluation experimentation evaluation feature weight set following way described penalty skipped word range depending word position sentence penalty substituted word set substituting word preferable skipping word fragmentation feature given weight prefer skipping word reduces fragmentation count penalty summed converted statistical score parse set new sentence test set goal fold wanted compare parsing capability glr parser original glr parser wished test effectiveness evaluation heuristic selecting best parse wanted evaluate ability parse quality heuristic classify glr par bad ran parser time test set run skipping disabled equivalent running original glr parser second run conducted skipping enabled heuristic run conducted skipping enabled simple heuristic prefers par based number word skipped run single selected parse result sentence evaluated determine parser returned correct parse result experiment seen table result indicate glr parser result significant improvement performance heuristic percentage sentence parser returned parse matched matched correct parse increased result skipping capability glr succeeds parse sentence parsable original glr parser test sentence parsable glr significant portion sentence return bad par skipping essential word input looked effectiveness parse quality heuristic identifying bad par heuristic successful labeling bad par bad good close par labeled good heuristic harsh heuristic effective identifying bad par result indicate integrated heuristic scheme selecting best parse performs simple heuristic considers number word skipped simple heuristic good close par returned sentence involved degree skipping integrated heuristic scheme good close par returned sentence additional sentence analysis showed sentence par better selected integrated parse evaluation heuristic",
        "describe method analysing temporal structure discourse take account effect tense aspect temporal adverbial rhetorical structure minimises unnecessary ambiguity temporal structure discourse grammar implemented carpenter ale formalism method building temporal structure discourse combine constraint preference use constraint reduce number possible structure exploiting hpsg type hierarchy unification purpose apply preference choose remaining option temporal centering mechanism end recommending underspecified representation structure technique avoid generating temporal rhetorical structure higher level information disambiguate",
        "paper describe method analysing temporal structure discourse component implemented discourse grammar english goal temporal component yield detailed representation temporal structure discourse taking account effect tense aspect temporal expression time minimising unnecessary ambiguity temporal structure method combine constraint based approach approach based preference exploit hpsg type hierarchy unification arrive temporal structure constraint placed structure tense aspect rhetorical structure temporal expression use temporal centering preference described kameyama poesio rate possibility temporal structure choose best starting point work scha polanyi discourse grammar scha polanyi pruest implementation extended hpsg grammar pollard sag gerald penn bob carpenter encoded ale carpenter paper focus temporal processing algorithm particular analysis narrative progression rhetorical structure perfect temporal expression",
        "known algorithm tracking narrative progression developed kamp hinrichs partee formalises observation event occur preceding event state overlap preceding event algorithm give correct result example following event mary standing understood occur john enters room state mary seated understood overlap event john entering room rhetorical relationship eventuality causation elaboration enablement temporal default overridden following example causal relationship mary pushing john falling second event understood precede second sentence elaboration refer aspect event sequential event suggested world knowledge allows detect default overridden example lascarides asher suggest general knowledge postulate case pushing cause falling invoked generate backward movement reading problem practical system twofold assume case narrative kamp hinrichs partee algorithm default time default applied need check available world knowledge world knowledge postulate overriding assumption processing text expensive operation alternative assume temporal ordering event consecutive sentence possibility precede event overlap resulting temporal structure ambiguous small discourse ambiguity unwarranted appear reading possible john gave mary slice pizza stared started stare undesirable temporal processing mechanism postulate ambiguity case course possible advantage certain cue word indicate constrain rhetorical relation example order event understood reverse cue word signal causal relationship event kehler point movement time considered default consecutive event sentence use cause temporal clash felicitous expression noon previous thursday similar effect override default temporal relation place constraint tensein example default interpretation john detroit overlap boston phrase previous thursday override giving interpretation john detroit precedes boston suggests temporal information given tense act weaker constraint temporal structure information given temporal adverbial possibility rhetorical relation narration elaboration causal relation constrained aspect example state elaborate state event event elaborate event eventive second sentence elaboration sentence occur stative form example progressive building dog house consideration aim implementation work treat",
        "tense aspect cue word rhetorical relation constraining specific information explicit cue word having higher priority specific information tense main advantage approach reduces temporal structure ambiguity having rely detailed world knowledge postulate list possible temporal relation eventuality described consecutive sentence temporal expression cue word sentence tense aspect second sentence express simple past event constrain way lack space additional constraint given hitzeman example simple eventive sentence follows simple eventive sentence second event understood occur precede refer event elaboration relation event overlap constraint weaker explicit clue cue word rhetorical relation temporal expression express state possible temporal relation hold event described event activity preceding temporal focus referred solved problem point tense provide constraint temporal structure discourse add ambiguity event described past perfect sentence precede event described simple past sentence sentence added ambiguity result following possible continuation temporal relation continuation portion earlier text attach constrained line sketched problem determining thread continue continues thread sam ring bell continues thread sam loses key ambiguity sentence perfect continuation preceding thread start new thread sentence continues thread losing key start new thread problem multi sentence discourse thread sentence continue use implementation centering kameyama poesio technique similar type centering nominal anaphora sidner grosz assumes discourse understanding requires notion aboutness nominal centering assumes object current discourse temporal centering assumes thread discourse following addition tense aspect constraint preference new utterance continue thread parallel tense related preference continue current thread switching thread confirmed preference testing idea brown corpus example temporal centering preference technique reduce ambiguity recall example possible continuation shown difficulty example determining sentence continues thread begun second sentence example preference technique allows choose thread second assigns higher rating thread tense parallel new sentence case sam rang bell hannah opened door simple past tense example fact key mentioned second sentence link",
        "second thread handle example employ preference relating sentence thread content word rated close sentence store semantic pattern word cheap quick form world knowledge pattern easier provide detailed world knowledge postulate required approach result similar precise temporal structure processing semantic pattern know key keyring close semantic link second sentence prefer connect sentence thread begun second approach representing semantic relationship morris hirst word lexicon associated thesaurus like fashion given rating according close avoid relying high level inference specific world knowledge postulate goal determine temporal structure possible application higher level inference",
        "following scha polanyi pruest model discourse consists unit called discourse constituent unit dcus related temporal rhetorical relation basic dcu represents sentence clause complex dcus built basic complex dcus ale implementation dcu contains following slot temporal information cue rhetorical structure contains content word found dcu compare content word current dcu previous thread order rate semantic closeness dcu thread contains semantic aspect event state activity extended penn carpenter implementation hpsg grammar semantic aspect calculated stored relation dcu previous item phrase cue word stored affect value slot temporal centering keep track thread followed preference continuing current thread thread constructed discourse existing thread thread available continuation store semantic interpretation temporal expression associated dcu store temporal relation eventuality discourse recent event current thread subsequent eventuality elaborate event overlap come precede keep track tense syntactic aspect dcu dcu simple pres futaspect simple perf prog allow mentioned type information constrain employ hierarchy rhetorical temporal relation illustrated figure ale system way clue tense cue word work reduce number possible temporal structure approach improves earlier work discourse structure lascarides asher kehler reducing number possible ambiguity precise kamp hinrichs partee approach take account way apparent default overridden differentiates event activity behave narrative progression aspect rhetorical relation temporal expression affect value type express relationship dcus cue word marked according rhetorical relation specify relation passed dcu relation marker cue word temporal relation consistent priority indicator tense aspect example sentence ruled cue phrase result conflict temporal expression minute hand temporal expression indicate overlap relation cue word indicate background relation contribution consistent type contain background value specific value",
        "reason space difficult example sign based output grammar ale rule restrict summary algorithm rendition system output algorithm calculating temporal structure discourse summarised follows consists part constraint based portion preference based portion possible temporal rhetorical relation constrained temporal expression determines temporal relationship new dcu previous one default ignored item cue word influence value type figure step attempt place conflicting value slot parse fail temporal expression cue phrase tense semantic aspect influence value type table rhetorical relation tense aspect constrain possibility exists semantic preference choose possibility semantic distance rating new dcu previous thread determined existing thread new thread started preference preference relating new dcu thread parallel tense employed kameyama poesio detail resulting rating factored rating thread thread followed highest rated thread thread continued corresponds temporal centering preference continue current thread dcu continue highest rated thread solution generated table provide observation use fill value observation summarised follows event variable associated tempfoc recent event activity processed overlap ifdescribes state ordescribes state describes activity occur tempfoc ifdescribes simple tense event ordescribes complex tense clause describes complex tense event ordescribes event describes atelic simple tense state ordescribes state describes simple tense activity precede ifdescribes event describe activity describes perfect stative elaborate describes event ordescribes activity describes atelic orand describe state describes simple tense state ordescribes complex tense state algorithm identify rhetorical temporal relation cue word rhetorical structure present narrow possibility cue word present constraint based observation tense aspect interaction shown table example represents simple eventive sentence perfect eventive sentence spite lack rhetorical cue know precedes structure possible narrow possibility preference example allow possible temporal relation event continuation sentence reading reading sentence begin new thread constraint reduce number reading preference reduce reading continuation correct temporal relation shown",
        "method classifying word according context use scientific practical interest scientific question arise connection distributional view linguistic lexical structure relation question lexical acquisition psychological computational learning perspective practical point view word classification address question data sparseness generalization statistical language model model deciding alternative analysis proposed grammar known simple tabulation frequency certain word participating certain configuration example frequency pair transitive main verb head noun direct object comparing likelihood different alternative configuration problem large corpus number possible joint event larger number event occurrence corpus event seen making frequency count unreliable estimate probability proposed dealing sparseness problem estimating likelihood unseen event similar event seen instance estimate likelihood particular direct object verb likelihood direct object similar verb requires reasonable definition verb similarity similarity estimation method hindle proposal word similar strong statistical evidence tend participate event notion similarity agree intuition case clear construct word class corresponding model association research address question us similar raw data investigate factor word association tendency association word certain hidden sens class association class worthwhile base model preexisting sense class resnik work described look derive class distributional data model sens probabilistic concept cluster corresponding cluster membership probability word class based modeling technique natural language rely hard boolean class brown construction demanding depends frequency count joint event involving particular word unreliable source information noted approach avoids problem follows consider major word class verb noun experiment single relation experiment relation transitive main verb head noun direct object raw knowledge relation consists frequency occurrence particular pair v required configuration training corpus form text analysis required collect collection pair corpus experiment derived newswire text parsed hindle parser fidditch hindle constructed similar table help statistical speech tagger church tool regular expression pattern matching tagged corpus yarowsky compared accuracy coverage method systematic bias introduce took care filter certain systematic error instance misparsing subject complement clause direct object main verb report verb like consider problem classifying noun according distribution direct object verb converse problem",
        "similar theoretical basis method support use clustering build model n ary relation term association element coordinate appropriate hidden unit cluster centroid association hidden unit noun classification problem empirical distribution noun given conditional density problem study use classify classification method construct set cluster cluster membership probability cluster associated cluster centroid discrete density obtained averaging cluster noun according conditional verb distribution need measure similarity distribution use purpose relative entropy kullback leibler distance distribution natural choice variety reason sketch case increase probability decrease relative frequency distribution random sample drawn according probability mass given set sample length relative frequency distribution bounded cover thomas trying distinguish hypothesis relative frequency distribution observation give relative weight evidence favor similar relation hold empirical distribution probability drawn distribution use relative entropy context distribution word measure likely instance cluster centroid information theoretic perspective measure inefficient average use code based encode variable distributed according respect problem give loss information cluster centroid actual distribution word modeling distributional property relative entropy natural measure similarity distribution clustering minimization lead cluster centroid simple weighted average member distribution technical difficulty defined sidestep problem smoothing frequency church gale satisfactory goal work avoid problem data sparseness grouping word class turn problem avoided clustering technique need compute distance individual word distribution word distribution average distribution current cluster centroid guaranteed nonzero word distribution useful advantage method compared agglomerative clustering technique need compare individual object considered grouping",
        "follows consider major word class verb noun experiment single relation experiment relation transitive main verb head noun direct object raw knowledge relation consists frequency occurrence particular pair v required configuration training corpus form text analysis required collect collection pair corpus experiment derived newswire text parsed hindle parser fidditch hindle constructed similar table help statistical speech tagger church tool regular expression pattern matching tagged corpus yarowsky compared accuracy coverage method systematic bias introduce took care filter certain systematic error instance misparsing subject complement clause direct object main verb report verb like consider problem classifying noun according distribution direct object verb converse problem similar theoretical basis method support use clustering build model n ary relation term association element coordinate appropriate hidden unit cluster centroid association hidden unit noun classification problem empirical distribution noun given conditional density problem study use classify classification method construct set cluster cluster membership probability cluster associated cluster centroid discrete density obtained averaging",
        "cluster noun according conditional verb distribution need measure similarity distribution use purpose relative entropy kullback leibler distance distribution natural choice variety reason sketch case increase probability decrease relative frequency distribution random sample drawn according probability mass given set sample length relative frequency distribution bounded cover thomas trying distinguish hypothesis relative frequency distribution observation give relative weight evidence favor similar relation hold empirical distribution probability drawn distribution use relative entropy context distribution word measure likely instance cluster centroid information theoretic perspective measure inefficient average use code based encode variable distributed according respect problem give loss information cluster centroid actual distribution word modeling distributional property relative entropy natural measure similarity distribution clustering minimization lead cluster centroid simple weighted average member distribution technical difficulty defined sidestep problem smoothing frequency church gale satisfactory goal work avoid problem data sparseness grouping word class turn problem avoided clustering technique need compute distance individual word distribution word distribution average distribution current cluster centroid guaranteed nonzero word distribution useful advantage method compared agglomerative clustering technique need compare individual object considered grouping",
        "general interested organize set linguistic object word according context occur instance grammatical construction n gram theoretical analysis outlined applies general problem address specific problem object noun context verb noun direct object problem seen learning joint distribution pair large sample pair pair coordinate come large set preexisting topological metric structure training data sequence drawn pair learning perspective problem fall unsupervised supervised learning unsupervised learning goal learn underlying distribution data contrast unsupervised learning setting object involved internal structure attribute allowing compared information object statistic joint appearance statistic weak form object labelling analogous supervision cluster based distributional similarity interesting seen mean summarizing joint distribution particular like find set cluster conditional distribution decomposed membership probability conditional probability given centroid distribution cluster decomposition written symmetric form asassuming coincide basic clustering model determine decomposition need solve connected problem finding find suitable form cluster membership centroid distribution maximizing goodness fit model distribution observed data fit determined model likelihood observation maximum likelihood estimation principle natural tool determine centroid distribution membership probability determined relevant measure object cluster similarity present work relative entropy object cluster centroid distribution information available membership determined maximizing configuration entropy subject fixed average distortion maximum entropy membership distribution estimation equivalent minimization average distortion data combined entropy maximization entropy distortion minimization carried stage iterative process similar method stage iteration maximum likelihood minimum distortion estimation cluster centroid given fixed membership probability second iteration stage entropy membership distribution maximized fixed average distortion joint optimization search saddle point distortion entropy parameter equivalent minimizing linear combination known free energy statistical mechanic analogy statistical mechanic coincidental provide better understanding clustering procedure maximum likelihood argument start estimating likelihood sequence independent observation pair sequence model log likelihood isfixing number cluster model size want maximize respect distribution variation respect distribution iswith kept normalized bayes formula haveorfor substitute obtainsince expression useful cluster distribution exponential form provided step described point need specify clustering model detail",
        "derivation treated corresponding cluster verb noun verb noun association principle symmetric model accurate paper concentrate asymmetric model cluster membership associated component joint distribution cluster centroid specified component particular model use experiment noun cluster cluster membership determined centroid distribution determined asymmetric model simplifies estimation dealing single component disadvantage joint distribution different consistent expression term asymmetric model coordinate variation equation independent treat fixed average distortion cluster centroid distribution data find cluster membership probability bayes inverse maximize entropy cluster distribution membership distribution obtained look maximize log likelihood turn value minimize average distortion asymmetric cluster model data similarity measure noun cluster centroid average cluster distortion isif maximize cluster membership entropysubject normalization fixed obtain following standard exponential form class membership normalization sum partition function need symmetric derivation distribution related bayes rule log likelihood variation use assumption asymmetric model cluster membership stay fixed adjust centroid variation eqn included variation large sample replace sum observation average applying bayes rule becomesat log likelihood maximum variation vanish use relative entropy similarity measure make vanish maximum log likelihood maximized minimizing average distortion respect class centroid class membership kept fixedor inner sum vanishwe minimization relative entropy yield natural expression cluster centroidsto minimize average distortion observe variation distance noun centroid distribution respect centroid distribution centroid distribution normalized lagrange given bysubstituting expression obtainsince independent obtain desired centroid expression desired weighted average noun distribution variation vanishes centroid distribution given follows combined minimum distortion maximum entropy optimization equivalent minimization single function free energywhere average distortion cluster membership entropy free energy determines distortion membership entropy throughwith temperature important property free energy minimum determines balance disordering maximum entropy ordering distortion minimization system found fact probability find system given configuration exponential fso system found minimal free energy configuration analogy statistical mechanic",
        "suggests deterministic annealing procedure clustering rose number cluster determined sequence phase transition increasing parameter following annealing schedule higher local influence noun definition centroid dissimilarity play role distortion scale parameter close dissimilarity irrelevant word contribute centroid lowest average distortion solution involves cluster average word density increased point phase transition reached natural solution involves distinct centroid original cluster split new cluster general cluster twin centroid small random pertubation critical split membership centroid reestimation procedure given equation converge cluster critical value centroid diverge giving rise daughter clustering procedure follows start low single cluster centroid average noun distribution given current set leaf cluster corresponding current free energy local minimum refine solution search lowest critical value current leaf cluster split split critical value practical performance numerical accuracy reason split new critical point splitting procedure repeated achieve desired number cluster model cross entropy",
        "cluster based distributional similarity interesting seen mean summarizing joint distribution particular like find set cluster conditional distribution decomposed membership probability conditional probability given centroid distribution cluster decomposition written symmetric form asassuming coincide basic clustering model determine decomposition need solve connected problem finding find suitable form cluster membership centroid distribution maximizing goodness fit model distribution observed data fit determined model likelihood observation maximum likelihood estimation principle natural tool determine centroid distribution membership probability determined relevant measure object cluster similarity present work relative entropy object cluster centroid distribution information available membership determined maximizing configuration entropy subject fixed average distortion maximum entropy membership distribution estimation equivalent minimization average distortion data combined entropy maximization entropy distortion minimization carried stage iterative process similar method stage iteration maximum likelihood minimum distortion estimation cluster centroid given fixed membership probability second iteration stage entropy membership distribution maximized fixed average distortion joint optimization search saddle point distortion entropy parameter equivalent minimizing linear combination known free energy statistical mechanic analogy statistical mechanic coincidental provide better understanding clustering procedure maximum likelihood argument start estimating likelihood sequence independent observation pair sequence model log likelihood isfixing number cluster model size want maximize respect distribution variation respect distribution iswith kept normalized bayes formula haveorfor substitute obtainsince expression useful cluster distribution exponential form provided step described point need specify clustering model detail derivation treated corresponding cluster verb noun verb noun association principle symmetric model accurate paper concentrate asymmetric model cluster membership associated component joint distribution cluster centroid specified component particular model use experiment noun cluster cluster membership determined centroid distribution determined asymmetric model simplifies estimation dealing single component disadvantage joint distribution different consistent expression term asymmetric model coordinate variation equation independent treat fixed average distortion cluster centroid distribution data find cluster membership probability bayes inverse maximize entropy cluster distribution membership distribution obtained look maximize log likelihood turn",
        "value minimize average distortion asymmetric cluster model data similarity measure noun cluster centroid average cluster distortion isif maximize cluster membership entropysubject normalization fixed obtain following standard exponential form class membership normalization sum partition function need symmetric derivation distribution related bayes rule log likelihood variation use assumption asymmetric model cluster membership stay fixed adjust centroid variation eqn included variation large sample replace sum observation average applying bayes rule becomesat log likelihood maximum variation vanish use relative entropy similarity measure make vanish maximum log likelihood maximized minimizing average distortion respect class centroid class membership kept fixedor inner sum vanishwe minimization relative entropy yield natural expression cluster centroidsto minimize average distortion observe variation distance noun centroid distribution respect centroid distribution centroid distribution normalized lagrange given bysubstituting expression obtainsince independent obtain desired centroid expression desired weighted average noun distribution variation vanishes centroid distribution given follows combined minimum distortion maximum entropy optimization equivalent minimization single function free energywhere average distortion cluster membership entropy free energy determines distortion membership entropy throughwith temperature important property free energy minimum determines balance disordering maximum entropy ordering distortion minimization system found fact probability find system given configuration exponential fso system found minimal free energy configuration",
        "maximum likelihood argument start estimating likelihood sequence independent observation pair sequence model log likelihood isfixing number cluster model size want maximize respect distribution variation respect distribution iswith kept normalized bayes formula haveorfor substitute obtainsince expression useful cluster distribution exponential form provided step described point need specify clustering model detail derivation treated corresponding cluster verb noun verb noun association principle symmetric model accurate paper concentrate asymmetric model cluster membership associated component joint distribution cluster centroid specified component particular model use experiment noun cluster cluster membership determined centroid distribution determined asymmetric model simplifies estimation dealing single component disadvantage joint distribution different consistent expression term asymmetric model coordinate",
        "variation equation independent treat fixed average distortion cluster centroid distribution data find cluster membership probability bayes inverse maximize entropy cluster distribution membership distribution obtained look maximize log likelihood turn value minimize average distortion asymmetric cluster model data similarity measure noun cluster centroid average cluster distortion isif maximize cluster membership entropysubject normalization fixed obtain following standard exponential form class membership normalization sum partition function need symmetric derivation distribution related bayes rule log likelihood variation use assumption asymmetric model cluster membership stay fixed adjust centroid variation eqn included variation large sample replace sum observation average applying bayes rule becomesat log likelihood maximum variation vanish use relative entropy similarity measure make vanish maximum log likelihood maximized minimizing average distortion respect class centroid class membership kept fixedor inner sum vanish",
        "analogy statistical mechanic suggests deterministic annealing procedure clustering rose number cluster determined sequence phase transition increasing parameter following annealing schedule higher local influence noun definition centroid dissimilarity play role distortion scale parameter close dissimilarity irrelevant word contribute centroid lowest average distortion solution involves cluster average word density increased point phase transition reached natural solution involves distinct centroid original cluster split new cluster general cluster twin centroid small random pertubation critical split membership centroid reestimation procedure given equation converge cluster critical value centroid diverge giving rise daughter clustering procedure follows start low single cluster centroid average noun distribution given current set leaf cluster corresponding current free energy local minimum refine solution search lowest critical value current leaf cluster split split critical value practical performance numerical accuracy reason split new critical point splitting procedure repeated achieve desired number cluster model cross entropy",
        "experiment involve asymmetric model described previous section explained clustering procedure yield value set cluster minimizing free energy asymmetric model estimate conditional verb distribution noun bywhere depends experiment method classify noun appearing head direct object verb fire year associated press newswire corpus chosen noun appear direct object head total distinct verb noun represented density verb show word similar cluster centroid cluster resulting cluster split seen split separate object corresponding weaponry sense fire cluster one corresponding personnel action cluster second split refines weaponry sense projectile sense cluster gun sense cluster split sharp distinguishing context occur corpus show closest noun centroid set hierarchical cluster derived verb object pair involving frequent noun june electronic version encyclopedia word",
        "preceding qualitative discussion provides indication aspect distributional relationship discovered clustering need evaluate clustering basis model distributional relationship looked kind measurement model quality relative entropy held data asymmetric model andperformance task deciding verb likely given noun direct object data relating verb noun witheld training data evaluation described performed largest data set worked extracted word associated press newswire pattern matching technique mentioned collection process yielded verb object pair selected subset involving frequent noun corpus clustering divided training set pair test set pair plot average relative entropy data set asymmetric clustered model different size given relative frequency distribution verb taking direct object test set critical value relative entropy respect asymmetric model based training set set train selected held test set set test held data noun clustered set new training set relative entropy decrease test set relative entropy decrease minimum cluster start increasing suggesting larger model overtrained new noun test set intended test cluster based frequent noun useful classifier selectional property noun general figure show cluster model provides bit information selectional property new noun overtraining effect sharper held data involving clustered noun evaluated asymmetric cluster model verb decision task closer possible application disambiguation language analysis task consists judging verb given noun object occurrence training set deleted test evaluates model reconstruct missing data verb distribution cluster centroid close data test built training data previous following way based suggestion dagan small number pair frequent verb occurrence picked occurrence pair training set deleted resulting training set build sequence cluster model model decide verb appear noun data deleted training set decision compared corresponding one derived original event frequency initial data set deleted pair verb occurred initial data half compared sign initial data set error rate model proportion sign disagreement selected triple show error rate model selected",
        "figure plot average relative entropy data set asymmetric clustered model different size given relative frequency distribution verb taking direct object test set critical value relative entropy respect asymmetric model based training set set train selected held test set set test held data noun clustered set new training set relative entropy decrease test set relative entropy decrease minimum cluster start increasing suggesting larger model overtrained new noun test set intended test cluster based frequent noun useful classifier selectional property noun general figure show cluster model provides bit information selectional property new noun overtraining effect sharper held data involving clustered noun",
        "evaluated asymmetric cluster model verb decision task closer possible application disambiguation language analysis task consists judging verb given noun object occurrence training set deleted test evaluates model reconstruct missing data verb distribution cluster centroid close data test built training data previous following way based suggestion dagan small number pair frequent verb occurrence picked occurrence pair training set deleted resulting training set build sequence cluster model model decide verb appear noun data deleted training set decision compared corresponding one derived original event frequency initial data set deleted pair verb occurred initial data half compared sign initial data set error rate model proportion sign disagreement selected triple show error rate model selected exceptional triple log frequency ratio differs log marginal frequency ratio word exceptional case prediction based marginal frequency initial cluster model represents wrong overtraining largest model considered exceptional verb",
        "demonstrated general divisive clustering procedure probability distribution group word according participation particular grammatical relation word resulting cluster informative construct class based word coocurrence model substantial predictive power cluster derived proposed method case significant intuition need grounded rigorous assessment addition predictive power evaluation kind carried worth comparing derived cluster human judgement suitable experimental setting direction class based language model plan consider additional distributional relation instance adjective noun apply result clustering grouping lexical association lexicalized grammar framework stochastic lexicalized tree adjoining grammar schabes",
        "certain researcher psycholinguistic community pritchett gorrell press argued binary distinction distinct type garden path sentence garden path ambiguous sentence rise reanalysis detectable cause conscious sensation difficulty surprise effect garden path hand cause reanalysis detectable noticed speaker hearer binary distinction motivate level architecture human syntactic processing system core parser performs standard attachment able reanalyse easy case reaching hurt assistance higher level resolver use abney terminology abney abney required solve difficult case reaching melted core parser subject number computational implementation including marcus deterministic parser description theory d theory marcus abney licensing based model abney abney subject number psycholinguistic study theoretical level pritchett gorrell press implementation described paper based recent model gorrell press model interesting allow parser employ delay tactic lookahead buffer marcus marcus waiting head phrase appear input constructing phrase abney abney pritchett processing guided principle incremental licensing state parser attempt satisfy principle grammar purpose implementation interpreted mean word attached connected phrase marker found input psychological desirability attachment model argued regard processing head final language evidence found pre head structuring inoue fodor frazier model explored milward crocker",
        "gorrell employ d theoretic device building set dominance precedence relation node set intended constrained informational monotonicity asserted set relation deleted overridden restricts constraint primary structural relation dominance precedence secondary relation thematic case dependency constrained repeated point john know truth processed complete clause built description include information verb know precedes dominates subsequent input hurt structure reanalysed asserting extra clausal node dominating embedded subject turn dominated matrix achieved adding following structural relation tree descriptionsince description processing disambiguating word hurt subset final tree description monotonicity requirement satisfied particular dominance transitive relation inheritance condition tree node inherits precedence relation ancestor statement dom prec remain true reanalysis model fail reanalyse sentence reanalysis require retraction domination relation adverbial clause ice cream",
        "gorrell proposes general principle guide initial attachment decision simplicity vacuous structure building specifies condition unconscious reanalysis occur model leaf unspecified problem system implemented particular interest problem parser decides relation add set point time disambiguating point basic framework implementation built similar tree adjoining grammar joshi lexical category associated set structural relation determine lexical subtree set subtree projection lexical category example subtree projection verb english grammar follows lex variable instantiated actual verb found input category associated list left right attachment site case correspond subject verb unified left attachment site transitive verb found input parser consults verb argument structure creates new right attachment site asserting new dominated preceded attachment performed way defined term current tree description intended denote set structural relation built point processing left attachment thought term attaching current tree description left corner projection new word right attachment corresponds attaching projection new word right corner current tree description equivalent abney attach l attach left attachment let current tree description root node subtree projection new word left attachment site identical syntactic category updated tree description unified right attachment let current tree description right attachment site subtree projection new word root identical syntactic category updated tree description unified clear simple left right attachment suffice attaching argument reanalysis allow derive reanalysis required example require mean inserting tree description require illustrated intended represent current tree description built john know truth parsed intended represent subtree description new word hurt operation tree lowering operation find node current tree description match left attachment site projection new word attache inserting root new projection place result node chosen lowered subordinated order maintain structural coherence new word attached tree lowering preceded word attached description guarantee requiring lowered node dominate word attached need ensure avoid crossing branch lowered node dominate unsaturated attachment site dangling node define accessibility tree lowering follows definition accessibility let node current tree description word attached tree accessible iff dominates dominate unsaturated attachment site",
        "tree lowering let current tree description subtree projection new word left attachment site match node accessible root node licensed grammar position occupied set local relation participates result substituting instance attachment node unified updated tree description noticed tree lowering similar spirit adjunction operation tree adjoining grammar joshi difference foot root node auxiliary tree tag corresponding lowered node node replaces syntactic category seen example model proposed node different category resulting structure licensed grammar case example point truth processed parser find accessible node match category left attachment site hurt choice local relation participates found substituted root new projection derive new relation relation found licensed verb dominates know subcategorise clause new relation added set adding subtree projection hurt set unifying left attachment site result derived structure subordinated lower clause tree lowering operation defined problem finding relation add set disambiguating point reduces search accessible node apply operation implies node exists parser given preference making requisite decision following sentence fragment example input continues verb choice node lowering experimental work type sentence intuitive preference lower attachment site binding constraint force lowering applied applied native english speaker report easier standard x bar assumption attachment post modifier derived lowering node case lowered node replacement syntactic category root foot node tag auxiliary tree noted general preference low attachment post modifier accounted principle late closure frazier rayner suggest reasonable search strategy english search set accessible node direction english algorithm constructed way lowering attempted case simple attachment fails mean argument incorporated simple attachment attached adjunct incorporated lowering capture general preference argument adjunct attachment accounted principle minimal attachment frazier rayner principle simplicity gorrell press",
        "basic framework implementation built similar tree adjoining grammar joshi lexical category associated set structural relation determine lexical subtree set subtree projection lexical category example subtree projection verb english grammar follows lex variable instantiated actual verb found input category associated list left right attachment site case correspond subject verb unified left attachment site transitive verb found input parser consults verb argument structure creates new right attachment site asserting new dominated preceded",
        "simple attachment performed way defined term current tree description intended denote set structural relation built point processing left attachment thought term attaching current tree description left corner projection new word right attachment corresponds attaching projection new word right corner current tree description equivalent abney attach l attach left attachment let current tree description root node subtree projection new word left attachment site identical syntactic category updated tree description unified right attachment let current tree description right attachment site subtree projection new word root identical syntactic category updated tree description unified",
        "clear simple left right attachment suffice attaching argument reanalysis allow derive reanalysis required example require mean inserting tree description require illustrated intended represent current tree description built john know truth parsed intended represent subtree description new word hurt operation tree lowering operation find node current tree description match left attachment site projection new word attache inserting root new projection place result node chosen lowered subordinated order maintain structural coherence new word attached tree lowering preceded word attached description guarantee requiring lowered node dominate word attached need ensure avoid crossing branch lowered node dominate unsaturated attachment site dangling node define accessibility tree lowering follows definition accessibility let node current tree description word attached tree accessible iff dominates dominate unsaturated attachment site tree lowering let current tree description subtree projection new word left attachment site match node accessible root node licensed grammar position occupied set local relation participates result substituting instance attachment node unified updated tree description noticed tree lowering similar spirit adjunction operation tree adjoining grammar joshi difference foot root node auxiliary tree tag corresponding lowered node node replaces syntactic category seen example model proposed node different category resulting structure licensed grammar case example point truth processed parser find accessible node match category left attachment site hurt choice local relation participates found substituted root new projection derive new relation relation found licensed verb dominates know subcategorise clause new relation added set adding subtree projection hurt set unifying left attachment site result derived structure subordinated lower clause tree lowering operation defined problem finding relation add set disambiguating point reduces search accessible node apply operation implies node exists parser given preference making requisite decision following sentence fragment example input continues verb choice node lowering experimental work type sentence intuitive preference lower attachment site binding constraint force lowering applied",
        "applied native english speaker report easier standard x bar assumption attachment post modifier derived lowering node case lowered node replacement syntactic category root foot node tag auxiliary tree noted general preference low attachment post modifier accounted principle late closure frazier rayner suggest reasonable search strategy english search set accessible node direction english algorithm constructed way lowering attempted case simple attachment fails mean argument incorporated simple attachment attached adjunct incorporated lowering capture general preference argument adjunct attachment accounted principle minimal attachment frazier rayner principle simplicity gorrell press",
        "japanese present challenge incremental parsing model possible determine embedded clause begin following example verb kaita wrote string interpretable clause gap meaning john wrote essay incremental parser build requisite structure appearance head noun seito student mean preceding clause reinterpreted relative clause including gap note overt relative pronoun japanese way looking happening subject john dissociated clause attached reattached main clause looking different perspective gorrell noted subject remaining main clause constituent bracketed ronbun kaita wrote essay lowered relative clause possible expect example unconscious garden path reflected intuitive data mazuka itoh press allow parser handle example expand definition tree lowering order build relative clause assert extra material including subject new node justified lexical requirement disambiguating word head noun seito involves reconstructing clausal structure dominating lowering site including asserting argument position reference verb case frame attempting attach result relative clause head noun describes minimal expulsion strategy predicts preference reanalysis expelling minimum material clause term mean assuming binary right branching clause structure verb right corner node selected lowering high possible mean search use english predict maximal expulsion strategy case assuming search post clausal noun reached input parser start search node dominating word incorporated verb relative clause mean case preference lower verb expel subject object human preference lower object verb expel subject parser second choice search strategy itoh press note example subject object expelled relative clause choice search cause conscious garden path effect example adapted mazuka itoh following order capture minimal expulsion strategy class japanese example search lowering node conducted investigating consequence changing search strategy way",
        "japanese present challenge incremental parsing model possible determine embedded clause begin following example verb kaita wrote string interpretable clause gap meaning john wrote essay incremental parser build requisite structure appearance head noun seito student mean preceding clause reinterpreted relative clause including gap note overt relative pronoun japanese way looking happening subject john dissociated clause attached reattached main clause looking different perspective gorrell noted subject remaining main clause constituent bracketed ronbun kaita wrote essay lowered relative clause possible expect example unconscious garden path reflected intuitive data mazuka itoh press allow parser handle example expand definition tree lowering order build relative clause assert extra material including subject new node justified lexical requirement disambiguating word head noun seito involves reconstructing clausal structure dominating lowering site including asserting argument position reference verb case frame attempting attach result relative clause head noun",
        "inoue describes minimal expulsion strategy predicts preference reanalysis expelling minimum material clause term mean assuming binary right branching clause structure verb right corner node selected lowering high possible mean search use english predict maximal expulsion strategy case assuming search post clausal noun reached input parser start search node dominating word incorporated verb relative clause mean case preference lower verb expel subject object human preference lower object verb expel subject parser second choice search strategy itoh press note example subject object expelled relative clause choice search cause conscious garden path effect example adapted mazuka itoh following order capture minimal expulsion strategy class japanese example search lowering node conducted investigating consequence changing search strategy way",
        "having formulated constraint gorrell model term accessibility node tree lowering model falsified find case relevant disambiguating information come point processing node required lowered accessible following pair sentence familiar psycholinguistic literature preference attaching phrase instrumental argument verb reading telescope instrument seeing assumption saw selects instrumental argument derive preference present model preference attach argument opposed adjunct constrained incrementality attachment decision preposition encountered attached preferred reading sister verb mean case acceptable reading adjunct man attachment revised adjoined relevant node preposition attached required node accessible conscious garden path effect predicted occur garden path effect preposition separated disambiguating head noun series adjective saw man neat quaint old fashioned moustache telescope result obtains particular implementational detail tree lowering return level gorrell state model attached argument verb reanalysed adjunct preceding precede reanalysis dominate reanalysis exclusivity condition tree node stand dominance precedence relation similar problem concern example following gibson gibson manipulated number agreement force low middle high attachment bracketed relative clause result line experiment low attachment corresponding easiest middle attachment corresponding difficult behaviour captured adopt search tree lowering incorporate required preference parser constraint incrementality force decision encountering mean assuming decide attach low number agreement force high attachment conscious garden path effect predicted lowering derive reanalysis true level node description precede original low position relative clause dominated subsequent high position relative clause sentence cause conscious garden path effect",
        "course developing natural language interface computational linguist position evaluating different theoretical approach analysis natural language want toevaluate improve current system add capability system combine module different system goal adding discourse component system evaluating improving place discourse module combine theory centering local focusing grosz sidner global focus grosz coherence relation hobbs event reference webber intonational structure pierrehumbert hirschberg system user belief pollack plan intent recognition production cohen allen perrault sidner israel control whittaker stenton complex syntactic structure prince evaluate relative contribution factor compare approach problem order step establishing methodology type comparison conducted case study attempt evaluate different approach anaphoric processing discourse comparing accuracy coverage published algorithm finding co specifier pronoun occurring text dialogue hobbs brennan part paper present quantitative result hand simulating algorithm hobbs algorithm brennan algorithm analysis give rise qualitative evaluation recommendation performing evaluation general illustrate general difficulty encountered quantitative evaluation problem allowing underlying assumption determining handle underspecifications andevaluating contribution false positive error chaining algorithm theory discourse posit interaction algorithm inference intentional component use reasoning tandem algorithm operation choice want able analyse performance algorithm different domain focus linguistic basis approach selectional restriction analysis independent vagary particular knowledge representation evaluating extent algorithm suffice narrow search inference component analysis give indication contribution syntactic constraint task structure global focus anaphoric processing data compare algorithm important evaluate claim generality look type input clear division textual interactive input related identical factor language analysed produced person distinction conflated textual material novel contain reported conversation person interactive dialogue task oriented master slave type expertise initiative rest person person dialogue party contribute discourse entity conversation equal basis factor interest dialogue human human human computer modality communication spoken typed researcher indicated dialogue us reference vary dimension cohen henisz thompson guindon dahlbach johnson whittaker stenton analyse performance algorithm type data sample hobbs developing algorithm excerpt novel sample journalistic writing remaining sample set human human",
        "embarking comparison convenient assume input algorithm identical compare output researcher agree phenomenon explained boundary module system case brennan centering algorithm hobbs algorithm assumption system component sense specification operation algorithm order hand simulate algorithm major set assumption based discourse segmentation syntactic representation attempt explicit algorithm pinpoint algorithm behave assumption founded addition number underspecifications description algorithm arise theory attempt categorize occurring data algorithm based prey unencountered example example brennan salience hierarchy discourse entity based grammatical relation implicit assumption utterance subject novel wheel example reported dialogue asone wonder subject mr vale case algorithm need specificied order able process data highlight algorithm need modified section general count underspecifications failure clear definition success particular clear case algorithm produce multiple partial interpretation situation system flag utterance ambiguous draw support discourse component arises present analysis reason constraint given grosz allow choose preferred interpretation brennan algorithm proposes ranked interpretation parallel happen hobbs algorithm proposes interpretation sequential manner time chose count failure situation brennan algorithm reduces number possible interpretation hobbs algorithm stop correct interpretation ignores fact hobbs rejected number interpretation stopping needed decision score algorithm find interpretation utterance human find ambiguous centering algorithm defined brennan bnf algorithm derived set rule constraint grosz grosz grosz shall reproduce algorithm brennan main structure centering algorithm backward looking center discourse ordered list looking center discourse entity available utterance pronominalization centering framework predicts local coherent stretch dialogue speaker prefer continue talking discourse entity ranked entity previous utterance forward center realized current utterance pronominalized centering framework order forward center list intended reflect salience discourse entity brennan algorithm order list grammatical relation complement main verb subject object indirect object subcategorized complement noun phrase found adjunct clause capture intuition subject",
        "salient discourse entity brennan algorithm added linguistic constraint contra indexing centering framework constraint exemplified fact sentence like entity cospecified cospecified contra indexed brennan algorithm depends semantic processing precompute constraint derived syntactic structure depend notion c command reinhart assumption dependent syntax representation discourse entity marked grammatical function realized subject brennan algorithm assumes mechanism structure written text task oriented dialogue hierarchical segment present concern grammar discourse determines structure derived cue cooperative speaker hearer aid processing centering local phenomenon intended operate segment needed deduce segmental structure order analyse data intention task structure cue word intonational property utterance coherence relation scoping modal operator mechanism shifting control discourse participant proposed way determining discourse segmentation grosz grosz sidner reichman pierrehumbert hirschberg hirschberg litman hobbs hobbs robert whittaker stenton use combination orthography anaphora distribution cue word task structure rule published text paragraph new segment sentence pronoun subject position pronoun preceding sentence internal noun phrase match syntactic feature task oriented dialogue action pick mark task boundary segment boundary word mark segment boundary co occur sufficient marking segment boundary state cospecifiers pronoun segment preferred previous segment implicit assumption line research derived sidner work local focusing initial utterance situation brennan algorithm prefer sentence noun phrase cospecifier pronoun hobbs algorithm based searching pronoun co specifier syntactic parse tree input sentence hobbs reproduce algorithm appendix example algorithm operates sentence time structure previous sentence discourse available stated term search parse tree looking intrasentential antecedent search conducted left right breadth manner looking pronoun antecedent sentence tree left pronoun failing look previous sentence assume segmentation discourse structure algorithm algorithm text find antecedent recent work hobbs us notion coherence relation structure discourse hobbs martin order hobbs algorithm traverse parse tree closest thing framework prediction discourse entity salient main prefers co specifier pronoun sentence one closer pronoun sentence amount claim",
        "different discourse entity salient depending position pronoun sentence seeking intersentential co specification hobbs algorithm search parse tree previous utterance breadth left predicts entity realized subject position salient adjunct clause precedes main subject noun phrase deeper parse tree mean object indirect object possible antecedent found general depth syntactic embedding important determiner discourse prominence assumption syntax note hobbs assumes produce correct syntactic structure utterance adjunct phrase attached proper point parse tree addition order obey linguistic constraint coreference algorithm depends existence parse tree node denotes noun phrase determiner example appendix algorithm encodes contra indexing constraint skipping node node dominates parse tree pronoun found mean guarantee contra indexed pronoun choose co specifier assumes algorithm collect discourse entity mentioned set co specifier plural anaphor discus length assumption make capability interpretive process operates algorithm hobbs includes thing able recover recoverable omitted text elided verb phrase identity speaker hearer dialogue major component discourse algorithm prediction entity salient factor contribute salience discourse entity identified prince prince brown fish hudson obvious question algorithm different prediction main difference choice co specifier pronoun hobbs algorithm depends position pronoun sentence centering framework matter criterion us order forward center list pronoun salient entity antecedent irrespective pronoun position ordering entity previous utterance varies brennan possessor come case marked object indirect object difference relevant analysis follows effect assumption measurable attempt specify effect measure effect hobbs syntax assumption difficult likely wrong parse adopt set collection assumption algorithm ability recover identity speaker hearer dialogue text algorithm analysed arthur hailey novel wheel july edition newsweek sentence wheel short simple long sequence consisting reported conversation similar conversational text article newsweek typical journalistic writing text occurrence singular plural person pronoun test performance algorithm task dialogue contain total us pronoun figure note possessive counted accusative",
        "counted performed analysis quantitative result comparison algorithm data set overall analysis data set combined revealed significant difference performance algorithm significant addition algorithm tested significant difference performance different textual type algorithm performed worse task dialogue hobbs brennan wonder confidence view number significant factor considered contribution false positive error chaining false positive algorithm get right answer wrong reason simple example phenomenon illustrated sequence task dialogue refers pump algorithm get right antecedent little handle fails brennan algorithm pump centered continues select antecedent text mean brennan get wrong co specifier error allows correct co specifier type false positive example ishobbs get correct long willing accept antecedent idiomatic use chaining refers fact algorithm make error error result example algorithm fails fail choice cospecifier following example dependent choice possible measure effect false positive sense subjective judgement measure effect error chaining reporting number correct error chaining misleading error produced error chain corrected algorithm significant improvement analysis error chain contributed failure hobbs algorithm failure brennan",
        "embarking comparison convenient assume input algorithm identical compare output researcher agree phenomenon explained boundary module system case brennan centering algorithm hobbs algorithm assumption system component sense specification operation algorithm order hand simulate algorithm major set assumption based discourse segmentation syntactic representation attempt explicit algorithm pinpoint algorithm behave assumption founded addition number underspecifications description algorithm arise theory attempt categorize occurring data algorithm based prey unencountered example example brennan salience hierarchy discourse entity based grammatical relation implicit assumption utterance subject novel wheel example reported dialogue asone wonder subject mr vale case algorithm need specificied order able process data highlight algorithm need modified section general count underspecifications failure clear definition success particular clear case algorithm produce multiple partial interpretation situation system flag utterance ambiguous draw support discourse component arises present analysis reason constraint given grosz allow choose preferred interpretation brennan algorithm proposes ranked interpretation parallel happen hobbs algorithm proposes interpretation sequential manner time chose count failure situation brennan algorithm reduces number possible interpretation hobbs algorithm stop correct interpretation ignores fact hobbs rejected number interpretation stopping needed decision score algorithm find interpretation utterance human find ambiguous centering algorithm defined brennan bnf algorithm derived set rule constraint grosz grosz grosz shall reproduce algorithm brennan main structure centering algorithm backward looking center discourse ordered list looking center discourse entity available utterance pronominalization centering framework predicts local coherent stretch dialogue speaker prefer continue talking discourse entity ranked entity previous utterance forward center realized current utterance pronominalized centering framework order forward center list intended reflect salience discourse entity brennan algorithm order list grammatical relation complement main verb subject object indirect object subcategorized complement noun phrase found adjunct clause capture intuition subject",
        "salient discourse entity brennan algorithm added linguistic constraint contra indexing centering framework constraint exemplified fact sentence like entity cospecified cospecified contra indexed brennan algorithm depends semantic processing precompute constraint derived syntactic structure depend notion c command reinhart assumption dependent syntax representation discourse entity marked grammatical function realized subject brennan algorithm assumes mechanism structure written text task oriented dialogue hierarchical segment present concern grammar discourse determines structure derived cue cooperative speaker hearer aid processing centering local phenomenon intended operate segment needed deduce segmental structure order analyse data intention task structure cue word intonational property utterance coherence relation scoping modal operator mechanism shifting control discourse participant proposed way determining discourse segmentation grosz grosz sidner reichman pierrehumbert hirschberg hirschberg litman hobbs hobbs robert whittaker stenton use combination orthography anaphora distribution cue word task structure rule published text paragraph new segment sentence pronoun subject position pronoun preceding sentence internal noun phrase match syntactic feature task oriented dialogue action pick mark task boundary segment boundary word mark segment boundary co occur sufficient marking segment boundary state cospecifiers pronoun segment preferred previous segment implicit assumption line research derived sidner work local focusing initial utterance situation brennan algorithm prefer sentence noun phrase cospecifier pronoun hobbs algorithm based searching pronoun co specifier syntactic parse tree input sentence hobbs reproduce algorithm appendix example algorithm operates sentence time structure previous sentence discourse available stated term search parse tree looking intrasentential antecedent search conducted left right breadth manner looking pronoun antecedent sentence tree left pronoun failing look previous sentence assume segmentation discourse structure algorithm algorithm text find antecedent recent work hobbs us notion coherence relation structure discourse hobbs martin order hobbs algorithm traverse parse tree closest thing framework prediction discourse entity salient main prefers co specifier pronoun sentence one closer pronoun sentence amount claim",
        "different discourse entity salient depending position pronoun sentence seeking intersentential co specification hobbs algorithm search parse tree previous utterance breadth left predicts entity realized subject position salient adjunct clause precedes main subject noun phrase deeper parse tree mean object indirect object possible antecedent found general depth syntactic embedding important determiner discourse prominence assumption syntax note hobbs assumes produce correct syntactic structure utterance adjunct phrase attached proper point parse tree addition order obey linguistic constraint coreference algorithm depends existence parse tree node denotes noun phrase determiner example appendix algorithm encodes contra indexing constraint skipping node node dominates parse tree pronoun found mean guarantee contra indexed pronoun choose co specifier assumes algorithm collect discourse entity mentioned set co specifier plural anaphor discus length assumption make capability interpretive process operates algorithm hobbs includes thing able recover recoverable omitted text elided verb phrase identity speaker hearer dialogue major component discourse algorithm prediction entity salient factor contribute salience discourse entity identified prince prince brown fish hudson obvious question algorithm different prediction main difference choice co specifier pronoun hobbs algorithm depends position pronoun sentence centering framework matter criterion us order forward center list pronoun salient entity antecedent irrespective pronoun position ordering entity previous utterance varies brennan possessor come case marked object indirect object difference relevant analysis follows effect assumption measurable attempt specify effect measure effect hobbs syntax assumption difficult likely wrong parse adopt set collection assumption algorithm ability recover identity speaker hearer dialogue",
        "centering algorithm defined brennan bnf algorithm derived set rule constraint grosz grosz grosz shall reproduce algorithm brennan main structure centering algorithm backward looking center discourse ordered list looking center discourse entity available utterance pronominalization centering framework predicts local coherent stretch dialogue speaker prefer continue talking discourse entity ranked entity previous utterance forward center realized current utterance pronominalized centering framework order forward center list intended reflect salience discourse entity brennan algorithm order list grammatical relation complement main verb subject object indirect object subcategorized complement noun phrase found adjunct clause capture intuition subject salient discourse entity brennan algorithm added linguistic constraint contra indexing centering framework constraint exemplified fact sentence like entity cospecified cospecified contra indexed brennan algorithm depends semantic processing precompute constraint derived syntactic structure depend notion c command reinhart assumption dependent syntax representation discourse entity marked grammatical function realized subject brennan algorithm assumes mechanism structure written text task oriented dialogue hierarchical segment present concern grammar discourse determines structure derived cue cooperative speaker hearer aid processing centering local phenomenon intended operate segment needed deduce segmental structure order analyse data intention task structure cue word intonational property utterance coherence relation scoping modal operator mechanism shifting control discourse participant proposed way determining discourse segmentation grosz grosz sidner reichman pierrehumbert hirschberg hirschberg litman hobbs hobbs robert whittaker stenton use combination orthography anaphora distribution cue word task structure rule published text paragraph new segment sentence pronoun subject position pronoun preceding sentence internal noun phrase match syntactic feature task oriented dialogue action pick mark task boundary segment boundary word mark segment boundary co occur sufficient marking segment boundary state cospecifiers pronoun segment preferred previous segment implicit assumption line research derived sidner work local focusing initial utterance situation brennan algorithm prefer sentence noun phrase cospecifier pronoun",
        "hobbs algorithm based searching pronoun co specifier syntactic parse tree input sentence hobbs reproduce algorithm appendix example algorithm operates sentence time structure previous sentence discourse available stated term search parse tree looking intrasentential antecedent search conducted left right breadth manner looking pronoun antecedent sentence tree left pronoun failing look previous sentence assume segmentation discourse structure algorithm algorithm text find antecedent recent work hobbs us notion coherence relation structure discourse hobbs martin order hobbs algorithm traverse parse tree closest thing framework prediction discourse entity salient main prefers co specifier pronoun sentence one closer pronoun sentence amount claim different discourse entity salient depending position pronoun sentence seeking intersentential co specification hobbs algorithm search parse tree previous utterance breadth left predicts entity realized subject position salient adjunct clause precedes main subject noun phrase deeper parse tree mean object indirect object possible antecedent found general depth syntactic embedding important determiner discourse prominence assumption syntax note hobbs assumes produce correct syntactic structure utterance adjunct phrase attached proper point parse tree addition order obey linguistic constraint coreference algorithm depends existence parse tree node denotes noun phrase determiner example appendix algorithm encodes contra indexing constraint skipping node node dominates parse tree pronoun found mean guarantee contra indexed pronoun choose co specifier assumes algorithm collect discourse entity mentioned set co specifier plural anaphor discus length assumption make capability interpretive process operates algorithm hobbs includes thing able recover recoverable omitted text elided verb phrase identity speaker hearer dialogue",
        "major component discourse algorithm prediction entity salient factor contribute salience discourse entity identified prince prince brown fish hudson obvious question algorithm different prediction main difference choice co specifier pronoun hobbs algorithm depends position pronoun sentence centering framework matter criterion us order forward center list pronoun salient entity antecedent irrespective pronoun position ordering entity previous utterance varies brennan possessor come case marked object indirect object difference relevant analysis follows effect assumption measurable attempt specify effect measure effect hobbs syntax assumption difficult likely wrong parse adopt set collection assumption algorithm ability recover identity speaker hearer dialogue",
        "text algorithm analysed arthur hailey novel wheel july edition newsweek sentence wheel short simple long sequence consisting reported conversation similar conversational text article newsweek typical journalistic writing text occurrence singular plural person pronoun test performance algorithm task dialogue contain total us pronoun figure note possessive counted accusative counted performed analysis quantitative result comparison algorithm data set overall analysis data set combined revealed significant difference performance algorithm significant addition algorithm tested significant difference performance different textual type algorithm performed worse task dialogue hobbs brennan wonder confidence view number significant factor considered contribution false positive error chaining false positive algorithm get right answer wrong reason simple example phenomenon illustrated sequence task dialogue refers pump algorithm get right antecedent little handle fails brennan algorithm pump centered continues select antecedent text mean brennan get wrong co specifier error allows correct co specifier type false positive example ishobbs get correct long willing accept antecedent idiomatic use chaining refers fact algorithm make error error result example algorithm fails fail choice cospecifier following example dependent choice possible measure effect false positive sense subjective judgement measure effect error chaining reporting number correct error chaining misleading error produced error chain corrected algorithm significant improvement analysis error chain contributed failure hobbs algorithm failure brennan",
        "number presented previous section unsatisfying tell make algorithm general improved addition given assumption needed order produce wonder extent data result assumption fails indicate algorithm missed example covering different set phenomenon relative distribution success failure having hand simulation order produce number information available section discus relative importance factor producing number discus algorithm modified flexibility framework allowing modification important dimension evaluation figure pronominal category distribution success failure algorithm main purpose evaluation improve theory evaluating interesting case one algorithm performance varies algorithm get correct discus wheel data example rest assumption identity speaker hearer recoverable example example rest able produce collection discourse entity occurred explicit instruction hearer produce collection phrase case hobbs get brennan class stand case relevant factor hobbs preference intrasentential co specifier class exemplified byall involved preposition descriptive adjunct noun phrase adjunct common visual description found data task dialogue quick inspection grosz task oriented dialogue revealed grosz class possessive case possessive co specified subject sentence relative clause co specified subject clause case marked subject matching construction link clause us s s case inwe subject matching example inas sentential complement asthe fact marked significant term possible effect noted definition success section favor hobbs hobbs algorithm choose end antecedent second brennan algorithm hand interpretation second co specifies red piece co specifies end continuing interpretation co specifies constraint choice example brennan succeed hobbs fails extended discussion discourse entity instance example hobbs fails choosing co specifier rubber ring segment blue cap example novel wheel given hobbs get use miss result missing second choosing housekeeper co specifier executive vice president centered sentence continued",
        "following sentence brennan algorithm choose cospecifier example algorithm get example task dialogue referring global focus pump case shift global focus marked cue word marked case felicitous pump salient global focus case pronominal reference entity introduced rare example implicit reference problem pump working example sentential anaphora asneither hobbs algorithm brennan attempt cover example example us lexicalized certain verb imagine treated phrasal lexical item handled anaphoric processing component abeille schabes interchange task dialogue consist client responding command cue ready let expert know completed task party contribute discourse entity common ground algorithm fail claim contraindexed resolved hole noun phrase prepositional phrase modify hole theory contra indexing questionable main factor question little red piece focus question focus despite fact syntactic construction focus hole green plunger sidner example suggest questioned entity left focused point dialogue question resolved fact noted marker response question support analysis schiffrin relevant factor switching control discourse participant whittaker stenton mixed initiative feature sequence different text structure pump dialogue important factor relates use global focus case algorithm fail reference pump global focus include global focus centering framework separate notion current mean case shift global focus marked cue word segment rule allow brennan global focus example add end forward center list sidner algorithm local focusing sidner let brennan example event anaphora discus fact algorithm modified event anaphora hobbs interesting fact case hobbs algorithm get correct co specifier brennan relevant factor hobbs preference intrasentential co specifier view case discourse anaphora principled way distinction carter proposed extension sidner algorithm local focusing relevant carter argues intra sentential candidate iscs preferred candidate previous utterance case discourse center established discourse center rejected syntactic selectional",
        "reason us hobbs algorithm produce ordering iscs compatible centering framework underspecified choose establish discourse center co specifier previous utterance adopt carter rule centering framework find case hobbs get brennan case discourse center established current center rejected basis syntactic sortal information carter rule get rest want establish discourse entity previous utterance addition constraint allow brennan example algorithm got combination way making best algorithm addition modification change quantitative result figure statistical analysis significant difference performance algorithm general case performance algorithm varies depending data significant difference result modification brennan algorithm performs better pump dialogue",
        "figure pronominal category distribution success failure algorithm main purpose evaluation improve theory evaluating interesting case one algorithm performance varies algorithm get correct discus wheel data example rest assumption identity speaker hearer recoverable example example rest able produce collection discourse entity occurred explicit instruction hearer produce collection phrase case hobbs get brennan class stand case relevant factor hobbs preference intrasentential co specifier class exemplified byall involved preposition descriptive adjunct noun phrase adjunct common visual description found data task dialogue quick inspection grosz task oriented dialogue revealed grosz class possessive case possessive co specified subject sentence relative clause co specified subject clause case marked subject matching construction link clause us s s case inwe subject matching example inas sentential complement asthe fact marked significant term possible effect noted definition success section favor hobbs hobbs algorithm choose end antecedent second brennan algorithm hand interpretation second co specifies red piece co specifies end continuing interpretation co specifies constraint choice example brennan succeed hobbs fails extended discussion discourse entity instance example hobbs fails choosing co specifier rubber ring segment blue cap example novel wheel given hobbs get use miss result missing second choosing housekeeper co specifier executive vice president centered sentence continued following sentence brennan algorithm choose cospecifier example algorithm get example task dialogue referring global focus pump case shift global focus marked cue word marked case felicitous pump salient global focus case pronominal reference entity introduced rare example implicit reference problem pump working example sentential anaphora asneither hobbs algorithm brennan attempt cover example example us lexicalized certain verb imagine",
        "treated phrasal lexical item handled anaphoric processing component abeille schabes interchange task dialogue consist client responding command cue ready let expert know completed task party contribute discourse entity common ground algorithm fail claim contraindexed resolved hole noun phrase prepositional phrase modify hole theory contra indexing questionable main factor question little red piece focus question focus despite fact syntactic construction focus hole green plunger sidner example suggest questioned entity left focused point dialogue question resolved fact noted marker response question support analysis schiffrin relevant factor switching control discourse participant whittaker stenton mixed initiative feature sequence different text",
        "case hobbs get brennan class stand case relevant factor hobbs preference intrasentential co specifier class exemplified byall involved preposition descriptive adjunct noun phrase adjunct common visual description found data task dialogue quick inspection grosz task oriented dialogue revealed grosz class possessive case possessive co specified subject sentence relative clause co specified subject clause case marked subject matching construction link clause us s s case inwe subject matching example inas sentential complement asthe fact marked significant term possible effect noted definition success section favor hobbs hobbs algorithm choose end antecedent second brennan algorithm hand interpretation second co specifies red piece co specifies end continuing interpretation co specifies constraint choice",
        "example algorithm get example task dialogue referring global focus pump case shift global focus marked cue word marked case felicitous pump salient global focus case pronominal reference entity introduced rare example implicit reference problem pump working example sentential anaphora asneither hobbs algorithm brennan attempt cover example example us lexicalized certain verb imagine treated phrasal lexical item handled anaphoric processing component abeille schabes interchange task dialogue consist client responding command cue ready let expert know completed task party contribute discourse entity common ground algorithm fail claim contraindexed resolved hole noun phrase prepositional phrase modify hole theory contra indexing questionable main factor question little red piece focus question focus despite fact syntactic construction focus hole green plunger sidner example suggest questioned entity left focused point dialogue question resolved fact noted marker response question support analysis schiffrin relevant factor switching control discourse participant whittaker stenton mixed initiative feature sequence different text",
        "task structure pump dialogue important factor relates use global focus case algorithm fail reference pump global focus include global focus centering framework separate notion current mean case shift global focus marked cue word segment rule allow brennan global focus example add end forward center list sidner algorithm local focusing sidner let brennan example event anaphora discus fact algorithm modified event anaphora hobbs interesting fact case hobbs algorithm get correct co specifier brennan relevant factor hobbs preference intrasentential co specifier view case discourse anaphora principled way distinction carter proposed extension sidner algorithm local focusing relevant carter argues intra sentential candidate iscs preferred candidate previous utterance case discourse center established discourse center rejected syntactic selectional reason us hobbs algorithm produce ordering iscs compatible centering framework underspecified choose establish discourse center co specifier previous utterance adopt carter rule centering framework find case hobbs get brennan case discourse center established current center rejected basis syntactic sortal information carter rule get rest want establish discourse entity previous utterance addition constraint allow brennan example algorithm got combination way making best algorithm addition modification change quantitative result figure statistical analysis significant difference performance algorithm general case performance algorithm varies depending data significant difference result modification brennan algorithm performs better pump dialogue",
        "benefit way performing evaluation general result methodology evaluation discover way improve current theory split evaluation effort quantitative qualitative incoherent trust result quantitative evaluation considerable qualitative analysis perform qualitative analysis component significant contribution quantitative result need able measure effect factor measurement comparison data level term general result identified factor evaluation type complicated lead evaluate quantitative result care decide evaluate underspecifications contribution assumption andto determine effect false positive error chaining advocate approach contribution underspecification assumption tabulated effect error chain principled way found identify false positive effect reported quantitative evaluation addition taken step determining relative importance different factor successful operation discourse module percent success algorithm indicates syntax strong influence reduce inference required case algorithm correct result mean large number case potential conflict co specifier addition analysis shown task oriented dialogue global focus significant factor general discourse structure important task dialogue simple device cue word way determining structure note evaluation allows determine generality approach performance hobbs brennan varies according type text fact worse task dialogue text question performance vary input annotated corpus comprising input type discussed introduction way giving basis evaluate generality theory",
        "algorithm example reproduced denotes noun phrase denotes sentence node dominating pronoun parse tree tree encounter node node path reach branch node left path left right breadth fashion antecedent node encountered node path highest node sentence continue step traverse surface parse tree previous sentence text reverse chronological order acceptable antecedent found tree traversed left right breadth manner node encountered proposed antecedent node tree node encountered new node path traversed reach node path pas node dominates propose antecedent branch node left path left right breadth manner node encountered node encountered antecedent step purpose step observe contra indexing constraint consider simple conversational sequence trying find antecedent second utterance algorithm step step parse tree figure starting point step called mark path dotted line traverse left encounter node mean contra indexed structure corresponded craige mom like craige left node craige selected antecedent node highest node previous sentence traverse tree encounter lyn mom proposed antecedent",
        "purpose paper develop principled technique attaching probabilistic interpretation feature structure technique apply feature structure described carpenter structure one pollard sag relevance computational grammar apparent basis usefulness probabilistic context free grammar charniak plausible assume extension probabilistic technique structure allow application known new technique parse ranking grammar induction interesting grammar case paper structured follows start reviewing training use probabilistic context free grammar pcfgs develop technique allow analogous probabilistic annotation type hierarchy give clear account relationship large class feature structure probability treat entrancy conclude sketching technique treat structure know previous work associate score feature structure kim aware previous treatment make explicit link classical probability theory unconventional perspective feature structure easier cast theory general framework incremental description refinement mellish exploit usual metaphor constraint based grammar fact afford remain agnostic mean hpsg grammar associate sign linguistic string need order train stochastic procedure corpus sign known valid description string",
        "review standard probabilistic interpretation pcfg tuple set terminal symbol set non terminal symbol starting symbol set rule form string terminal non terminal rule probability probability rule expand given non terminal sum associate probability partial phrase marker set terminal non terminal node generated beginning starting node expanding non terminal leaf partial tree marker partial phrase marker non terminal leaf assigned following inductive definition partial phrase marker partial phrase marker differs single non terminal node expanded definition act specification accessibility relationship hold node tree admitted grammar rule probability specify cost making particular choice way rule develop going turn analogous system accessibility relation present probabilistic type hierarchy define definition pcfgs implies probability phrase marker depends choice rule expanding non terminal node particular probability depend order rule applied unwelcome consequence pcfgs unable certain discrimination tree differ configuration model developed paper build similar independence assumption large art probabilistic language modelling resides management trade descriptive power merit allowing discrimination want independence assumption merit making training practical allowing treat similar situation equivalent crucial advantage pcfgs cfgs trained learned corpus fact unfamiliar referred charniak textbook charniak space recapitulate discussion training found illustrate outcome training simple grammar figure training corpus figure plural sentence singular sentence optimal set parameter reflect distribution found corpus shown figure hoped ratio consequence assumption independence algorithm ascribing difference distribution singular plural sentence joint effect independent decision like recognize independent decision effect grammar mean enforcing number agreement system prefers plural singular lead agreement clash bus stop estimated bus stop bus stop probability bus stop probability behaviour unmotivated corpus arises inadequacy probabilistic model",
        "definition pcfgs implies probability phrase marker depends choice rule expanding non terminal node particular probability depend order rule applied unwelcome consequence pcfgs unable certain discrimination tree differ configuration model developed paper build similar independence assumption large art probabilistic language modelling resides management trade descriptive power merit allowing discrimination want independence assumption merit making training practical allowing treat similar situation equivalent crucial advantage pcfgs cfgs trained learned corpus fact unfamiliar referred charniak textbook charniak space recapitulate discussion training found illustrate outcome training",
        "consider simple grammar figure training corpus figure plural sentence singular sentence optimal set parameter reflect distribution found corpus shown figure hoped ratio consequence assumption independence algorithm ascribing difference distribution singular plural sentence joint effect independent decision like recognize independent decision effect grammar mean enforcing number agreement system prefers plural singular lead agreement clash bus stop estimated bus stop bus stop probability bus stop probability behaviour unmotivated corpus arises inadequacy probabilistic model",
        "carpenter ale carpenter allows user define type hierarchy grammar writing collection clause denote inheritance hierarchy set feature set appropriateness condition example hierarchy given ale syntax figure inheritance information tell sign forced choice sentence phrase phrase forced choice noun phrase verb phrase number value num partitioned singular sing plural feature defined left num appropriateness information say feature num introduces new instance type num phrase left right introduce sentence parallel make possible apply pcfg training scheme unchanged sub type given super type partition feature structure type way different rule expand given non terminal pcfg partition space tree topmost node feature defined hierarchy act accessibility relation node way purpose equivalent way right hand side rule introduce new node partial phrase marker hierarchy figure related isomorphic grammar figure difference num introduced feature hierarchy present original grammar difference use left right model dominance relationship node",
        "parallel make possible apply pcfg training scheme unchanged sub type given super type partition feature structure type way different rule expand given non terminal pcfg partition space tree topmost node feature defined hierarchy act accessibility relation node way purpose equivalent way right hand side rule introduce new node partial phrase marker hierarchy figure related isomorphic grammar figure difference num introduced feature hierarchy present original grammar difference use left right model dominance relationship node",
        "purpose probabilistic type hierarchy pth tuplewhere set maximal type set non maximal type starting symbol set introduction relationship form multiset maximal non maximal type introduction relationship probability probability introduction relationship apply given non maximal type sum thing stand definition isomorphic given pcfgs major difference change rule introduction relationship relax stipulation item right hand rule string allowing multisets introduce additional term head introduction rule signal fact apply particular introduction relationship node specialize type node picking direct subtypes current type need deal case non maximal achieved defining iterated introduction relationship corresponding chain introduction relationship refine type maximal type probabilistic type hierarchy iterated introduction relationship correspond context free rewrite rule pcfg useful effect preserve invariant type fringe structure maximal hierarchy ale syntax given figure captured new notation figure associate probability feature structure set maximal non maximal node generated beginning starting node expanding non maximal leaf partial tree specified feature structure feature structure maximal leaf assigned following inductive definition feature structure partial feature structure differs single non maximal node type refined type expanded notation definition identical given pcfgs correspondence definition pth pcfg apparent training method apply provide example treated crucial matter entrancy inappropriate stochastic hpsg refer stochastic hierarchy figure analysis sentence figure figure matter counting transition found observed result count refine initial estimate probability particular transition analogous went pcfgs result training identical given optimal assignment shown figure point provided system allows use feature structure dealt question entrancy form crucial expressive power typed feature structure return consider detailed implication similarity result figurewe model distribution observed corpus assuming independent decision strange ranking par favour number disagreement spite fact grammar generated corpus enforces number agreement difference result earlier one hierarchy us bot start symbol probability tell corpus contains free standing structure type num",
        "probability ofcodifies similar observation free standing structure type phrase item type phrase introduced type form sub type transition phrase corpus initial estimate probability transition unaffected training pcfg symmetry expansion singular plural variant implicit pth distribution singular plural variant encoded single location num refined independence assumption built training algorithm type refined according probability distribution irrespective context expanded seen consequence pth lump occasion num expanded enclosing context moment prepared tolerate clarity decision lead system clear probabilistic semantics number parameter estimated grammar linear function size type hierarchyeasy extensibility clear route grained account allow expansion probability conditioned surrounding context increase number parameter estimated prove problem",
        "hierarchy figure analysis sentence figure figure matter counting transition found observed result count refine initial estimate probability particular transition analogous went pcfgs result training identical given optimal assignment shown figure point provided system allows use feature structure dealt question entrancy form crucial expressive power typed feature structure return consider detailed implication similarity result figurewe model distribution observed corpus assuming independent decision strange ranking par favour number disagreement spite fact grammar generated corpus enforces number agreement difference result earlier one hierarchy us bot start symbol probability tell corpus contains free standing structure type num probability ofcodifies similar observation free standing structure type phrase item type phrase introduced type form sub type transition phrase corpus initial estimate probability transition unaffected training pcfg symmetry expansion singular plural variant implicit pth distribution singular plural variant encoded single location num refined independence assumption built training algorithm type refined according probability distribution irrespective context expanded seen consequence pth lump occasion num expanded enclosing context moment prepared tolerate clarity decision lead system clear probabilistic semantics number parameter estimated grammar linear function size type hierarchyeasy extensibility clear route grained account allow expansion probability conditioned surrounding context increase number parameter estimated prove problem",
        "turn extension system take proper account entrancies structure essence approach define stochastic procedure expands node tree way outlined guess pattern entrancies relate pay stipulate structure build inequated sense defined carpenter essential insight choice inequated feature structure involving set node thing choice arbitrary equivalence relation node turn equivalent choice partition set node set non set set node equivalence class standard recursive procedure generating partition element non add node equivalence class partition node consider new node singleton set basis stochastic procedure generating inequated feature structure interleave generation equivalence class expansion initial node described purpose expansion algorithm inequated feature structure consists feature tree equivalence relation maximal node tree task algorithm generate structure equip probability proceed case entrancy expand sub tree case new node begin new equivalence class avoids double counting problem remaining task assigning score equivalence relation satisfactory solution problem reason like assign probability intermediate structure way probability expanded structure independent route arrived method adopt merit simplicity associate single probabilistic parameter type derive probability structure particular pairwise equation node type equated multiplying probability structure decision derive probability corresponding inequated structure multiplying analogous way ensures probability equated inequated extension original structure sum original probability cost deficiency modelling take account fact token identity node transitive generated thing stand stochastic procedure free generate structure fact legal feature structure lead distortion probability estimate training algorithm spends probability mass impossible structure crude account entrancy ignoring issue proposed get right result case counting discussed obvious room improvement treatment provide required parametrisable mean distributing probability mass distinct equivalence relation extend current structure attractive possibility enumerate relation obtained adding current node different equivalence class available apply scoring function class normalize total score alternative introduce unpleasant dependency probability feature structure order stochastic procedure chooses expand node normalisation",
        "associate single probabilistic parameter type derive probability structure particular pairwise equation node type equated multiplying probability structure decision derive probability corresponding inequated structure multiplying analogous way ensures probability equated inequated extension original structure sum original probability cost deficiency modelling take account fact token identity node transitive generated thing stand stochastic procedure free generate structure fact legal feature structure lead distortion probability estimate training algorithm spends probability mass impossible structure",
        "crude account entrancy ignoring issue proposed get right result case counting discussed obvious room improvement treatment provide required parametrisable mean distributing probability mass distinct equivalence relation extend current structure attractive possibility enumerate relation obtained adding current node different equivalence class available apply scoring function class normalize total score alternative introduce unpleasant dependency probability feature structure order stochastic procedure chooses expand node normalisation carried knowledge equivalence class current node associated appropriate choice scoring function circumvent difficulty left matter research",
        "analyzing compound noun crucial issue natural language processing system particular system aim wide coverage domain compound noun dictionary impractical approach create new compound noun combining noun mechanism analyze structure compound noun individual noun necessary order identify structure compound noun find set word compose compound noun task trivial language english word separated space situation worse japanese space placed word process identify word boundary called segmentation processing language japanese ambiguity segmentation resolved time analyzing structure instance japanese compound noun singatakansetuzei new indirect tax produce segementations possibility case consulting japanese dictionary filter case remaining possibility sin new gata type kansetu indirect zei tax singata new kansetu indirect zei tax choose correct segmentation singata new kansetu indirect zei tax analyze structure japanese difficult syntactic knowledge expect sequence segmented word input structure analysis information structure expected improve segmentation accuracy research attacking problem applied hmm model segmentation probabilistic cfg analyzing structure compound noun accuracy method identifying correct structure kanzi character sequence average length character approach word boundary identified statistical information hmm model linguistic knowledge dictionary hmm model suggest improper character sequence word nonterminal symbol cfg derived statistical analysis word collocation number tends large number cfg rule large assumed compound noun consist character word character word questionable method extended handle case include character word lowering accuracy paper propose method analyze structure japanese compound noun word collocational information thesaurus collocational information acquired corpus kanzi character word outline procedure acquire collocational information follows extract collocation noun corpus kanzi character word noun collocation thesaurus category obtain collocation thesaurus category occurrence frequency collocational pattern thesaurus category possible structure compound noun preference calculated based collocational information structure highest score win rooth collocational information solve ambiguity pp attachment english hindle rooth resolved comparing strength associativity preposition verb preposition nominal head strength associativity calculated basis occurrence frequency word collocation corpus word collocation information use semantic knowledge thesaurus structure paper follows section explains knowledge structure analysis compound noun procedure acquire corpus section describes analysis algorithm section describes experiment conducted evaluate performance method section summarizes paper discus future research",
        "section describes procedure acquire collocational information analyzing compound noun corpus kanzi character word need occurrence frequency word collocation realistic collect word collocation use collocation thesaurus category word abstraction procedure consist following step collect kanzi character word section divide word middle produce pair kanzi character word thesaurus kanzi character word discarded section assign thesaurus category kanzi character word section count occurrence frequency category collocation section use corpus kanzi character word knowledge source collocational information reason follows japanese kanzi character sequence compound noun tendency confirmed comparing occurrence frequency kanzi character word text headword dictionary investigated tendency sample text newspaper article encyclopedia bunrui goi hyou bgh short standard japanese thesaurus sample text include sentence found character word represent thesaurus sample text collection kanzi character word corpus compound noun kanzi character sequence useful extract binary relation noun dividing kanzi character sequence middle give correct segmentation preliminary investigation show accuracy heuristic large corpus kanzi character word created prof tanaka yasuhito aiti syukutoku college tanaka corpus created newspaper article includes word collecting word collocation assign thesaurus category word difficult task word assigned multiple category case category collocation single word collocation incorrect choice follows use word collocation word assigned single category distribute frequency word collcations possible category collocation grishman sterling probability category collocation distribute frequency based probability probability collocation calculated method grishman sterling correct category collocation statistical method word collocation cowie yarowsky veronis lesk word assigned multiple category bgh use method collocation containing word multiple category represent corpus thesaurus assign multiple category word need use method assigning thesaurus category word count occurrence frequency category collocation follows collect word collocation time collect pattern word collocation care occurrence frequency pattern thesaurus category word produce category collocation pattern number category collocation pattern care frequency word collocation replacing word thesaurus category",
        "use corpus kanzi character word knowledge source collocational information reason follows japanese kanzi character sequence compound noun tendency confirmed comparing occurrence frequency kanzi character word text headword dictionary investigated tendency sample text newspaper article encyclopedia bunrui goi hyou bgh short standard japanese thesaurus sample text include sentence found character word represent thesaurus sample text collection kanzi character word corpus compound noun kanzi character sequence useful extract binary relation noun dividing kanzi character sequence middle give correct segmentation preliminary investigation show accuracy heuristic large corpus kanzi character word created prof tanaka yasuhito aiti syukutoku college tanaka corpus created newspaper article includes word",
        "collecting word collocation assign thesaurus category word difficult task word assigned multiple category case category collocation single word collocation incorrect choice follows use word collocation word assigned single category distribute frequency word collcations possible category collocation grishman sterling probability category collocation distribute frequency based probability probability collocation calculated method grishman sterling correct category collocation statistical method word collocation cowie yarowsky veronis lesk word assigned multiple category bgh use method collocation containing word multiple category represent corpus thesaurus assign multiple category word need use method",
        "analysis consists step enumerate possible segmentation input compound noun consulting headword thesaurus bgh assign thesaurus category wordscalculate preference structure compound noun according frequency category collocationswe assume structure compound noun expressed binary tree assume category right branch sub tree represents category sub tree assumption exsists japanese head final language modifier left modifiee assumption preference value structure calculated recursive function follows function return left right subtree tree cat return thesaurus category argument argument cat tree cat return category rightmost leaf tree return associativity measure category calculated frequency category collocation described previous section use measure return relative frequency collation appears left appears right modified mutual information statistic mi mean care similar mutual infromation church calculate semantic dependency word church different mutual information mi take account position word left right consider example singatakansetuzei possibility remain mentioned section assignment assigning thesaurus category provides digit number stand thesaurus category colon separate multiple category assigned word calculation case possible tructures represent tree list notation case ambiguity category sin expand ambiguity possible structure calculated case example preference structure calculated follows",
        "extract kanzi character sequence newspaper editorial column encyclopedia text overlap training corpus compound noun consisting kanzi character compound noun consisting kanzi character compound noun consisting kanzi character extracted set kanzi character sequence collection compound noun test data use thesaurus bgh standard machine readble japanese thesaurus structured tree hierarchical level show number category level experiment use category level compound noun knowledge use finer hierarchy level mentioned section create set collocation thesaurus category corpus kanzi character sequence bgh analyze test data according procedure described section segmentation use heuristic minimizing number content word order prune search space heuristic japanese morphological analysis correct structure test data created advance show result analysis kanzi character sequence mean correct answer obtained heuristic segmentation filtered correct segmentation row show percentage case correct answer identified tie row denoted show percentage correct answer th rank percentage correct answer ranked equal place correct answer second rank probabilistic measure provides better accuracy mutual information measure kanzi character compound noun result reversed kanzi character compound noun result kanzi character word equal order judge measure need experiment longer word obtain correct segmentation case kanzi character word case kanzi character word case kanzi character word accuracy segmentation candidate failure word missing dictionary heuristic adopted mentioned section difficult correct segmentation syntactic knowledge heuristic reduce ambiguity segmentation ambiguity remain experiment case ambiguity solved heuristic case kanzi character word case kanzi character word case kanzi character word case correct segmentation identified applying structure analysis case correct structure identified case collection test data hand case segmented case analyzed structure collection sequence segmented word possible structure show possible structure word sequence occurrence data collection compound noun test data consists character case compound noun consisting word current data collection",
        "case table find significant deviation occurrence structure deviation strong correlation distance modifier modifees rightmost column labeled show sum distance modifier modifiee contained structure distance measured based number word modifier modifiee instance distance modifier modifiee adjacent correlation distance occurrence structure tell modifier tends modify closer modifiee tendency proven maruyama tendency expressed formula follows distance word probability word said distance modification relation redifined taking tendency formula follows redifined show result new cv obtained significant improvement kanzi kanzi collection assumed thesaurus category tree represented category right branch subtree japanese head final language right subtree word suffix assumption hold true ultimate aim analyze semantic structure compound noun dealing grammatical head semantic head consideration order need knowledge judge subtree represents semantic feature tree knowledge extracted corpus machine readable dictionary certain class japanese noun called sahen meisi behave verb verb noun adding special verb suru noun case frame ordinary verb compound noun including noun use case frame selectional restriction analyze structure process analyzing ordinary sentence",
        "extract kanzi character sequence newspaper editorial column encyclopedia text overlap training corpus compound noun consisting kanzi character compound noun consisting kanzi character compound noun consisting kanzi character extracted set kanzi character sequence collection compound noun test data use thesaurus bgh standard machine readble japanese thesaurus structured tree hierarchical level show number category level experiment use category level compound noun knowledge use finer hierarchy level mentioned section create set collocation thesaurus category corpus kanzi character sequence bgh analyze test data according procedure described section segmentation use heuristic minimizing number content word order prune search space heuristic japanese morphological analysis correct structure test data created advance",
        "table show result analysis kanzi character sequence mean correct answer obtained heuristic segmentation filtered correct segmentation row show percentage case correct answer identified tie row denoted show percentage correct answer th rank percentage correct answer ranked equal place correct answer second rank probabilistic measure provides better accuracy mutual information measure kanzi character compound noun result reversed kanzi character compound noun result kanzi character word equal order judge measure need experiment longer word obtain correct segmentation case kanzi character word case kanzi character word case kanzi character word accuracy segmentation candidate failure word missing dictionary heuristic adopted mentioned section difficult correct segmentation syntactic knowledge heuristic reduce ambiguity segmentation ambiguity remain experiment case ambiguity solved heuristic case kanzi character word case kanzi character word case kanzi character word case correct segmentation identified applying structure analysis case correct structure identified case collection test data hand case segmented case analyzed structure collection sequence segmented word possible structure show possible structure word sequence occurrence data collection compound noun test data consists character case compound noun consisting word current data collection case table find significant deviation occurrence structure deviation strong correlation distance modifier modifees rightmost column labeled show sum distance modifier modifiee contained structure distance measured based number word modifier modifiee instance distance modifier modifiee adjacent correlation distance occurrence structure tell modifier tends modify closer modifiee tendency proven maruyama tendency expressed formula follows distance word probability word said distance modification relation redifined taking tendency formula follows redifined show result new cv obtained significant improvement kanzi kanzi collection assumed thesaurus category tree represented category right branch subtree japanese head final language right subtree word suffix assumption",
        "hold true ultimate aim analyze semantic structure compound noun dealing grammatical head semantic head consideration order need knowledge judge subtree represents semantic feature tree knowledge extracted corpus machine readable dictionary certain class japanese noun called sahen meisi behave verb verb noun adding special verb suru noun case frame ordinary verb compound noun including noun use case frame selectional restriction analyze structure process analyzing ordinary sentence",
        "analysis temporal expression natural language discourse provides challenge contemporary semantic theory introduced notion temporal anaphora account way temporal expression depend surrounding element discourse semantic contribution discourse paper discus interaction temporal anaphora quantification eventuality interaction interesting right good test bed theory semantic interpretation temporal expression discus case analysis sentence swart framework discourse representation theory drt kamp give wrong truth condition temporal connective sentence drt sentence trigger box splitting eventuality subordinate clause updated reference time antecedent box eventuality main clause consequent box causing undesirable universal quantification reference time problem analyzed swart instance proportion problem given solution generalized quantifier approach led seek solution problem drt drt advantage general theory discourse choice underlying formalism research project deal sentence context natural language specification computerized system paper propose solution based careful distinction different role reichenbach reference time adapted kamp reyle show minimal pair drs sentence according partee analysis according",
        "analysis mechanism temporal anaphoric reference hinge understanding ontological logical foundation temporal reference concept literature primitive range temporal instant tense logic prior interval time bennet partee analysis temporal connective heinmki event structure kamp hinrichs analysis temporal anaphora important factor interpretation temporal expression classification situation different aspectual class aktionsarten based distributional semantic property paper consider event state termed eventuality bach narrative sequence event clause advance narrative time state block progression mechanism account phenomenon hinrichs partee based notion reference time proposed reichenbach known account interpretation different tense form us temporal relation temporal index utterance time event time reference time reference time according reichenbach determined context temporal adverbial partee use notion reference time provides unified treatment temporal anaphoric relation discourse include narrative progression sequence simple tense sentence temporal adverb temporal adverbial clause introduced temporal connective concept reference time instant time interval approach summarized follows processing discourse discourse initial sentence argued require determined reference time event clause discourse introduce new event included current reference time event cause reference time updated time partee event clause introduce new state include current reference time update example analysis consider following narrative discourse partee show drs sentence discourse according hinrichs partee analysis drs mnemonic utterance time event discourse john getting interpreted relative understood reference time event included current reference time new reference time marker introduced recorded current reference time following event continue fashion updating reference time second sentence discourse processed sentence denotes state includes current reference time phrase phrasal sunday clausal bill left processed main clause introduce reference time override current reference time provides anaphoric antecedent tense main clause mechanism explain tense temporal adverbial combine locate occurrence running problem relative scope hinrichs tense morpheme main clause locates event time respect reference time temporal adverbial locate reference time clause example introduce new reference time ordered event described preceding discourse eventuality clause related reference time discussed respect narrative progression state includes reference time event included eventuality main clause interpreted respect reference time main clause event clause event introduces",
        "new reference time event time main clause example consider following discourse partee partee construct drs discourse illustrate diagram figure circle denoting inclusion extends hinrichs treatment temporal anaphora analysis sentence contain temporal adverbial quantification eventuality analysis trigger box splitting clause drt kamp following example partee subordinate clause interpreted relative single reference time mary telephoning specified occur specific time sentence need interpreted relative reference time reference time large interval contain relevant occurrence mary telephoning bill asleep reference time represented sub drs trigger box splitting event marker introduced antecedent box condition included current reference time cause introduction new reference time marker stative clause cause introduction includes reference time embedding condition construction regular clause sentence true proper embedding antecedent box extended proper embedding combination antecedent consequent box mean desired choice event mary telephoning reference time state sam asleep surround sentence sentence replaced added main clause drs noted partee analysis extend straightforward manner case operator replaced unrestricted quantified context similar drs sentence give wrong truth condition example figure show drs sentence according principle reference time interpretation main clause placed universe antecedent box temporal connective restricted lie embedding condition determine reference time quantified causing erroneous reading event john calling earlier time light cigarette john light cigarette time preceding phone preceding phone encounter problem drs figure reference time quantified drs restricted follow restricted replaced minute unrestricted analyzed time problem arises refer problem partee quantification problem suggests case insure reference time appears universe consequent drs causing quantified giving desired interpretation swart note moving right hand box agree hinrichs assumption temporal clause processed main clause update reference time respect main clause interpreted proposed solution reference time moved right box different notion",
        "hinrichs partee use notion reference time provides unified treatment temporal anaphoric relation discourse include narrative progression sequence simple tense sentence temporal adverb temporal adverbial clause introduced temporal connective concept reference time instant time interval approach summarized follows processing discourse discourse initial sentence argued require determined reference time event clause discourse introduce new event included current reference time event cause reference time updated time partee event clause introduce new state include current reference time update example analysis consider following narrative discourse partee show drs sentence discourse according hinrichs partee analysis drs mnemonic utterance time event discourse john getting interpreted relative understood reference time event included current reference time new reference time marker introduced recorded current reference time following event continue fashion updating reference time second sentence discourse processed sentence denotes state includes current reference time phrase phrasal sunday clausal bill left processed main clause introduce reference time override current reference time provides anaphoric antecedent tense main clause mechanism explain tense temporal adverbial combine locate occurrence running problem relative scope hinrichs tense morpheme main clause locates event time respect reference time temporal adverbial locate reference time clause example introduce new reference time ordered event described preceding discourse eventuality clause related reference time discussed respect narrative progression state includes reference time event included eventuality main clause interpreted respect reference time main clause event clause event introduces new reference time event time main clause example consider following discourse partee partee construct drs discourse illustrate diagram figure circle denoting inclusion",
        "partee extends hinrichs treatment temporal anaphora analysis sentence contain temporal adverbial quantification eventuality analysis trigger box splitting clause drt kamp following example partee subordinate clause interpreted relative single reference time mary telephoning specified occur specific time sentence need interpreted relative reference time reference time large interval contain relevant occurrence mary telephoning bill asleep reference time represented sub drs trigger box splitting event marker introduced antecedent box condition included current reference time cause introduction new reference time marker stative clause cause introduction includes reference time embedding condition construction regular clause sentence true proper embedding antecedent box extended proper embedding combination antecedent consequent box mean desired choice event mary telephoning reference time state sam asleep surround sentence sentence replaced added main clause drs",
        "noted partee analysis extend straightforward manner case operator replaced unrestricted quantified context similar drs sentence give wrong truth condition example figure show drs sentence according principle reference time interpretation main clause placed universe antecedent box temporal connective restricted lie embedding condition determine reference time quantified causing erroneous reading event john calling earlier time light cigarette john light cigarette time preceding phone preceding phone encounter problem drs figure reference time quantified drs restricted follow restricted replaced minute unrestricted analyzed time problem arises refer problem partee quantification problem suggests case insure reference time appears universe consequent drs causing quantified giving desired interpretation swart note moving right hand box agree hinrichs assumption temporal clause processed main clause update reference time respect main clause interpreted proposed solution reference time moved right box different notion reference time shown exempt criticism",
        "swart see partee quantification problem temporal manifestation proportion problem arises case kadmon sentence false case woman owns cat happy woman cat miserable predicted unselective binding quantifier drt quantify free variable scope case woman cat pair swart partee quantification problem similar universal quantifier sentence bind pair event updated reference time desired quantificational scheme universal quantification event existential reference time swart offer solution generalized quantifier approach based analysis quantified np transitive sentence analysis reference time implicit variable needed interpretation temporal relation quantificational structure connective viewed relation set event quantificational structure sentence analyzed iteration monadic quantifier single dyadic quantifier type approach adverb quantification q adverb assigned structure denote set event described subordinate main clause denotes image set temporal connective set event related relation presented second approach structure de swart solution overcome partee quantification problem drt existential quantification stipulated analysis acquires existential quantification free",
        "analysis partee quantification problem us different notion reference time account exposition kamp reyle split role reference time account large array phenomenon independent mechanism separation allows analysis drt temporal subordinate clause quantified sentence avoids partee problem mechanism discus location time rpt perf contain temporal marker corresponding location time rpts location time interval locate eventuality accordance aspectual classification included location time recorded drs respective marker state overlap location time recorded verb tense determines relation location time utterance time tense simple location time lie utterance time simple present location time coincides utterance time adverbial restrict location time temporal adverb introduce drs condition location time temporal subordinate clause introduce relation event time subordinate clause location time main clause exact temporal relation denoted temporal connective depends aspectual class eventuality related example following sentence event trigger introduction event marker location time marker drs drs condition past tense verb add condition sentence location time event main clause restricted fall event time event subordinate clause progression dealt feature rpt reference point rpt event time discourse marker present drs recorded assignment rpt interpreted respect rpt event taken follow current rpt state include rpt reset processing discourse terminal drs ready embedding test auxiliary rpts disappear participate embedding perfect analyzed notion nucleus moens steedman account inner structure eventuality nucleus defined structure containing preparatory process culmination consequent state categorization verb phrase different aspectual class phrased term nucleus refer perfect seen kamp reyle aspectual operator eventuality described perfect verb refer consequent state nucleus example following sentence denotes state holding present mary met president state result event mary met president state start end kamp reyle abut represented",
        "extending analysis temporal subordinate clause kamp reyle sentence include quantification eventuality propose alternative drt solution partee quantification problem partee sentence trigger box splitting location time eventuality subordinate clause serf antecedent location time eventuality main clause approach relevant temporal marker resides appropriate box yielding correct quantificational structure quantification structure need stipulated q adverb meaning arises temporal system illustrate analysis constructing drs figure sentence drs denotes utterance time subordinate clause trigger introduction event marker event time marker main clause trigger introduction event marker location time marker drs condition assymetry event time location time arises interpretation rule temporal connective quantified non quantified sentence temporal connective sentence relation marker precedence adopt suggestion chierchia partee implication rendered state state atomic eventuality complex state denoting john habit state hold present location time solution prone swart criticism naive solution moving reference time right drs temporal clause processed main clause location time replaces reference time partee analysis temporal index eventuality main clause arises processing main clause updating reference time subordinate clause",
        "section present application analysis related construction consider past perfect sentence swart give example illustrate inability interpret temporal connective use reference time swart subordinate clause determines reference time verb lie event time use event time wrong analysis troublesome approach us location time event main clause reference time problem analysis perfect use operator perf analysis eventuality referred main clause result state previous event temporal relation sentence inclusion event time anne coming location time result state paul having prepared dinner consider narrative progression quantified context sentence basic construction paradigm structure narrative progression consequent box narrative progression handled ordinary narrative progression kamp reyle resetting rpt drs figure describes complex state event john coming sequence subsequent event according activity deal sentence contain iteration implicit generic quantifier situation described john squinting sun shining analyzed complex state state hold john beach recorded condition location time overlap event time john beach figure",
        "ellipsis pervasive natural language received attention computational theoretical linguistics condition representation utterance serve suitable basis interpreting subsequent elliptical form remain understood past attempt characterize process single traditional module language processing considering syntax semantics discourse isolation failed account data paper claim variety fact concerning ellipsis resolution event reference interclausal coherence explained interaction syntactic semantic property form question type discourse inference operative establishing coherence antecedent elided clause section introduce fact concerning gapping vp ellipsis non elliptical event reference seek explain section categorize elliptical event referential form according feature expression leaf constituent syntax andwhether expression semantics section describe type discourse inference common topic inference coherent situation inference specific proposal concerning interface syntactic semantic representation utilize section proposal account data presented section contrast account relevant past work section conclude section",
        "noted previous work felicity certain form ellipsis dependent type coherence relationship extant antecedent elided clause prince kehler section review relevant fact form ellipsis gapping vp ellipsis compare fact concerning non elliptical event reference characterized antecedent sentence called source sentence elision constituent circumstance constituent subsequent target sentence exemplified sentence concerned particular fact gapping noticed levin prince gapping acceptable conjunctive symmetric meaning conjoining clause causal asymmetric meaning paraphraseable result sentence conjunctive reading sentence understood mean hillary angry caused came result bill upset seen embedding example context reinforces meaning instance gapping felicitous passage context support symmetric reading infelicitous passage intended causal meaning common stipulation literature stating gapping applies coordinate structure subordinate one account coordinated case unacceptable ellipsis characterized initial source sentence subsequent target sentence bare auxiliary indicating elision verb phrase distribution vp ellipsis shown sensitive coherence relationship extant source target clause different respect previous paper kehler context vp ellipsis examined determine representation retrieved syntactic semantic nature given vp ellipsis copy syntactic representation termed parallel construction predicting unacceptability voice mismatch example nominalized source example copy semantic representation non parallel construction predicting acceptability voice mismatch example nominalized source example example analogous gapping case constraint mismatch syntactic form hold symmetric parallel use example asymmetric non parallel meaning example fact appears gapping felicitous construction vp ellipsis requires syntactic antecedent gapping infelicitous case vp ellipsis requires suitable semantic antecedent approach vp ellipsis operate single module language processing fail distinction necessary account difference hankamer note elliptical sentence unacceptable voice mismatch similar example non elided event referential form acceptable adequate theory ellipsis event reference account distinction sum felicity gapping vp ellipsis appears dependent type coherence relation extant source target clause event reference hand appears display dependence seek account fact section follow",
        "section characterize form addressed term feature form leaf constituent syntax andwhether form semantics subsequent section distinct mechanism recovering type missing information interact type discourse inference predict phenomenon noted previous section illustrate relevant syntactic semantic property form version categorial semantics described pereira montagovian tradition semantic representation generated correspondence constituent modification relationship manifest syntax predicate curried associated assumption discharged suitable construction show representation sentence bill upset serve initial source clause representation example follow analysis gapping follow sag hypothesizing post surface structure level syntactic representation basis interpretation source clause gapping construction constituent source parallel overt constituent target abstracted clause representation simplicity assume abstraction achieved fronting constituent post surface structure hinge analysis compatible possible mechanism syntactic semantic representation source clause example fronting shown figure fronting leaf trace assumption discharged combined antecedent clause gapping construction represented overt constituent fronted elided sentence node instance representation target clause example shown figure node indicated constituent reconstructed copying embedded sentence source target clause parallel trace assumption bound target semantics embedded sentence open proposition clause share semantics claim recovered copying syntax gapping result anaphoric expression semantics fact seen figure copying take place sentence level semantics gapped clause gapping vp ellipsis result constituent syntax case verb phrase gapping vp ellipsis result anaphoric form semantics show representation clause hillary anaphoric expression indicated representation figure source semantics missing recovered way syntactic copied corresponding semantics semantics complete sentence derived case anaphoric expression constrained semantics copied constituent anaphoric expression resolved resulting discharge anaphoric assumption higher order unification method developed dalrymple purpose case sentence level semantics recovered copying syntactic representation referential form constitute verb phrase syntax noted halliday hasan inter alia main verb operative form anaphora contrast auxiliary operative vp ellipsis pronoun event referential form anaphoric fact pronoun refer event result type constraint imposed main verb form",
        "coherent utterance discourse segment require embodied individual syntactic semantic representation additional inter utterance constraint met describe type inference enforce constraint imposed coherence relation case argument coherence relation form semantic representation retrieved corresponding node syntax operation performed representation dictated nature constraint imposed type inference distinguished level syntax argument retrieved segment utterance standing common topic relation requires determination point commonality parallelism departure contrast set corresponding entity property utterance process reliant performing comparison generalization operation corresponding representation scha polanyi hobbs pruest asher sketch definition common topic relation taken adapted hobbs case hearer understand relation inferring sentence inferring sentence listed constraint order meet constraint identification require arbitrary level generalization relation stated utterance relation given sentence instance coherent understanding john fred common property having support clinton coherent virtue inference resulting identifying parallel element property including john young aspiring politician democrat clinton identified party candidate characteristic common topic relation share require identification parallel entity relation argument constraint posit syntactic representation guide identification parallel element retrieve semantic representation utterance standing coherent situation relation requires hearer convince utterance describe coherent situation given knowledge world process requires path inference established situation event state described participating utterance regard constraint parallelism sub sentential constituent relation summarized table case hearer infer sentence sentence constraint presupposition listed abduced hobbs relation given sentence asserted clause understanding sentence requires presupposition politician implies dishonest reliant sentential level semantics clause identified true hume called contiguity relation including hobbs occasion figure ground relation purpose paper consider weaker case cause effect reiterate crucial observation common topic inference utilizes syntactic structure identifying semantics sub sentential constituent serve argument coherence constraint contrast coherent situation inference utilizes sentential level semantic form required abducing coherent situation question arises happens constituent syntax utterance discourse inference mechanism retrieve semantic form node syntax syntax recovered node accessed missing posit missing constituent recovered product common",
        "understanding segment utterance standing common topic relation requires determination point commonality parallelism departure contrast set corresponding entity property utterance process reliant performing comparison generalization operation corresponding representation scha polanyi hobbs pruest asher sketch definition common topic relation taken adapted hobbs case hearer understand relation inferring sentence inferring sentence listed constraint order meet constraint identification require arbitrary level generalization relation stated utterance relation given sentence instance coherent understanding john fred common property having support clinton coherent virtue inference resulting identifying parallel element property including john young aspiring politician democrat clinton identified party candidate characteristic common topic relation share require identification parallel entity relation argument constraint posit syntactic representation guide identification parallel element retrieve semantic representation",
        "understanding utterance standing coherent situation relation requires hearer convince utterance describe coherent situation given knowledge world process requires path inference established situation event state described participating utterance regard constraint parallelism sub sentential constituent relation summarized table case hearer infer sentence sentence constraint presupposition listed abduced hobbs relation given sentence asserted clause understanding sentence requires presupposition politician implies dishonest reliant sentential level semantics clause identified true hume called contiguity relation including hobbs occasion figure ground relation purpose paper consider weaker case cause effect reiterate crucial observation common topic inference utilizes syntactic structure identifying semantics sub sentential constituent serve argument coherence constraint contrast coherent situation inference utilizes sentential level semantic form required abducing coherent situation question arises happens constituent syntax utterance discourse inference mechanism retrieve semantic form node syntax syntax recovered node accessed missing posit missing constituent recovered product common topic inference allow parallel property entity serving argument coherence relation accessed reconstructed structure hand copying triggered coherent situation inference argument retrieved level sentence node present section difference account data given section",
        "previous section classified elliptical event referential form leave constituent syntax anaphoric semantics constituent syntax referential recovered common topic inference expression semantics referential resolved semantic mean type discourse inference section phenomenon presented section follow property section gapping construction felicitous symmetric common topic meaning fact predicted account following way case common topic construction missing sentence target copied source sentential semantics derived argument coherence relation identified reasoning carried predicting felicity case coherent situation relation recovery syntax take place gapped clause sentence level semantics gapping fails felicitous case account explains similar difference felicity coordinating conjunction discussed kehler gapping infelicitous construction subordinating conjunction indicating coherent situation relation exemplified stripping construction similar gapping bare constituent target receiving contrastive accent vp ellipsis stranded auxiliary predict stripping acceptable common topic construction coherent situation construction appears case summary gapping related construction infelicitous case coherent situation inference employed mechanism recovering sentential semantics elided clause section coherent situation construction vp ellipsis obtain semantic antecedent regard constraint structural parallelism exemplified voice mismatch sentence fact predicted account case common topic construction suitable syntactic antecedent reconstructed site node result anaphoric expression take accompanying semantics vp ellipsis predicted require suitable syntactic antecedent scenario coherent situation construction node reconstructed case anaphoric expression resolved semantic ground vp ellipsis constrained having suitable semantic antecedent analysis account range data given kehler point departure exists account current respect clause conjoined previous account case classified non parallel resulting prediction require semantic source representation analysis expect case pure contrast pattern parallel class common topic construction opposed violated expectation use indicates coherent situation relation current account make correct prediction example contrast meaning appear acceptable example violated expectation meaning summarize data presented earlier account example conflict analysis predicted account given final note consider interaction vp ellipsis gapping following pair example adapted sag pg sag defines alphabetic variance condition",
        "predicts sentence infelicitous predicts sentence suggests weakening condition result example predicted acceptable consider solution predicting judgement stated felicity sentence infelicity sentence account predicts example clause common topic relationship second requires reconstructed target site suitable form object abstracted yielding trace assumption subsequent vp ellipsis fails felicitous contrast conjunction clause example indicates coherent situation relation clause need reconstructed subsequent based resolution anaphoric form succeeds apparent paradox example expect sag hankamer note elliptical sentence unacceptable voice mismatch similar example event referential form acceptable exemplified sentence stated earlier form anaphoric leave constituent syntax follows present account reference successful regard type discourse inference employed",
        "recall section gapping construction felicitous symmetric common topic meaning fact predicted account following way case common topic construction missing sentence target copied source sentential semantics derived argument coherence relation identified reasoning carried predicting felicity case coherent situation relation recovery syntax take place gapped clause sentence level semantics gapping fails felicitous case account explains similar difference felicity coordinating conjunction discussed kehler gapping infelicitous construction subordinating conjunction indicating coherent situation relation exemplified stripping construction similar gapping bare constituent target receiving contrastive accent vp ellipsis stranded auxiliary predict stripping acceptable common topic construction coherent situation construction appears case summary gapping related construction infelicitous case coherent situation inference employed mechanism recovering sentential semantics elided clause",
        "recall section coherent situation construction vp ellipsis obtain semantic antecedent regard constraint structural parallelism exemplified voice mismatch sentence fact predicted account case common topic construction suitable syntactic antecedent reconstructed site node result anaphoric expression take accompanying semantics vp ellipsis predicted require suitable syntactic antecedent scenario coherent situation construction node reconstructed case anaphoric expression resolved semantic ground vp ellipsis constrained having suitable semantic antecedent analysis account range data given kehler point departure exists account current respect clause conjoined previous account case classified non parallel resulting prediction require semantic source representation analysis expect case pure contrast pattern parallel class common topic construction opposed violated expectation use indicates coherent situation relation current account make correct prediction example contrast meaning appear acceptable example violated expectation meaning summarize data presented earlier account example conflict analysis predicted account given final note consider interaction vp ellipsis gapping following pair example adapted sag pg sag defines alphabetic variance condition predicts sentence infelicitous predicts sentence suggests weakening condition result example predicted acceptable consider solution predicting judgement stated felicity sentence infelicity sentence account predicts example clause common topic relationship second requires reconstructed target site suitable form object abstracted yielding trace assumption subsequent vp ellipsis fails felicitous contrast conjunction clause example indicates coherent situation relation clause need reconstructed subsequent based resolution anaphoric form succeeds apparent paradox example expect",
        "literature ellipsis event reference voluminous attempt comprehensive comparison compare current work previous study tie ellipsis resolution account discourse structure coherence previous account kehler account pruest asher kehler presented analysis vp ellipsis distinguished type relationship clause parallel non parallel architecture presented utterance parsed propositional representation integrated discourse model posited vp ellipsis access propositional discourse model representation case parallel construction source resided propositional representation case non parallel construction source integrated discourse model kehler showed architecture accounted fact levin prince noted gapping current work improves analysis respect need posited syntactic representation disappear integrated discourse model syntactic semantic representation co exist issue regard interpretation propositional representation rendered moot dichotomy respect level representation vp ellipsis locates copy antecedent distinct factor separated resolution missing constituent common topic inference syntactic resolution anaphoric expression case semantic apparent dichotomy vp ellipsis data arises interaction different phenomenon current approach scale complex case instance clear previous account non parallel construction embedded parallel construction handled sentence current approach account case work pruest asher provide analysis vp ellipsis context account discourse structure coherence pruest utilizing mixed representation called syntactic semantic structure asher utilizing discourse representation theory construct defines mechanism determining relation parallelism contrast give constraint resolving vp ellipsis related form general framework follows sag requiring elided representation alphabetic variant referent constraint rule case vp ellipsis obtains mismatched antecedent example non parallel case given kehler appears approach account infelicity mixed gapping vp ellipsis case sentence",
        "paper present text planner verbalization natural deduction style proof gentzen similar attempt found previous work era generation system expound chester characterized example direct translation sophisticated linearization applied input proof step translated template driven way proof tested input early version mumble system mcdonald main aim feasibility architecture recent attempt found thinker edgar pelletier implement interesting isolated proof presentation strategy giving comprehensive underlying model computational model viewed attempt comprehensive computational model produce adequate argumentative text style proof main aim existing text planning technique adapted particular application test feasibility computational model implemented system called proverb current text planner assume language generation planned behavior adopt hierarchical planning approach hovy moore dale reithinger psychological evidence language unplanned spontaneous aspect ochs observation researcher exploited organizing text respect local relation implemented system generating description object strong domain structure house chip family discourse started local structure suggest object available planning short range strategy employed organize short segment text computational point view hierarchical planner elaborates initial communicative goal final subgoals achieved applying primitive operator text generator based local organization contrast chooses remaining task carry macroplanner proverb combine hierarchical planning local organization uniform planning framework hierarchical planning realized called presentation operator split task presenting particular proof subtasks presenting subproofs overall planning mechanism follows rst based planning approach moore reithinger planning operator resemble schema schema based planning mckeown paris presentation operator devised simulate unplanned aspect intermediate conclusion presented chosen guidance local focus mechanism spontaneous way operator embody explicit communicative norm given higher priority presentation operator applicable presentation operator chosen distinction planned unplanned presentation lead natural segmentation discourse attentional hierarchy following theory grosz sidner correspondence intentional hierarchy attentional hierarchy attentional hierarchy reference choice inference method presented intermediate conclusion inference choice main concern microplanner proverb huang",
        "text planner discussed paper macroplanner proverb translates machine found proof step natural language adopts reconstructive approach proof machine oriented formalism generated proof development environment mkrp new proof resembles found mathematical textbook reconstructed huang reconstructed proof proof tree proof node derived child applying inference method called justification step justified application definition theorem rest justified inference rule natural deduction calculus case rule example segment possible input proof node labeled convenience justification dsubgr tsol stand definition unit element subgroup subset group theorem solution input proof tree augmented ordered list node root subproofs planned order proof figure associated list",
        "macroplanner proverb elaborates communicative goal selects order piece information fulfill goal output ordered sequence proof communicative act intention pcas viewed speech act domain application combine mentioned presentation mode encoding communication knowledge planning presentation form operator uniform planning framework presentation operator embody explicit communicative norm given higher priority presentation chosen presentation operator applies overall planning framework realized function present input subproof present executes basic planning cycle input subproof conveyed cycle carry presentation operator present try choose apply operator impossible operator chosen function present called entire proof presentation task execution presentation operator generate subtasks calling discourse produced present form attentional unit compare subsection discourse carried recorded discourse model recording semantic object property discourse model consists input proof tree conveyed discourse model segmented attentional hierarchy subproofs posted presentation operator subtasks constitute attentional unit following notion useful formulation presentation operator task subproof input proof presentation current task focus intermediate conclusion presented semantic object involved local focus called focal center primitive action planned macroplanning achieve communicative goal speech act pcas defined term communicative goal fulfill possible verbalization analysis proof mathematical textbook pca goal combination following subgoals conveying step derivation simplest pca operator derive depending reference choice possible verbalization given following element subset according definition subset element update global attentional structure pcas convey partial plan presentation group pcas include creating new attentional unit setting premise goal new unit closing current unit reallocating attention reader attentional unit pcacreates attentional unit assumption formula goal producing verbalization prove formula let consider case assuming pcas employed proverb huang detail presentation activity different nature corresponding communication knowledge encoded presentation operator planning framework similar plan operator generation system hovy moore dale reithinger general presentation operator map original presentation task sequence subtasks sequence pcas following slot proof proof schema characterizes syntactical structure proof segment operator designed play role goal slot traditional planning framework condition predicate procedure carry sequence presentation act primitive pcas recursive call",
        "proverb combine mentioned presentation mode encoding communication knowledge planning presentation form operator uniform planning framework presentation operator embody explicit communicative norm given higher priority presentation chosen presentation operator applies overall planning framework realized function present input subproof present executes basic planning cycle input subproof conveyed cycle carry presentation operator present try choose apply operator impossible operator chosen function present called entire proof presentation task execution presentation operator generate subtasks calling discourse produced present form attentional unit compare subsection",
        "pcas primitive action planned macroplanning achieve communicative goal speech act pcas defined term communicative goal fulfill possible verbalization analysis proof mathematical textbook pca goal combination following subgoals conveying step derivation simplest pca operator derive depending reference choice possible verbalization given following element subset according definition subset element update global attentional structure pcas convey partial plan presentation group pcas include creating new attentional unit setting premise goal new unit closing current unit reallocating attention reader attentional unit pcacreates attentional unit assumption formula goal producing verbalization prove formula let consider case assuming pcas employed proverb huang detail",
        "presentation activity different nature corresponding communication knowledge encoded presentation operator planning framework similar plan operator generation system hovy moore dale reithinger general presentation operator map original presentation task sequence subtasks sequence pcas following slot proof proof schema characterizes syntactical structure proof segment operator designed play role goal slot traditional planning framework condition predicate procedure carry sequence presentation act primitive pcas recursive call procedure present subproofs list feature help select set applicable operator",
        "section elaborates communicative norm concerning proof presented split subproofs structured subproofs mapped linear order presentation contrast operator employed rst based planner split goal according rhetorical structure operator encode standard schema presenting proof contain subgoals presentation operator divided category schema based operator encoding complex schema presentation proof specific pattern integrated proverb general operator embodying general presentation norm concerning splitting proof ordering subgoals look operator devised proof segment containing case corresponding schema proof tree shown figure circumstance writer recognize confronted proof segment containing case subproof structure figure current presentation task tested task disjunction presented mode tested local focus circumstance communication norm motivates writer present leading second case subgoal achieved proceed case enforces certain pcas mediate part proof procedure captured presentation operator implicitproof given figureapplicability condition act conveyed present subgoal pca verbalization let consider case assuming present subgoal pca verbalization consider second case assuming present subgoal mark conveyedfeatures compulsory implicit feature value divided group characterizing style text operator produce concerning planning aspect implicit stylistic feature value indicating splitting proof subgoals explicit explicit dual case explicit pca added beginning act slot produce verbalization prove let prove consider case feature value compulsory indicates applicability condition satisfied style operator conforms global style text planner committed operator chosen weaker value reflect specificity plan operator specific general presentation operator perform simple task according general text organization principle eitherenforce linearization subproofs presented orsplit task presentation proof ordered subproofs subtasks ordering operator operationalizes general ordering strategy called minimal load principle principle predicate writer present shorter branch longer one argument levelt simple branch chosen described writer choice node flagged memory return follows shorter branch duration load shorter concrete operator omitted subproofs ordered subproofs planned corresponding proof constructed ordering operator based general ordering principle local focus principle proof time order principle huang invocation ordering operator followed invocation splitting operator post subgoals calling function present ordered goal",
        "presentation process simulates unplanned proof presentation splitting presentation goal subgoals according standard schema follows local derivation relation find proof node subproof presented sense similar local organization technique sibun presentation operator applies proverb chooses operator node presented suggested mechanism local focus proof node having local focus child chosen step greatest semantic overlapping focal center preferred mentioned focal center semantic object mentioned proof node local focus based observation proved property semantic object tends continue talk particular object turning new object examine situation proof awaiting presentation node local focus set focal center presented node node current task chosen node presented introduce new semantic object overlap focal center larger different circumstance derivation node presented different way corresponding presentation knowledge encoded presentation operator present step derivation derive applicability condition suggested focus mechanism node conveyed pca conveys fact derived premise applying general explicit detailed conclusion premise method instantiated def subset following verbalization produced element subset element according definition subset trivial subproof presented single derivation omitting intermediate node subproof suggested local focus simulated operator called simplify operator integrated proverb",
        "node presented suggested mechanism local focus proof node having local focus child chosen step greatest semantic overlapping focal center preferred mentioned focal center semantic object mentioned proof node local focus based observation proved property semantic object tends continue talk particular object turning new object examine situation proof awaiting presentation node local focus set focal center presented node node current task chosen node presented introduce new semantic object overlap focal center larger",
        "different circumstance derivation node presented different way corresponding presentation knowledge encoded presentation operator present step derivation derive applicability condition suggested focus mechanism node conveyed pca conveys fact derived premise applying general explicit detailed conclusion premise method instantiated def subset following verbalization produced element subset element according definition subset trivial subproof presented single derivation omitting intermediate node subproof suggested local focus simulated operator called simplify operator integrated proverb",
        "macroplanning produce sequence pcas microplanner restricted treatment reference choice inference method presented intermediate conclusion depends static salience relating domain knowledge similar subsequent reference sensitive context particular segmentation attentional hierarchy space restriction following piece preverbal message example pca enriched reference choice reason method microplanner huang huang surface generator tag gen kilger produce utterance element element notice reason labeled explicit verbalized demonstrate type proof generated proverb complete output proof constructed mkrp theorem let group subgroup unit element let group subgroup unit element unit element according definition unit element suppose definition unit element subgroup group semigroup solution equation unit element unit element solution equation group uniqueness solution conclusion independent choice element",
        "paper put architecture combine established generation technique adapted particular application presentation style proof hope architecture general interest particular application important feature model hierarchical planning unplanned spontaneous presentation integrated uniform framework hierarchical planning view language generation planned behavior explicit communicative knowledge encoded schema hierarchical planning split presentation task subtasks overall presentation mechanism common rst based text planner planning operator contain complex presentation schema schema based planning schema based planning cover proof particular structure complemented mechanism called presentation presentation aim simulating unplanned proof presentation proof node subproof awaiting presentation chosen presented local derivation relation node available local focus mechanism employed single candidate having strongest semantic link focal center distinction planned unplanned behavior enables natural segmentation discourse attentional hierarchy provide appropriate basis discourse theory handle reference choice huang proof found mathematical textbook output proverb tedious inflexible tediousness ascribed lack plan level knowledge input proof distinguishes crucial step unimportant detail sophisticated plan recognition technique necessary inflexibility text produced inherited schema based approach grained planning term single pcas remedy fixed lexicon choice reimplementing",
        "general purpose natural language analysis system started use declarative unification based sentence grammar formalism system type include sri clare system alshawi alvey tool anlt briscoe declarative formalism help ease task developing maintaining grammar kaplan addition syntactic processing system incorporate lexical morphological semantic processing applied analysis occurring text alshawi briscoe carroll grammar particular system shown wide coverage alshawi taylor practical throughput parser realistic grammar important example processing large amount text interactive application little published research compare performance different parsing algorithm wide coverage unification based grammar comparison focussed context free augmented parsing tomita billot lang small limited coverage unification grammar lexicon shann bouma van noord maxwell kaplan clear result scale reflect behaviour parser realistic complex unification based grammar particular grammar admitting ambiguity parse time tend increase increasing input length smaller grammar rule application constrained simple predictive technique study relate observed performance comparable parsing system implementational oversight apparent confounding factor general conclusion research directed improving throughput unification based parsing system concerned unification operation consume parse time tomabechi system lexicalist grammar formalism hpsg pollard sag parsing algorithm assume importance grammar having substantial phrase structure component clare employing hpsg like analysis contains ten rule anlt us formalism derived gpsg gazdar specific rule set control unification performed analysis syntactic information associated lexical item make parsing attractive cky kasami younger augmented prediction improve performance earley lang pratt describes unification based parser related polynomial complexity parsing algorithm incorporating unification increase complexity exponential grammar size input length section appears little impact practical performance section discus finding present conclusion",
        "parser study left corner parser non deterministic parser lr like parser based algorithm devised schabes parser accept grammar written anlt formalism briscoe distributed anlt package parser create parse forest tomita incorporate subtree sharing identical sub analysis shared differing superordinate analysis node packing sub analysis covering portion input root category subsumption relationship merged single node left corner bu lc parser operates left right breadth storing partial active constituent chart carroll give description pure parsing thought providing high performance actual implementation achieves good throughput section number significant optimisation efficient rule invocation cheap static rule indexing discrimination tree keyed feature value rule daughter interleave rule access unification share unification result group rule indexing partial complete constituent category type avoid attempting unification subsumption operation static analysis show fail storage minimisation deferring structure copying required unification operation constituent creation necessary unification success parse success optimisation improve throughput factor carroll describe methodology constructing parser unification based grammar backbone grammar constructed unification grammar parse table constructed backbone grammar parser driven table controlled unification residue feature unification grammar encoded backbone parser technique aho conjunction graph structured stack tomita adapting unification based parsing kipps tomita recogniser achieves polynomial complexity input length caching reduction parser performs unification specified unification grammar version backbone rule applied constitutes line parsing algorithm general case line variant unification deferred complete parse forest constructed guaranteed terminate anlt grammar drawback line algorithm variant kipps caching cache assume reduction given vertex rule number daughter build constituent time general case daughter unification category weaker kind cache partial analysis unification result found necessary implementation avoid duplication unification sped parser factor little space cost compiled earley parser based predictive chart based parsing algorithm devised schabes driven table compiling predictive component earley parser size table related size grammar technique demonstrates parser take fewer step earley time complexity space complexity cubic parser us earley representation parse forest incorporation unification parser follows methodology developed unification based parsing described previous section table computed backbone parser augmented",
        "line unification feature based subsumption operation driven table allow meaningful comparison parser parser us word lookahead version table constructed modified lalr technique carroll achieve cubic time bound parser able retrieve unit time item chart having given state start end position input string obvious array implementation word sentence anlt grammar contain element reason implementation employ sparse representation array small proportion element filled parser sort duplication unification occurs parser list partial analysis cached way",
        "left corner bu lc parser operates left right breadth storing partial active constituent chart carroll give description pure parsing thought providing high performance actual implementation achieves good throughput section number significant optimisation efficient rule invocation cheap static rule indexing discrimination tree keyed feature value rule daughter interleave rule access unification share unification result group rule indexing partial complete constituent category type avoid attempting unification subsumption operation static analysis show fail storage minimisation deferring structure copying required unification operation constituent creation necessary unification success parse success optimisation improve throughput factor",
        "briscoe carroll describe methodology constructing parser unification based grammar backbone grammar constructed unification grammar parse table constructed backbone grammar parser driven table controlled unification residue feature unification grammar encoded backbone parser technique aho conjunction graph structured stack tomita adapting unification based parsing kipps tomita recogniser achieves polynomial complexity input length caching reduction parser performs unification specified unification grammar version backbone rule applied constitutes line parsing algorithm general case line variant unification deferred complete parse forest constructed guaranteed terminate anlt grammar drawback line algorithm variant kipps caching cache assume reduction given vertex rule number daughter build constituent time general case daughter unification category weaker kind cache partial analysis unification result found necessary implementation avoid duplication unification sped parser factor little space cost",
        "compiled earley parser based predictive chart based parsing algorithm devised schabes driven table compiling predictive component earley parser size table related size grammar technique demonstrates parser take fewer step earley time complexity space complexity cubic parser us earley representation parse forest incorporation unification parser follows methodology developed unification based parsing described previous section table computed backbone parser augmented line unification feature based subsumption operation driven table allow meaningful comparison parser parser us word lookahead version table constructed modified lalr technique carroll achieve cubic time bound parser able retrieve unit time item chart having given state start end position input string obvious array implementation word sentence anlt grammar contain element reason implementation employ sparse representation array small proportion element filled parser sort duplication unification occurs parser list partial analysis cached way",
        "variable determine parser computational complexity grammar input string barton considered section term dependent grammar time complexity bu lc unification based parser described number category implicit grammar number rule space complexity dominated size parse forest result proved carroll anlt grammar feature nested maximum depth finite large briscoe grammar dependent complexity parser make appear intractable johnson show number state certain pathological grammar related size grammar input force parser visit state course parse total number operation performed space consumed vertex graph structured stack exponential function size grammar avoid complexity parser employ table construction method ensures number state parse table related size grammar resulting number operation performed parser worst polynomial function grammar size complexity returning par string related length number par exponential enumerated complexity parser measured computation parse forest extracting single analysis forest worse linear feature anlt grammar formalism kleene operator allowing indefinite repetition rule daughter disallowed complexity bu lc parser respect length input string maximum number daughter rule carroll inclusion operator increase complexity exponential retain polynomial time bound new rule introduced produce recursive tree structure iterated flat tree structure technique applied anlt grammar increased overhead rule invocation structure building slow parser time space complexity version parser unification version parser turn time bound greater cubic general case version pack identical sequence sub analysis reduction given point rule number daughter packed sequence formed higher level constituent stand processing unification version reduce action daughter rule involved unified possible alternative sequence sub analysis consumed rule effect expanding flattening packed sequence leading bound total number unification",
        "term dependent grammar time complexity bu lc unification based parser described number category implicit grammar number rule space complexity dominated size parse forest result proved carroll anlt grammar feature nested maximum depth finite large briscoe grammar dependent complexity parser make appear intractable johnson show number state certain pathological grammar related size grammar input force parser visit state course parse total number operation performed space consumed vertex graph structured stack exponential function size grammar avoid complexity parser employ table construction method ensures number state parse table related size grammar resulting number operation performed parser worst polynomial function grammar size",
        "complexity returning par string related length number par exponential enumerated complexity parser measured computation parse forest extracting single analysis forest worse linear feature anlt grammar formalism kleene operator allowing indefinite repetition rule daughter disallowed complexity bu lc parser respect length input string maximum number daughter rule carroll inclusion operator increase complexity exponential retain polynomial time bound new rule introduced produce recursive tree structure iterated flat tree structure technique applied anlt grammar increased overhead rule invocation structure building slow parser time space complexity version parser unification version parser turn time bound greater cubic general case version pack identical sequence sub analysis reduction given point rule number daughter packed sequence formed higher level constituent stand processing unification version reduce action daughter rule involved unified possible alternative sequence sub analysis consumed rule effect expanding flattening packed sequence leading bound total number unification",
        "ass practical performance unification based parser described series experiment conducted anlt grammar grover wide coverage grammar english grammar defined metagrammatical formalism compiled unification based object grammar syntactic variant definite clause grammar formalism pereira warren containing feature phrase structure rule us fixed arity term unification grammar provides coverage following construction declarative sentence imperative question yes tag wh question unbounded dependency type topicalisation relativisation wh question exhaustive treatment verb adjective complement type phrasal prepositional verb complement type passivisation verb phrase extraposition sentence verb phrase modification noun phrase complement pre post modification partitive coordination major category type nominal adjectival comparative grammar linked lexicon containing definition base form word experiment draw smaller lexicon word consisting closed class vocabulary open class vocabulary definition sample word taken exhibit range possible complementation pattern issue lexical coverage concern experiment anlt grammar loaded set sentence input parser order provide independent basis comparison sentence input sri core language engine cle parser moore alshawi grammar alshawi state art system accessible author sentence taken initial sample representative sentence extracted corpus form anlt package corpus defining type construction grammar intended cover written linguist developed anlt grammar check adverse effect coverage grammar modified grammar development initial sentence grammar failed parse case punctuation required missing corpus anlt grammar failed parse additional sentence removed sample leaving mean length word declarative sentence wh question sentence gap passive sentence containing co ordination show total parse time storage allocated bu lc parser parser parser anlt grammar lexicon parser implemented author similar high standard similar implementation technique parser parser share unification module run lisp environment compiled optimisation setting profiled tool hand optimised similar extent difference performance stem algorithmic implementational reason predictive parser employ symbol lookahead incorporated parsing table lalr technique show result cle parser grammar lexicon figure include garbage collection time phrasal appropriate processing parse forest unpacking grammar analysis similar level detail result parser bu lc parser allocates storage magnitude speed",
        "expected given enthusiastic advocation non deterministic parsing researcher tomita wright light improvement observed predictive pure parsing moore dowding assumption incorrect prediction gap main avoidable source performance degradation moore dowding investigation show speed maximum possible anlt grammar throughput parser half parser bu lc parser intermediate term storage allocated difference performance parser fact performs unification expected corresponding finite state automaton determinised avoid theoretical exponential time complexity grammar size paying price run time reason poor performance parser overhead involved maintaining sparse representation chart fact anlt grammar generates packed parse forest parse table state fewer action table encodes contextual distinction billot lang anlt grammar similar wide coverage return similar number syntactic analysis input better throughput parser described paper cle parser indicates contain significant implementational deficiency bias result second experiment carried cle parser built grammar lexicon replaced version anlt object grammar lexical entry translated cle formalism reverse configuration grammar translated anlt formalism possible central rule contain sequence daughter specified single list variable counterpart anlt simulated throughput configuration fiftieth bu lc parser anlt grammar contains time rule sentence level portion grammar alshawi personal communication point cle parser run grammar containing large number rule contrast anlt parser mean sentence length experiment shorter word length depending genre etc common real text test sentence cover wide range syntactic construction exhibit constructional bias set sentence extracted random single corpus investigate performance longer sentence relationship sentence length parse time set sentence length distributed word created hand author added previous test data show relationship sentence length mean parse time bu lc parser contrast result experiment throughput parser bu lc parser sentence word length par sentence fast small proportion parsed wide variability respect bu lc parser absolute variability parse time reflected large",
        "standard deviation table sentence performance contain occurrence passive construction length case group sentence word parser performed likely constraining power parse table improved area difference throughput bu lc revert figure seen experiment standard deviation number par large maximum number par word sentence hand longest sentence fewer par note time taken parse forest unpacking included parse time vary large magnitude result experiment displayed figure quadratic function function suggests bu lc parser parse time related input length previous work anlt briscoe carroll throughput raw corpus data worse observed experiment constant factor fact vocabulary corpus concerned exhibit higher lexical ambiguity sentence taken specific corpus constructional bias observed training phase exploited improve performance samuelsson rayner",
        "experiment anlt grammar loaded set sentence input parser order provide independent basis comparison sentence input sri core language engine cle parser moore alshawi grammar alshawi state art system accessible author sentence taken initial sample representative sentence extracted corpus form anlt package corpus defining type construction grammar intended cover written linguist developed anlt grammar check adverse effect coverage grammar modified grammar development initial sentence grammar failed parse case punctuation required missing corpus anlt grammar failed parse additional sentence removed sample leaving mean length word declarative sentence wh question sentence gap passive sentence containing co ordination show total parse time storage allocated bu lc parser parser parser anlt grammar lexicon parser implemented author similar high standard similar implementation technique parser parser share unification module run lisp environment compiled optimisation setting profiled tool hand optimised similar extent difference performance stem algorithmic implementational reason predictive parser employ symbol lookahead incorporated parsing table lalr technique show result cle parser grammar lexicon figure include garbage collection time phrasal appropriate processing parse forest unpacking grammar analysis similar level detail result parser bu lc parser allocates storage magnitude speed expected given enthusiastic advocation non deterministic parsing researcher tomita wright light improvement observed predictive pure parsing moore dowding assumption incorrect prediction gap main avoidable source performance degradation moore dowding investigation show speed maximum possible anlt grammar throughput parser half parser bu lc parser intermediate term storage allocated difference performance parser fact performs unification expected corresponding finite state automaton determinised avoid theoretical exponential time complexity grammar size paying price run time reason poor performance parser overhead involved maintaining sparse representation chart fact anlt grammar generates packed parse forest parse table state fewer action",
        "second experiment carried cle parser built grammar lexicon replaced version anlt object grammar lexical entry translated cle formalism reverse configuration grammar translated anlt formalism possible central rule contain sequence daughter specified single list variable counterpart anlt simulated throughput configuration fiftieth bu lc parser anlt grammar contains time rule sentence level portion grammar alshawi personal communication point cle parser run grammar containing large number rule contrast anlt parser",
        "mean sentence length experiment shorter word length depending genre etc common real text test sentence cover wide range syntactic construction exhibit constructional bias set sentence extracted random single corpus investigate performance longer sentence relationship sentence length parse time set sentence length distributed word created hand author added previous test data show relationship sentence length mean parse time bu lc parser contrast result experiment throughput parser bu lc parser sentence word length par sentence fast small proportion parsed wide variability respect bu lc parser absolute variability parse time reflected large standard deviation table sentence performance contain occurrence passive construction length case group sentence word parser performed likely constraining power parse table improved area difference throughput bu lc revert figure seen experiment standard deviation number par large maximum number par word sentence hand longest sentence fewer par note time taken parse forest unpacking included parse time vary large magnitude result experiment displayed figure quadratic function function suggests bu lc parser parse time related input length previous work anlt briscoe carroll throughput raw corpus data worse observed experiment constant factor fact vocabulary corpus concerned exhibit higher lexical ambiguity sentence taken specific corpus constructional bias observed training phase exploited improve performance samuelsson rayner",
        "parser theoretical worst case complexity exponential polynomial grammar size large multiplier practical experiment reported previous section parser achieve good throughput general purpose wide coverage grammar natural language likely grammar type considered paper detailed phrase structure component simple unification perspective realistic bring parsing algorithm involved worst case complexity experiment technique result parser worse performance normal technique anlt grammar number state term technique reduces exponential linear grammar size smaller standard table suggests considering complexity parser issue parse table size minor importance realistic grammar long implementation represents table improvement complexity result respect grammar size interesting theoretical standpoint little practical relevance processing natural language schabes claim problem exponential grammar complexity acute natural language processing context input length small word grammar size large hundred thousand rule symbol experiment indicate wide coverage grammar input length parsed input word length occur written text problem grammar size take significance longer input implausible appears fact major problem lie area grammar size input length parser worst case complexity exponential input length theoretical bound suggest parsing performance degraded long sentence relationship length sentence parse time anlt grammar sentence tested appears quadratic reason performance better complexity result suggest important kleene star context analysis coordination rule grammar daughter andvery rule license left right recursion instance sort analyse noun compounding little apparent theoretical difference cle anlt grammar formalism fact explicit formal process tuning parser grammar perform carried anlt clare system result experiment comparing performance respective parser anlt grammar suggests parallel development software grammar occurred appears caused happen likely implementational decision optimisation based subtle property specific grammar important worst case complexity considering practical performance parsing algorithm",
        "kind language model speech understanding suffer imperfect modeling intra sentential contextual influence argue problem addressed clustering sentence training corpus subcorpora criterion entropy reduction calculating separate language model parameter cluster kind clustering offer way represent important contextual effect improve performance model offer automatic mean gather evidence complex context sensitive model general kind linguistic information reward effort required develop clustering improves performance model prof existence context dependency exploited unclustered model evidence claim present result showing clustering improves model atis domain result consistent finding model suggesting existence improvement brought clustering good pointer worth developing unclustered model",
        "speech recognition understanding system kind language model choose word sentence hypothesis evidence acoustic data word word sequence syntactic construction semantic structure likely occur presence likely object sentence hypothesis evidence correctness hypothesis different knowledge source combined attempt optimize selection correct hypothesis alshawi carter rayner rosenfeld knowledge source purpose score sentence hypothesis calculating simple linear combination score associated object n gram grammar rule characterize hypothesis preferred linguistic analysis score viewed log probability taking linear sum corresponds making independence assumption known true rise inaccuracy reduce effectiveness knowledge source obvious way knowledge source accurate increase structure context take account example bigram model replaced trigram fact dependency exist likelihood occurrence grammar rule different location parse tree modeled associating probability state parsing table rule briscoe carroll remedy drawback context extended important influence modeled example dependency word exist separation greater allowed trigram long distance n gram partial remedy associating score parsing table state model important correlation grammar rule extending model increase training data required sparseness problem kept control additional data unavailable expensive collect know advance work extending model particular direction practice improve result turn considerable ingenuity effort wasted paper argue general method extending context sensitivity knowledge source calculates sentence hypothesis score linear combination score object method related iyer involves clustering sentence training corpus number subcorpora predicting different probability distribution linguistic object utterance hypothesis encountered run time treated selected subpopulation sentence represented subcorpora technique address follows drawback alluded able capture important sentence internal contextual effect complexity probabilistic dependency object involved make modest additional demand training data applied standard way knowledge source different kind object improve unclustered model constitutes proof additional unexploited relationship exist linguistic object type model based worth looking specific powerful way model use corpus clustering boost power knowledge source specific hand coded extension example clustered bigram model powerful trigram model clustering important us provide improvement model",
        "absence additional human computational resource required hand coded extension use existence improvement brought clustering good indicator additional performance fact gained extending model hand data collection considerable additional effort extension entail course reason clustering give advantage conjunction extension hand produce improvement evidence claim present experimental result showing particular task training corpus clustering produce sizeable improvement unigram bigram based model trigram based one consistent experience speech understanding community moving bigram trigram produce definite payoff trigram gram yield clear benefit domain question task corpus clustering produce improvement sentence assessed according word contain according syntax rule best parse work go iyer focusing methodological importance corpus clustering usefulness improving overall system performance exploring detail way effectiveness varies dimension language model type language model complexity number cluster differs iyer work clustering utterance paragraph level training corpus thousand million sentence speech application available training data chunked paragraph",
        "work clustering language modeling pereira net addressed problem data sparseness clustering word class predict smoothed probability occurrence event observed training process agglomerative large initial set word clumped smaller number cluster approach described different involves clustering sentence word aim tackle data sparseness grouping large number object smaller number class increase precision model dividing single object training corpus larger number sub object cluster sentence reason clustering sentence prediction combined clustering word reduce sparseness operation orthogonal type clustering based assumption utterance modeled sampled training corpus fall number cluster word object associated utterance probability distribution differ cluster estimating relative likelihood utterance interpretation combining fixed probability associated characteristic view probability conditioned initial choice cluster subpopulation utterance drawn case independence assumption known best reasonable approximation clustering reflects significant dependency worst inaccuracy assumption reduced system performance improve result domain task lend clustering approach obvious trivial case clustering useful speech understander use traveler international airport utterance consist word natural language cluster different language dissimilar clustering significant leverage monolingual case dialogue handling capability system rigid system ask user small number different question modulo filling slot different value example clare interface autoroute package lewin simple dialogue model allows ask dozen different type question user wizard exercise carried collect data task conducted rigid way straightforward divide training corpus cluster cluster utterance following kind system query corpus wall street journal article expected fall cluster different subject area iyer report positive result corpus clustering application obvious extrinsic basis dividing training corpus cluster arpa air travel information atis domain example mention concept place time date fare meal airline plane type ground transportation utterance mention obvious restriction occur utterance human atis database access system structured autoroute case reason automatic clustering attempted ground expect distinct underlying subpopulation exist clustering reflects underlying variability data accurate prediction",
        "different criterion quantifying dis similarity analysis sentence cluster sentence provides good overview criterion selected general impractical find optimal clustering data variety algorithm find optimal solution moment consider case language model consists unigram probability distribution word vocabulary n gram fuller linguistic constraint considered obvious measure similarity sentence cluster jaccard coefficient ratio number word occurring sentence number occurring possibility euclidean distance word vocabulary defining dimension vector space make sense choose similarity measure quantity like final clustering arrangement minimize expected entropy perplexity sentence domain goal analogous work described finding word class clustering simple unigram language model clustering training corpus perplexity minimized likelihood maximized assigning word probability frequency total size corpus corpus likelihood word entropy minimized cover thomas reasoning model language consisting sentence drawn random different subpopulation unigram probability distribution word estimated corpus probability iswhere iteration utterance corpus cluster arise word utterance likelihood utterance arising cluster subpopulation likelihood assigned word cluster relative frequency cluster ideal set cluster maximizes cluster dependent corpus likelihood clustering problem finding global maximum impractical derive good approximation adopt following algorithm random ordering training corpus initialize cluster contain kth sentence ordering remaining training corpus sentence turn creating additional singleton cluster pair cluster entail additional cost smallest reduction value subcorpus seen training utterance incorporated find triple probability maximized parallel cluster movement required practice track overall corpus entropy record contribution cluster make frequency find value quantity possible merged cluster merge second step algorithm chosen minimizing increase entropy unmerged merged cluster adjustment process step algorithm attempt decrease entropy achieve clustering desirable property training sentence predicted cluster belongs cluster heightens similarity cluster difference reduces arbitrariness introduced clustering process order training sentence presented approach applicable minor modification n gram probability word",
        "experiment carried ass effectiveness clustering existence unexploited contextual dependency instance general type language model experiment sentence hypothesis evaluated n gram word word class contained second experiment evaluation basis grammar rule word occurrence experiment reference version set domain relevant class atis sentence allocated cluster unigram bigram trigram condition unigrams bigram run repeated different random order presentation training data unclustered version language model evaluated word sequence word san francisco replaced class name improve performance item entropy training set word entropy ignoring need distinguish different word class unigram language model bigram trigram giving perplexity greater value clustering reduced apparent training set item entropy course thing reducing test set entropy reduction unigrams bigram trigram little variation different run condition improvement clustering measured language model selection n best sentence hypothesis list choice test convenience commitment n best paradigm technique described form speech language interface clustering tested hypothesis list output version decipher speech recognizer murveit simpler bigram model hypothesis output sentence considered list subset sentence set february november atis evaluation set reference sentence occurred hypothesis clustered language model select likely hypothesis list paying attention score decipher assigned hypothesis basis acoustic information bigram model ordering list real system decipher score taken account ignored order maximize discriminatory power test presence test utterance avoid penalizing hypothesis probability assigned hypothesis normalized sentence length probability assigned cluster n gram taken simple maximum likelihood relative frequency value non n gram test data observed training sentence assigned given cluster failure representing small probability assigned number backoff scheme degree sophistication including katz tried produced improvement performance worsened average percentage sentence identified clustering condition given table maximum possible score baseline score expected random choice sentence list unigram",
        "bigram score steady fact significant increase number cluster cluster bigram score fact give advantage unclustered bigram given moving unclustered unclustered trigram clustering trigram produce improvement score fact give small significant deterioration increase number parameter need calculated random choice presentation order data meant different clustering arrived run given condition n gram cluster evidence clustering condition better happening perform particular test data trial needed establish presentation order genuine difference quality clustering appear small improvement available unigram bigram case increasing number cluster second experiment training sentence test sentence hypothesis analysed core language engine alshawi trained atis domain agns sentence discarded sentence word length atis adaptation concentrated sentence word analysis longer sentence reliable slower sentence analysed semantic analysis general created selection basis trained preference function alshawi carter purpose experiment clustering hypothesis selection performed basis word sentence grammar rule construct preferred analysis simplest condition referred rule analogous unigram case word based evaluation sentence modeled bag rule attempt clustering account dependency rule condition rule analogy bigram tried rule occurrence represented isolation context rule parse tree predecessor tree traversed choice assumption dominating rule important influence likelihood occurrence particular rule choice involving sister rule rule related position compilation rule common combination samuelsson rayner worked better purpose illustrate ass way explicit context modeling combined clustering training corpus consisted sentence sentence set analysable consisted word test corpus consisted hypothesis list selected way basis length analysability reference sentence experiment baseline score test corpus expected random choice analysable hypothesis higher word based selection hypothesis list general shorter unanalysable hypothesis having excluded average percentage correct hypothesis actual word string rule represent selected rule rule condition given table result clustering give significant advantage rule rule type model",
        "experiment reference version set domain relevant class atis sentence allocated cluster unigram bigram trigram condition unigrams bigram run repeated different random order presentation training data unclustered version language model evaluated word sequence word san francisco replaced class name improve performance item entropy training set word entropy ignoring need distinguish different word class unigram language model bigram trigram giving perplexity greater value clustering reduced apparent training set item entropy course thing reducing test set entropy reduction unigrams bigram trigram little variation different run condition improvement clustering measured language model selection n best sentence hypothesis list choice test convenience commitment n best paradigm technique described form speech language interface clustering tested hypothesis list output version decipher speech recognizer murveit simpler bigram model hypothesis output sentence considered list subset sentence set february november atis evaluation set reference sentence occurred hypothesis clustered language model select likely hypothesis list paying attention score decipher assigned hypothesis basis acoustic information bigram model ordering list real system decipher score taken account ignored order maximize discriminatory power test presence test utterance avoid penalizing hypothesis probability assigned hypothesis normalized sentence length probability assigned cluster n gram taken simple maximum likelihood relative frequency value non n gram test data observed training sentence assigned given cluster failure representing small probability assigned number backoff scheme degree sophistication including katz tried produced improvement performance worsened average percentage sentence identified clustering condition given table maximum possible score baseline score expected random choice sentence list unigram bigram score steady fact significant increase number cluster cluster bigram score fact give advantage unclustered bigram given moving unclustered unclustered trigram clustering trigram produce improvement score fact give small significant deterioration",
        "second experiment training sentence test sentence hypothesis analysed core language engine alshawi trained atis domain agns sentence discarded sentence word length atis adaptation concentrated sentence word analysis longer sentence reliable slower sentence analysed semantic analysis general created selection basis trained preference function alshawi carter purpose experiment clustering hypothesis selection performed basis word sentence grammar rule construct preferred analysis simplest condition referred rule analogous unigram case word based evaluation sentence modeled bag rule attempt clustering account dependency rule condition rule analogy bigram tried rule occurrence represented isolation context rule parse tree predecessor tree traversed choice assumption dominating rule important influence likelihood occurrence particular rule choice involving sister rule rule related position compilation rule common combination samuelsson rayner worked better purpose illustrate ass way explicit context modeling combined clustering training corpus consisted sentence sentence set analysable consisted word test corpus consisted hypothesis list selected way basis length analysability reference sentence experiment baseline score test corpus expected random choice analysable hypothesis higher word based selection hypothesis list general shorter unanalysable hypothesis having excluded average percentage correct hypothesis actual word string rule represent selected rule rule condition given table result clustering give significant advantage rule rule type model cluster created larger advantage cluster n gram experiment weak evidence clustering better condition",
        "suggested training corpus clustering extend effectiveness general class language model provide evidence particular language model benefit extending hand allow better account context useful reason believe training corpus divide particular number cluster extrinsic ground experimental result presented clustering increase absolute success rate unigram bigram language modeling particular atis task performance improves number cluster climb reasonable upper limit given training sentence cluster improve trigram modeling consistent experience rayner atis domain trigram model inter word effect better bigram extending n gram model beneficial n rule modeling clustering increase success rate n gram suggests conditioning occurrence grammar rule identity mother rule case account contextual influence operate sensible conclude result briscoe carroll complex model grammar rule interaction yield better result conditioning part parse tree mother node included different scheme briscoe carroll observation trigram represent limit usefulness n gram modeling atis non trivial contextual influence exist occurrence grammar rule novel remarkable right interest improvement particular language model application clustering consistent observation important evidence main hypothesis paper enhancing language model clustering software place important clue worth expending research programming data collection machine resource hand coded improvement way language model question model context resource devoted different additional kind language model",
        "recent year resurgence interest statistical approach natural language processing approach new witness statistical approach machine translation suggested weaver current level interest success applying hidden markov model n gram language model speech recognition success measurable term word recognition error rate prompting language processing researcher seek corresponding improvement performance robustness speech translation system necessity combine speech language technology natural place consider combining statistical conventional approach paper describes probabilistic model structural language analysis translation aim provide overall model translation best world factor lead conclude lexicalist statistical model dependency relation suited goal quantitative approach consider constraint logic based approach try distinguish characteristic wish preserve replaced statistical model implicit conventional approach translation characterization logical term given attempt explicit principle proceeding examine fashionable distinction section order clarify issue involved comparing approach attempt argue important distinction rational empirical symbolic statistical distinction qualitative quantitative followed discussion logic based model section overall quantitative model section monolingual model section translation model section conclusion section concentrate information language translation coded expressed logical constraint statistical parameter important little search algorithm rule acquisition parameter estimation",
        "contrast taken granted identification statistical symbolic distinction language processing instance empirical rational debate believe contrast exaggerated validity term accepted practice based approach empirical number way empirical approach adopted grammar development rule set modified according performance corpus natural text taylor class technique learning rule text recent example brill possible imagine building language model probability estimated according intuition reference real data giving probabilistic model empirical language processing labeled statistical involves associating real number valued parameter configuration symbol surprising given natural language written form symbolic classifying system symbolic refer different set internal symbol rule statistical system modeling event involving nonterminal category word sens notion symbol let internal symbol slippery build theory language way classify different theory notion real contrast driving shift statistic language processing contrast qualitative system dealing combinatoric constraint quantitative system involve computing numerical function bear problem brittleness complexity discrete approach language processing share example reasoning system based traditional logical inference relates inadequacy dominant theory linguistics capture shade meaning degree acceptability recognized people field important inherent property natural language qualitative quantitative distinction seen underlying difference classification system based feature specification unification formalism shieber clustering based variable degree granularity pereira unlikely variable aspect fluent natural language captured combinatoric model lead question introduce quantitative modeling language processing necessary quantity quantitative model probability example wish define valued function parse tree reflect extent tree conform minimal attachment parallelism conjuncts function tandem statistical function experiment disambiguation alshawi carter example connection strength neural network approach language processing shown certain network computing probability richard lippmann probability theory offer coherent understood framework selecting uncertain alternative making natural choice quantitative language processing case probability theory strengthened developed empirical methodology form statistical parameter estimation strong connection probability theory formal theory information communication connection exploited speech recognition example concept entropy provide motivated way measuring complexity recognition problem jelinek probability theory remains method choice making language processing quantitative leaf field open term carving language processing appropriate set event probability theory work translation direct approach parameter based surface position word source",
        "target sentence adopted candide system brown capture important structural property natural language account generalization translation independent exact word order source target sentence generalization central qualitative structural approach translation isabelle macklovitch alshawi aim quantitative language translation model presented section employ probabilistic parameter reflect linguistic structure discarding rich lexical information making model complex train term traditional classification seen hybrid symbolic statistical system deal linguistic structure perspective seen quantitative version logic based model model attempt capture similar information organization word phrase relation holding phrase referent tool modeling different",
        "consider hypothetical speech translation system language processing component follow conventional qualitative transfer design hypothetical design component similar existing database query rayner alshawi translation system alshawi recent version system taking quantitative flavor respect choosing alternative analysis hypothetical system purist qualitative approach overall design follows assume speech recognition subsystem delivers list text string corresponding transcription input utterance recognition hypothesis passed parser applies logic based grammar lexicon produce set logical form formula order logic corresponding possible interpretation utterance logical form filtered contextual word sense constraint passed translation component translation relation expressed set order axiom theorem prover derive target language logical form equivalent context source logical form grammar target language applied target form generating syntax tree fringe passed speech synthesizer component turn note undesirable property improved quantitative modeling grammar expressed set syntactic rule axiom set semantic rule axiom support relation form holding string logical form expressed order logic relation form associating string possible logical form interpretation analysis direction given search logical form generation search string given analysis generation treating string logical form object level entity interpretation translation meta level reasoning reasoning logical form proposition list text string handed recognizer parser assumed ordered accordance acoustic scoring scheme internal recognizer magnitude score ignored qualitative language processor process hypothesis time find produce complete logical form interpretation pass grammatical interpretation constraint point discard remaining hypothesis discarding acoustic score taking hypothesis satisfies constraint lead interpretation plausible derivable hypothesis recognition list point processing later hypothesis forced select interpretation random syntactic rule relate category predicate holding string spanning substring limit rule daughter simplicity variable quantified includes lexical axiom particular string consisting single word feature based grammar rule include conjuncts constraining value valued function string main problem grammar notion degree grammatical acceptability sentence grammatical ungrammatical small grammar mean acceptable string rejected large grammar vast number alternative tree chance selecting correct tree simple sentence worse grammar coverage increase problem requiring complex feature set describe idiosyncrasy lexicon grammar axiom belonging specify composition function deriving logical form phrase",
        "subphrases interpretation rule string set lexical semantic rule associating word predicate corresponding word sens particular word syntactic category small finite set word sense predicate order logic assumed semantic representation language come understood practical inferential machinery constraint solving applying machinery requires making logical form fine grained degree warranted information speaker utterance intended convey example explicit scoping lead large number alternative qualitative model difficulty choosing natural language sentence expressed order logic resort elaborate formula requiring complex semantic composition rule rule simplified higher order logic expense practical inferential machinery applying grammar generation faced problem balancing generation tweaking grammatical constraint way prefer grammatical target sentence marginal one approach grammar tend emphasize ability capture generalization main measure success linguistic modeling explain producing appropriate lexical collocation addressed model lexical collocation important fluent generation study collocation generation fit statistical technique illustrated smadja mckeown logic based model interpretation process identifying possible interpretation hold one consistent context interpretation state follows separated context contingent set contextual proposition set monolingual meaning postulate selectional restriction constrain word sense predicate context set assumption sufficient support interpretation given word interpretation abduction hobbs abduction deduction needed arrive assumption common type meaning postulate restriction hyponymy disjointness expressed follows restriction hyponymy disjointness compilation technique mellish allow selectional constraint stated fashion implemented scheme problematic respect start assumption small set sens word awkward difficult arrive optimal granularity sense distinction selectional restriction expressed meaning postulate problematic impossible devise set postulate filter alternative forced filter arbitrary choice remaining alternative quantitative qualitative model transfer approach translation depend interlingual symbol map representation constant associated source language corresponding expression constant target language qualitative model operable notion correspondence based logical equivalence constant source word sense predicate target sense predicate translation relation source logical form target logical form hold set monolingual bilingual meaning postulate set formula characterizing current context set assumption includes assumption supported bilingual meaning postulate order axiom relating source target sense predicate typical bilingual postulate translating form",
        "need assumption arises source language word vaguer possible translation target language different choice target word correspond translation different assumption example condition proved input logical form need assumed general case finding solution pair abductive schema undecidable theorem proving problem alleviated placing restriction form meaning postulate input formula heuristic search method approach applied success limited domain system translating logical form database query rayner alshawi impractical language translation ten thousand sense predicate related axiom intractability issue approach offer principled way choosing alternative solution proposed prover like prefer solution minimal set assumption difficult find motivated definition minimization qualitative framework",
        "grammar expressed set syntactic rule axiom set semantic rule axiom support relation form holding string logical form expressed order logic relation form associating string possible logical form interpretation analysis direction given search logical form generation search string given analysis generation treating string logical form object level entity interpretation translation meta level reasoning reasoning logical form proposition list text string handed recognizer parser assumed ordered accordance acoustic scoring scheme internal recognizer magnitude score ignored qualitative language processor process hypothesis time find produce complete logical form interpretation pass grammatical interpretation constraint point discard remaining hypothesis discarding acoustic score taking hypothesis satisfies constraint lead interpretation plausible derivable hypothesis recognition list point processing later hypothesis forced select interpretation random syntactic rule relate category predicate holding string spanning substring limit rule daughter simplicity variable quantified includes lexical axiom particular string consisting single word feature based grammar rule include conjuncts constraining value valued function string main problem grammar notion degree grammatical acceptability sentence grammatical ungrammatical small grammar mean acceptable string rejected large grammar vast number alternative tree chance selecting correct tree simple sentence worse grammar coverage increase problem requiring complex feature set describe idiosyncrasy lexicon grammar axiom belonging specify composition function deriving logical form phrase subphrases interpretation rule string set lexical semantic rule associating word predicate corresponding word sens particular word syntactic category small finite set word sense predicate order logic assumed semantic representation language come understood practical inferential machinery constraint solving applying machinery requires making logical form fine grained degree warranted information speaker utterance intended convey example explicit scoping lead large number alternative qualitative model difficulty choosing natural language sentence expressed order logic resort elaborate formula requiring complex semantic composition rule rule simplified higher order logic expense practical inferential machinery applying grammar generation faced problem balancing generation tweaking grammatical constraint way prefer grammatical target sentence marginal one approach grammar tend emphasize ability capture generalization main measure success linguistic modeling explain producing appropriate lexical collocation",
        "syntactic rule relate category predicate holding string spanning substring limit rule daughter simplicity variable quantified includes lexical axiom particular string consisting single word feature based grammar rule include conjuncts constraining value valued function string main problem grammar notion degree grammatical acceptability sentence grammatical ungrammatical small grammar mean acceptable string rejected large grammar vast number alternative tree chance selecting correct tree simple sentence worse grammar coverage increase problem requiring complex feature set describe idiosyncrasy lexicon",
        "semantic grammar axiom belonging specify composition function deriving logical form phrase subphrases interpretation rule string set lexical semantic rule associating word predicate corresponding word sens particular word syntactic category small finite set word sense predicate order logic assumed semantic representation language come understood practical inferential machinery constraint solving applying machinery requires making logical form fine grained degree warranted information speaker utterance intended convey example explicit scoping lead large number alternative qualitative model difficulty choosing natural language sentence expressed order logic resort elaborate formula requiring complex semantic composition rule rule simplified higher order logic expense practical inferential machinery applying grammar generation faced problem balancing generation tweaking grammatical constraint way prefer grammatical target sentence marginal one approach grammar tend emphasize ability capture generalization main measure success linguistic modeling explain producing appropriate lexical collocation addressed model lexical collocation important fluent generation study collocation generation fit statistical technique illustrated smadja mckeown",
        "logic based model interpretation process identifying possible interpretation hold one consistent context interpretation state follows separated context contingent set contextual proposition set monolingual meaning postulate selectional restriction constrain word sense predicate context set assumption sufficient support interpretation given word interpretation abduction hobbs abduction deduction needed arrive assumption common type meaning postulate restriction hyponymy disjointness expressed follows restriction hyponymy disjointness compilation technique mellish allow selectional constraint stated fashion implemented scheme problematic respect start assumption small set sens word awkward difficult arrive optimal granularity sense distinction selectional restriction expressed meaning postulate problematic impossible devise set postulate filter alternative forced filter arbitrary choice remaining alternative",
        "quantitative qualitative model transfer approach translation depend interlingual symbol map representation constant associated source language corresponding expression constant target language qualitative model operable notion correspondence based logical equivalence constant source word sense predicate target sense predicate translation relation source logical form target logical form hold set monolingual bilingual meaning postulate set formula characterizing current context set assumption includes assumption supported bilingual meaning postulate order axiom relating source target sense predicate typical bilingual postulate translating form need assumption arises source language word vaguer possible translation target language different choice target word correspond translation different assumption example condition proved input logical form need assumed general case finding solution pair abductive schema undecidable theorem proving problem alleviated placing restriction form meaning postulate input formula heuristic search method approach applied success limited domain system translating logical form database query rayner alshawi impractical language translation ten thousand sense predicate related axiom intractability issue approach offer principled way choosing alternative solution proposed prover like prefer solution minimal set assumption difficult find motivated definition minimization qualitative framework",
        "moving quantitative architecture propose retain basic characteristic qualitative model transfer organization analysis transfer generation component model analysis generation model code contrastive cross linguistic information phrase capturing recursive linguistic structure feature based syntax tree order logical form adopt simpler monostratal representation related found dependency grammar hudson representation large scale qualitative machine translation system mccord notion lexical head phrase central representation concentrate relation lexical head case dependency representation relation include one classified belonging syntax semantics pragmatic salient property language model lexical consists statistical parameter associated relation lexical item number ordering dependent lexical head lexical anchoring facilitates statistical training sensitivity lexical variation collocation order gain benefit probabilistic modeling replace task developing large rule set task estimating large number statistical parameter monolingual translation model give rise new cost trade human annotation judgement tractable automatic training necessitates research lexical similarity clustering pereira dagan improve parameter estimation sparse data model associate phrase relation graph relation graph directed labeled graph consisting set relation edge edge form atomic propositionwhere relation symbol lexical head phrase lexical head phrase subphrase phrase headed node word occurrence representable word index index identifying particular occurrence word discourse corpus set relation symbol open ended argument relation interpreted head second dependent respect relation relation model source target language need overlap language model simple restrict dependency graph tree unordered sibling particular phrase contiguous string word dependent head subphrases algorithmic issue relating representing searching space alternative hypothesis overall design quantitative system follows speech recognizer produce set word position hypothesis form word lattice corresponding set string hypothesis input source language model compute set possible relation graph associated probability string hypothesis probabilistic graph translation model provides source relation graph probability deriving corresponding graph word occurrence target language target graph include word possible translation utterance hypothesis specify surface order word different possible word ordering computed according ordering parameter form target language model following section explain probability processing stage combined select likely target word sequence word sequence handed speech synthesizer tighter integration generation synthesis information derivation",
        "target utterance passed synthesizer probability associated phrase description computed according statistical model analysis translation generation section relationship model arrive overall statistical model speech translation considering training issue paper number familiar technique ranging method maximum likelihood estimation direct estimation annotated data applicable object involved overall model follows omit target speech synthesis assumption proceeds target language word string acoustic evidence source language speech source language word string target language word string source language relation graph target language relation graph spoken input source language wish find target language string likely translation input interested conditional probability given conditional probability expressed follows chang apply simplifying independence assumption concerning relation graph derivation word string independent acoustic information translation independent original word acoustic involved target word string generation target relation edge independent source language representation extent markovian assumption hold depend extent relation edge represent relevant information translation particular mean express aspect surface relevant meaning topicalization predicate argument structure case simplifying assumption following rewritten application bayes rule given constant ignored finding maximum maximizes involves following factor source language acoustic source language generation source content relation source target transfer target language generation assume speech recognizer provides acoustic score proportional log score computed speech recognition system multiplied word based language model probability require application context approach language modeling cover content analysis language generation factor presented section transfer probability fall translation model section note application bayes rule replace factor changing part model formulation allows apply constraint imposed target language model filter inappropriate possibility suggested analysis transfer respect similar dagan itai approach word sense disambiguation statistical association second language",
        "moving quantitative architecture propose retain basic characteristic qualitative model transfer organization analysis transfer generation component model analysis generation model code contrastive cross linguistic information phrase capturing recursive linguistic structure feature based syntax tree order logical form adopt simpler monostratal representation related found dependency grammar hudson representation large scale qualitative machine translation system mccord notion lexical head phrase central representation concentrate relation lexical head case dependency representation relation include one classified belonging syntax semantics pragmatic salient property language model lexical consists statistical parameter associated relation lexical item number ordering dependent lexical head lexical anchoring facilitates statistical training sensitivity lexical variation collocation order gain benefit probabilistic modeling replace task developing large rule set task estimating large number statistical parameter monolingual translation model give rise new cost trade human annotation judgement tractable automatic training necessitates research lexical similarity clustering pereira dagan improve parameter estimation sparse data",
        "model associate phrase relation graph relation graph directed labeled graph consisting set relation edge edge form atomic propositionwhere relation symbol lexical head phrase lexical head phrase subphrase phrase headed node word occurrence representable word index index identifying particular occurrence word discourse corpus set relation symbol open ended argument relation interpreted head second dependent respect relation relation model source target language need overlap language model simple restrict dependency graph tree unordered sibling particular phrase contiguous string word dependent head subphrases algorithmic issue relating representing searching space alternative hypothesis overall design quantitative system follows speech recognizer produce set word position hypothesis form word lattice corresponding set string hypothesis input source language model compute set possible relation graph associated probability string hypothesis probabilistic graph translation model provides source relation graph probability deriving corresponding graph word occurrence target language target graph include word possible translation utterance hypothesis specify surface order word different possible word ordering computed according ordering parameter form target language model following section explain probability processing stage combined select likely target word sequence word sequence handed speech synthesizer tighter integration generation synthesis information derivation target utterance passed synthesizer",
        "probability associated phrase description computed according statistical model analysis translation generation section relationship model arrive overall statistical model speech translation considering training issue paper number familiar technique ranging method maximum likelihood estimation direct estimation annotated data applicable object involved overall model follows omit target speech synthesis assumption proceeds target language word string acoustic evidence source language speech source language word string target language word string source language relation graph target language relation graph spoken input source language wish find target language string likely translation input interested conditional probability given conditional probability expressed follows chang apply simplifying independence assumption concerning relation graph derivation word string independent acoustic information translation independent original word acoustic involved target word string generation target relation edge independent source language representation extent markovian assumption hold depend extent relation edge represent relevant information translation particular mean express aspect surface relevant meaning topicalization predicate argument structure case simplifying assumption following rewritten application bayes rule given constant ignored finding maximum maximizes involves following factor source language acoustic source language generation source content relation source target transfer target language generation assume speech recognizer provides acoustic score proportional log score computed speech recognition system multiplied word based language model probability require application context approach language modeling cover content analysis language generation factor presented section transfer probability fall translation model section note application bayes rule replace factor changing part model formulation allows apply constraint imposed target language model filter inappropriate possibility suggested analysis transfer respect similar dagan itai approach word sense disambiguation statistical association second language",
        "language model viewed term probabilistic generative process based choice lexical head phrase recursive generation subphrases ordering purpose define head word phrase word influence way phrase combined phrase notion central number approach grammar time including theory dependency grammar hudson hudson hpsg pollard sag statistical property association word head phrase active area research chang hindle rooth language model factor statistical derivation sentence word string follows range relation graph content model generation model component overall statistical model spoken language translation given decomposition viewed deciding content sentence formulated set relation edge according statistical model deciding word order according course decomposition simplifies reality language production real language generated context situation imaginary comprehensive model concerned language production context important translation setting produce context source relation graph assume availability model model deriving relation graph phrase taken consist choosing lexical head phrase phrase followed series node expansion step expansion step take node chooses set edge relation label ending node starting node consider case relation graph tree unordered sibling start let simplified case head word optional duplicated dependent relation set edgescorresponding local tree rooted dependent node set relation edge entire derivation union local edge set determine probability deriving relation graph phrase headed use parameter dependency parameter probability given node relation r dependent assumption dependent head chosen probability deriving probability choosing start derivation remove assumption r dependent head need elaborate derivation model include choosing number dependent model parametersthat probability head r dependent refer probability detail parameter previous assumption amounted stating parameter allow model example number adjectival modifier noun degree particular argument verb optional probability expansion giving rise local edge range set relation label r dependent combinatoric constant taking account fact distinguishing permutation dependent permutation r dependent dependent distinct root tree havewhere head set node set edge headed formulation approximation relation graph tree independence assumption allow dependency parameter multiplied hold",
        "general case graph cycle arise natural analysis certain linguistic construction calculating probability node node basis provide probability estimate accurate practical purpose return generation model mentioned includes word set relation generation model concerned surface order possibility use bi relation parameter probability dependent follows dependent approach problematic overall statistical model parameter independent detail parameter specifying number r dependent head adopt use sequencing parameter probability particular ordering dependent given multiset dependency relation known let identity relation stand head parameterswhere sequence relation label including occurrence multiset sequence head relation graph let sequence dependent relation induced particular word string generated range head number occurrence assuming ordering dependent likely use sequencing parameter overall model summarize monolingual model specified topmost head parametersdependency parametersdetail parameterssequencing overall model split contribution content ordering want model example pruning speech recognition hypothesis content ordering model parameter derived combining sequencing parameter detail parameter",
        "language model viewed term probabilistic generative process based choice lexical head phrase recursive generation subphrases ordering purpose define head word phrase word influence way phrase combined phrase notion central number approach grammar time including theory dependency grammar hudson hudson hpsg pollard sag statistical property association word head phrase active area research chang hindle rooth language model factor statistical derivation sentence word string follows range relation graph content model generation model component overall statistical model spoken language translation given decomposition viewed deciding content sentence formulated set relation edge according statistical model deciding word order according course decomposition simplifies reality language production real language generated context situation imaginary comprehensive model concerned language production context important translation setting produce context source relation graph assume availability model",
        "model deriving relation graph phrase taken consist choosing lexical head phrase phrase followed series node expansion step expansion step take node chooses set edge relation label ending node starting node consider case relation graph tree unordered sibling start let simplified case head word optional duplicated dependent relation set edgescorresponding local tree rooted dependent node set relation edge entire derivation union local edge set determine probability deriving relation graph phrase headed use parameter dependency parameter probability given node relation r dependent assumption dependent head chosen probability deriving probability choosing start derivation remove assumption r dependent head need elaborate derivation model include choosing number dependent model parametersthat probability head r dependent refer probability detail parameter previous assumption amounted stating parameter allow model example number adjectival modifier noun degree particular argument verb optional probability expansion giving rise local edge range set relation label r dependent combinatoric constant taking account fact distinguishing permutation dependent permutation r dependent dependent distinct root tree havewhere head set node set edge headed formulation approximation relation graph tree independence assumption allow dependency parameter multiplied hold general case graph cycle arise natural analysis certain linguistic construction calculating probability node node basis provide probability estimate accurate practical purpose",
        "return generation model mentioned includes word set relation generation model concerned surface order possibility use bi relation parameter probability dependent follows dependent approach problematic overall statistical model parameter independent detail parameter specifying number r dependent head adopt use sequencing parameter probability particular ordering dependent given multiset dependency relation known let identity relation stand head parameterswhere sequence relation label including occurrence multiset sequence head relation graph let sequence dependent relation induced particular word string generated range head number occurrence assuming ordering dependent likely use sequencing parameter overall model summarize monolingual model specified topmost head parametersdependency parametersdetail parameterssequencing overall model split contribution content ordering want model example pruning speech recognition hypothesis content ordering model parameter derived combining sequencing parameter detail parameter",
        "mentioned translation model defines mapping relation graph source language target language direct incomplete justification translation relation graph based simple referential view natural language semantics nominal modifier pick entity real imaginary world verb modifier refer action event entity participate role indicated edge relation view purpose translation mapping determine target language relation graph provides best approximation referential function induced source relation graph approximating referential equivalence referential view semantics adequate taking account complexity natural language including aspect quantification distributivity modality mean capture subtlety theory based logical equivalence expected hand proposed logic based approach qualitative model restrict simple order logic computational reason appear practical impoverished lexical relation representation costing practice aspect representation useful translation application convenience partial incremental representation content refine representation addition edge specified denotation meaning sentence required translation pointed discussing logic representation complete specification intended speaker provided denotational semantics set relation edge anticipate possible line developed monotonic semantics alshawi crouch practical model need decompose source target graph subgraphs small subgraph translation parameter estimated help node alignment relation node graph alignment relation similar respect alignment brown surface translation model translation probability sum probability different alignment different way model corresponding different kind alignment relation different independence assumption translation mapping quantitative design adopt simple model lexical relation structural probability assumed independent model alignment relation function word occurrence node word occurrence idea mean source word occurrence gave rise target word occurrence inverse relation need function allowing different number word source target sentence decompose lexical structural probability follows node set set edge target graph factor lexical component account relation source graph lexical component product alignment probability node probability map subset set assumed disjoint different source graph node replace factor product parameter source language word multiset target language word derive target set edge derivation step partition set source edge subgraphs subgraphs rise disjoint set relation edge form structural component",
        "translation model sum derivation probability edge set simplicity assume source graph tree consistent earlier assumption source language model partition source graph edge set local tree ensures partitioning deterministic probability derivation product probability derivation step complex model larger partition rooted node possible require additional parameter partitioning simple model remains specify derivation step probability probability derivation step given parameter form unlabeled graph node alignment function graph relation edge graph node labeled word edge relation label apply derivation step need notion graph matching respect edge label isomorphism modulo node label graph graph function node node derivation step parameter applicable source edge alignment giving rise target edge ifthere isomorphism isomorphism tofor node case condition ensures target graph partition join way compatible node alignment factoring translation model lexical structural component mean overgenerate aspect independent translation real natural language appropriate filter translation hypothesis rescoring according version overall statistical model included factor target language model constrains output translation model course case need model translation relation reverse direction parallel fashion forward direction described",
        "mentioned translation model defines mapping relation graph source language target language direct incomplete justification translation relation graph based simple referential view natural language semantics nominal modifier pick entity real imaginary world verb modifier refer action event entity participate role indicated edge relation view purpose translation mapping determine target language relation graph provides best approximation referential function induced source relation graph approximating referential equivalence referential view semantics adequate taking account complexity natural language including aspect quantification distributivity modality mean capture subtlety theory based logical equivalence expected hand proposed logic based approach qualitative model restrict simple order logic computational reason appear practical impoverished lexical relation representation costing practice aspect representation useful translation application convenience partial incremental representation content refine representation addition edge specified denotation meaning sentence required translation pointed discussing logic representation complete specification intended speaker provided denotational semantics set relation edge anticipate possible line developed monotonic semantics alshawi crouch",
        "practical model need decompose source target graph subgraphs small subgraph translation parameter estimated help node alignment relation node graph alignment relation similar respect alignment brown surface translation model translation probability sum probability different alignment different way model corresponding different kind alignment relation different independence assumption translation mapping quantitative design adopt simple model lexical relation structural probability assumed independent model alignment relation function word occurrence node word occurrence idea mean source word occurrence gave rise target word occurrence inverse relation need function allowing different number word source target sentence decompose lexical structural probability follows node set set edge target graph factor lexical component account relation source graph lexical component product alignment probability node probability map subset set assumed disjoint different source graph node replace factor product parameter source language word multiset target language word derive target set edge derivation step partition set source edge subgraphs subgraphs rise disjoint set relation edge form structural component translation model sum derivation probability edge set simplicity assume source graph tree consistent earlier assumption source language model partition source graph edge set local tree ensures partitioning deterministic probability derivation product probability derivation step complex model larger partition rooted node possible require additional parameter partitioning simple model remains specify derivation step probability probability derivation step given parameter form unlabeled graph node alignment function graph relation edge graph node labeled word edge relation label apply derivation step need notion graph matching respect edge label isomorphism modulo node label graph graph function node node derivation step parameter applicable source edge alignment giving rise target edge ifthere isomorphism isomorphism tofor node case condition ensures target graph partition join way compatible node alignment factoring translation model lexical structural component mean overgenerate aspect independent translation real natural language appropriate filter translation hypothesis rescoring according version overall statistical model included factor",
        "qualitative quantitative model similar overall structure clear parallel factoring logical constraint statistical parameter example monolingual postulate dependency parameter bilingual postulate translation parameter parallelism closer adopted style rule qualitative model argued section qualitative model suffered lack robustness having crudest mean choosing competing hypothesis intractable large vocabulary quantitative model better position cope problem brittle statistical association replaced constraint featural selectional etc satisfied probabilistic model systematic motivated way ranking alternative hypothesis quantitative model let escape undecidability logic based reasoning model lexical hope input word allow effective pruning limiting number search path having high probability retained basic assumption structure language moving quantitative model particular preserved notion hierarchical phrase structure motivated dependency grammar possible giving sensitivity lexical collocation underpin simple statistical model n gram quantitative model reduced overall complexity term set symbol addition word required symbol dependency relation qualitative model required symbol set linguistic category feature set word sense symbol apparent importance translation quantitative system avoid use word sense symbol problem granularity rise exploiting statistical association word target language filter implicit sense choice summary reason combining statistical method dependency representation language translation model inherent lexical sensitivity dependency representation facilitating parameter estimation quantitative preference based probabilistic derivation translation incremental partial specification content utterance useful translation decomposition complex utterance recursive linguistic structure factor suggest dependency grammar play important role language processing system seek combine structural collocational information",
        "principle based parsing probabilistic method analysis natural language popular decade borrows advanced linguistic specification syntax concerned extracting distributional regularity language aid implementation nlp system analysis corpus symbolic statistical approach beginning draw clear exist knowledge language posited year theoretical linguist useful constraining guiding statistical approach corpus available linguist resurrected desire account real language data principled way attempted paper fall approach statistical information derived corpus analysis weight syntactic analysis produced principle parameter parser use probabilistic information principle based grammar parser considered including discussion theoretical computational problem arise partial implementation idea presented preliminary result testing small set sentence",
        "principle parameter paradigm linguistics realised government binding theory chomsky chomsky grammar divided module filter ungrammatical structure level representation level related general transformation sketch organisation t model shown figure work complexity algorithm parse principle based grammar grammar exist accepted defined construct estimated general principle based parsing accomplished exponential time berwick weinberg weinberg feature principle based grammar potential assign meaningful representation string ungrammatical inherent feature phrase structure grammar classify string word language infinite set containing grammatical string ungrammatical string attempt modify grammar parser cope extragrammatical input carbonell hayes douglas dale jensen mellish feature added tends affect statement grammar lack accepted formalism specification principle based grammar crocker lewin define declarative proper branch formalism number different parsing method proper branch set node mother daughter constructed parser simple mechanism shift reduce interpreter licensed principle grammar complete phrase marker input string constructed following manner mother node proper branch daughter node dominating proper branch proper branch binary branching structure grammatical constraint need encoded develops representational reformulation transformational model decomposes syntactic analysis representation type including phrase structure chain coindexation allowing maintain local characterisation principle respect relevant representation type crocker lewin proper branch method axiomatising grammar structure building section parser constrained produce proper branch possible experiment different interpreter structure proposing engine keeping grammar constant",
        "small principle based parser built following proper branch formalism developed crocker lewin grammar use probability ranking parser output seen step implementing principle based parser specified collection grammar module grammar based module taken government binding theory x bar theory theta theory case theory embody spirit constraint found chomsky intended faithful specification syntactic theory single level representation constructed output purpose consulted parser representation interpreted s structure knowledge contained grammar principle given following section bar theory us set schema license local subtrees use parametrised version x bar schema similar muysken employing feature relate state head word theta grid schema figure node includes following feature category standard category name employed spec feature specifies word head phrase built requires specifier comp complement feature redundant information derive value present word theta grid checked formedness theta criterion information referenced comp feature limit number superfluous proper branch generated parser head lexical item node carried projection node theta grid probability occurrence x bar schema obtained sentence preliminary penn treebank corpus wall street journal chosen length head verb phrase main verb set theta role data obtained example parsed author probability calculated following equation specific schema set x bar schema variable category spec comp feature bundle different manner probability collected stochastic context free grammar identity mother node taken account equation result misleading probability x bar schema use schema bring probability parse compared parse string happened use overall x bar likelihood parse computed multiplying probability obtained application schema manner analogous obtain probability phrase marker generated scfg schema way suggests building structure category independent likely verb filled specifier position noun work stochastic context free grammar suggests different set result specific category involved expansion important scfgs tend deny category expand certain way probability claim homogeneous grammar formalism modular theory employed source category specific information obvious use lexical probability specifier complement co occurrence specific head lexical item exihibit property appear category specific fact caused common property",
        "shared lexical item category argued probabilistic information lexical item needed need use category specific information assigning probability syntactic configuration theory concerned assignment argument structure sentence verb number thematic theta role assigned argument transitive verb theta role discharge assigned binary branching formalism employed formalism argument item item sister problem access probability theta application presented easiest method obtaining applying theta probability reference theta grid theta grid word assigned probability dependent particular item grid occurrence theta grid preliminary version penn treebank bracketed corpus analysed extract information sister particular verb penn treebank data unreliable distinguish complement adjunct suitable parsed corpus author access distinction complement adjunct interesting process determining construction fill functional role analysis real text creates number problem hindle rooth discussion issue output fidditch parser hindle probability verb theta grid calculated equation probability theta grid occurring verb occurrence item licensed range theta grid simplest form case theory invokes case filter ensure noun phrase parse assigned case theory differs x bar theta theory category specific np require assigned case implement probabilistic version modular grammar theory incorporating case component relevant question multiple way assigning case noun phrase sentence ambiguity arise presence candidate case assigners theory suggests answer negative case assignment linked theta theory visibility possible receive theta role result use case probability parser unimportant form ambiguity needed module possible satisfy case filter way probability associated module use having provision probability deduced case information implemented parser fact use case parse ranking operation use heterogeneous grammar formalism multiple probability invokes problem combination way mother probability calculated probability information type daughter x bar probability calculating mother x bar probability combination daughter probability feature employed making x bar probability mother dependent stochastic information daughter including theta case probability etc need method combining daughter probability useful figure calculation mother probability involve trial error theory",
        "subject method relevant daughter probability fruitful path follow outset require way integrating probability different module parse progress expensive manner global probability calculated dependent information contained local probability calculation probability partial analysis calculated probability type subanalyses x bar theta probability level calculated distinct figure advantage making pure probability available x bar probability reflect likelihood structure uncontaminated information possible experiment different method combining probability obvious multiplying technique result type probabililty emerging important hand probability calculated parse different type probability account calculation x bar theta etc daughter taken account calculating mother x bar probability probability level pure lot information contained redundant share large subset probability separate calculation gain theoretical insight statistic profitable method combination haphazard affair pure probability parser testing employed method produced separate module probability node lack motivated method combining figure product probability taken global probability parse",
        "x bar theory us set schema license local subtrees use parametrised version x bar schema similar muysken employing feature relate state head word theta grid schema figure node includes following feature category standard category name employed spec feature specifies word head phrase built requires specifier comp complement feature redundant information derive value present word theta grid checked formedness theta criterion information referenced comp feature limit number superfluous proper branch generated parser head lexical item node carried projection node theta grid probability occurrence x bar schema obtained sentence preliminary penn treebank corpus wall street journal chosen length head verb phrase main verb set theta role data obtained example parsed author probability calculated following equation specific schema set x bar schema variable category spec comp feature bundle different manner probability collected stochastic context free grammar identity mother node taken account equation result misleading probability x bar schema use schema bring probability parse compared parse string happened use overall x bar likelihood parse computed multiplying probability obtained application schema manner analogous obtain probability phrase marker generated scfg schema way suggests building structure category independent likely verb filled specifier position noun work stochastic context free grammar suggests different set result specific category involved expansion important scfgs tend deny category expand certain way probability claim homogeneous grammar formalism modular theory employed source category specific information obvious use lexical probability specifier complement co occurrence specific head lexical item exihibit property appear category specific fact caused common property shared lexical item category argued probabilistic information lexical item needed need use category specific information assigning probability syntactic configuration",
        "theta theory concerned assignment argument structure sentence verb number thematic theta role assigned argument transitive verb theta role discharge assigned binary branching formalism employed formalism argument item item sister problem access probability theta application presented easiest method obtaining applying theta probability reference theta grid theta grid word assigned probability dependent particular item grid occurrence theta grid preliminary version penn treebank bracketed corpus analysed extract information sister particular verb penn treebank data unreliable distinguish complement adjunct suitable parsed corpus author access distinction complement adjunct interesting process determining construction fill functional role analysis real text creates number problem hindle rooth discussion issue output fidditch parser hindle probability verb theta grid calculated equation probability theta grid occurring verb occurrence item licensed range theta grid",
        "simplest form case theory invokes case filter ensure noun phrase parse assigned case theory differs x bar theta theory category specific np require assigned case implement probabilistic version modular grammar theory incorporating case component relevant question multiple way assigning case noun phrase sentence ambiguity arise presence candidate case assigners theory suggests answer negative case assignment linked theta theory visibility possible receive theta role result use case probability parser unimportant form ambiguity needed module possible satisfy case filter way probability associated module use having provision probability deduced case information implemented parser fact use case parse ranking operation",
        "use heterogeneous grammar formalism multiple probability invokes problem combination way mother probability calculated probability information type daughter x bar probability calculating mother x bar probability combination daughter probability feature employed making x bar probability mother dependent stochastic information daughter including theta case probability etc need method combining daughter probability useful figure calculation mother probability involve trial error theory subject method relevant daughter probability fruitful path follow outset require way integrating probability different module parse progress expensive",
        "manner global probability calculated dependent information contained local probability calculation probability partial analysis calculated probability type subanalyses x bar theta probability level calculated distinct figure advantage making pure probability available x bar probability reflect likelihood structure uncontaminated information possible experiment different method combining probability obvious multiplying technique result type probabililty emerging important hand probability calculated parse different type probability account calculation x bar theta etc daughter taken account calculating mother x bar probability probability level pure lot information contained redundant share large subset probability separate calculation gain theoretical insight statistic profitable method combination haphazard affair pure probability parser testing employed method produced separate module probability node lack motivated method combining figure product probability taken global probability parse",
        "parser tested sentence containing verb data collected penn treebank corpus sentence created author exhibit degree ambiguity came attaching post verbal phrase adjunct complement order force choice best parse verb probability theta grid noun preposition etc kept constant highest ranked par expected parse exhibiting form mi attachment fact string received multiple par mean number analysis median suggests probabilistic information guide selection single analysis possible result successful approach probabilistic principle based parsing implemented inconclusive nature result obtained number limiting factor implementation including simplicity grammar lack available data",
        "grammar employed partial characterisation chomsky government binding theory chomsky chomsky take account local constraint x bar theta case way encoding constraint proper branch formalism crocker needed grammar sufficient coverage useful corpus analysis formulated problem result obtained implementation given grammar underspecified leaf great task probabilistic information approach viewed putting cart horse usefulness stochastic information parser presumes certain level accuracy achieved grammar elegant theory cognitive syntax shown modular characteristion employed corpus analysis use preliminary penn treebank corpus extraction probability implementation choice forced lack suitable material parsed corpus available contain information specified level required grammar absolute limitation possible extract information semi corpus time constraint entailed rejection approach desirable use probability principle based parsing mirror way syntactic theory government binding handle construction module grammar conspire rule illegal structure derivation elegant result construction passive use probability chain case assignment etc select parse reflected lexical change undergone greater likelihood featuring verb theta grid property number module working hand hand need carried probabilistic domain objection linguist held statistical method disappearing result corpus analysis inadequacy linguistic theory applied occurring data case rise connectionist phoenix brought idea weighted probabilistic function cognition fore freeing hand linguist believe adequate theory grammar elegant construct human implementation usage computational linguist paper shown integration statistical method current linguistic theory goal worth pursuing",
        "grammar employed partial characterisation chomsky government binding theory chomsky chomsky take account local constraint x bar theta case way encoding constraint proper branch formalism crocker needed grammar sufficient coverage useful corpus analysis formulated problem result obtained implementation given grammar underspecified leaf great task probabilistic information approach viewed putting cart horse usefulness stochastic information parser presumes certain level accuracy achieved grammar elegant theory cognitive syntax shown modular characteristion employed corpus analysis",
        "use preliminary penn treebank corpus extraction probability implementation choice forced lack suitable material parsed corpus available contain information specified level required grammar absolute limitation possible extract information semi corpus time constraint entailed rejection approach desirable use probability principle based parsing mirror way syntactic theory government binding handle construction module grammar conspire rule illegal structure derivation elegant result construction passive use probability chain case assignment etc select parse reflected lexical change undergone greater likelihood featuring verb theta grid property number module working hand hand need carried probabilistic domain objection linguist held statistical method disappearing result corpus analysis inadequacy linguistic theory applied occurring data case rise connectionist phoenix brought idea weighted probabilistic function cognition fore freeing hand linguist believe adequate theory grammar elegant construct human implementation usage computational linguist paper shown integration statistical method current linguistic theory goal worth pursuing",
        "lot interest earley deduction pereira warren application parsing generation shieber gerdeman johnson doerre deduction attractive framwork natural language processing following property application reuse partial resultsincremental processing addition new itemshypothetical reasoning keeping track dependency itemsbest search mean agenda earley algorithm approach operate backward chaining interest focussed method certain degree goal directed paper present variant earley deduction find advantageous following reason incrementality portion input string analysed produced generated component decided verbalize grammar assume left corner predicted scanned driven processing algorithm suited processing grammatical theory categorial grammar hpsg allow general prediction use general schema construction specific rule grammar data driven processing appropriate true large coverage rule based grammar lead creation prediction checking algorithm prediction step need costly operation subsumption checking strategy case lexical entry associated preference information information exploited guide heuristic search",
        "earley deduction pereira warren based grammar encoded definite clause instantiation prediction rule earley deduction needed earley deduction prediction inference rule reduction rule literal sequence literal general unifier leftmost literal body non unit clause selected literal principle rule applied pair unit clause non unit clause program derive consequence program order reduce search space achieve goal directed behaviour rule applied pair clause clause selected contribute proof goal set selected clause called chart selection clause guided scanning step section indexing clause section purpose scanning step corresponds lexical lookup chart parser look base case recursive definition serve starting point processing scanning step selects clause appear leaf proof tree given goal following simple definition hpsg recursive definition predicate predicate defined base case predicate lexical restrictive find predicate base case given goal base case instantiated order find useful proving given goal case parsing lookup base case lexical item depend word present input string implied goal predicate constituent order principle determines phon value constituent constructed phon value daughter general assume constituent order principle make use linear non erasing operation combining string case word contained phon value goal lexical item selected unit clause start processing generation analogous condition logical form proposed shieber semantic monotonicity condition requires logical form base case subsume portion goal logical form case lookup defined different grammatical theory direction processing predicate argument goal second argument selected base case following clause defines lookup relation parsing hpsg base case clause instantiated step concatenation difference list operation string base case clause instantiated string follows avoids combination item adjacent input string earley deduction step proving goal perform lookup goal add resulting unit clause chart non unit clause program appear internal node proof tree goal added chart scanning step achieves certain degree goal directedness algorithm clause appear leaf proof tree goal added chart item normal context free chart parsing regarded pair consisting dotted rule",
        "substring item cover pair starting ending position fundamental rule chart parsing make use string position ensure adjacent substring combined result concatenation substring grammar formalism like dcg hpsg complex nonterminals argument feature phon represents covered substring combination substring explicit rule grammar consequence earley deduction need use string position clause pereira warren point use string position known chart parsing inflexible allows concatenation adjacent contiguous substring linguistic theory interest shifted phrase structure rule combine adjacent contiguous constituent toprinciple based approach grammar state general formedness condition describing particular construction hpsg operation string concatenation head wrapping pollard tree adjoining shanker sequence union reape string position known chart parsing inadequate generation pointed shieber generator item position item combined item string position useful indexing item detected combination contribute proof goal important algorithm goal directed processing indexing combination item useless proof goal fact infinitely item termination problem arise example order monotonic grammar formalism us sequence union operation combining string combination item useless result sign word order input string van noord generalize indexing scheme chart parsing order allow different operation combination string improves efficiency detecting combination fail avoiding combination item useless proof goal define item pair clause index idx written example possible indexing scheme indexing scheme needed reuse item useful lcfrs word input string proof generation goal logical form verbalized twice derivation adjacent combination indexing scheme useful order monotonic grammar directional adjacent combination indexing adjacent constituent combined order combination prescribed non directional basic categorial grammar adjacent combination grammar context free backbone combination allows item time proof example non unit clause program represented item form following table summarizes property combination scheme index associated non unit clause index associated unit clause result combining index case non adjacent combination index consist set string position operation union string position provided string position overlap reduction rule augmented handle index combination index use index lookup relation relation goal item following specification",
        "lookup relation provides indexing according string position chart parser usable combination scheme constraint based grammar predicate dealt earley deduction example head feature principle subcategorization principle hpsg head feature principle unifies variable executed compile time need called goal runtime subcategorization principle involves operation list different formalization need processing evaluated resolution argument instantiated managing item proof computational overhead proof terminate case consequence derived base case defined relation order deal goal associate goal body clause goal type goal relevant earley deduction called waiting goal wait activated unit clause unifies goal unit clause combined non unit clause goal waiting goal resulting clause proved according goal type new clause added selected goal waiting goal following inference rule clause mixed goal type sequence goal waiting goal sequence goal starting waiting goal general unifier substitution solution result proving sequence goal order correctness system scanning step add consequence program chart item derived inference rule consequence program clause easy clause added scanning step instance program clause inference rule performs resolution step correctness known logic programming goal type proved resolution potential source incompleteness algorithm scanning step add program clause chart needed proving goal indexing prevent derivation clause needed prove goal order avoid incompleteness scanning step add program clause needed proof goal chart combination index fail inference step useless proof goal lookup relation indexing scheme satisfy property shown particular grammar formalism order search space small finite ensure termination scanning step add item needed proving goal chart indexing chosen way excludes derived item useless proof goal",
        "purpose scanning step corresponds lexical lookup chart parser look base case recursive definition serve starting point processing scanning step selects clause appear leaf proof tree given goal following simple definition hpsg recursive definition predicate predicate defined base case predicate lexical restrictive find predicate base case given goal base case instantiated order find useful proving given goal case parsing lookup base case lexical item depend word present input string implied goal predicate constituent order principle determines phon value constituent constructed phon value daughter general assume constituent order principle make use linear non erasing operation combining string case word contained phon value goal lexical item selected unit clause start processing generation analogous condition logical form proposed shieber semantic monotonicity condition requires logical form base case subsume portion goal logical form case lookup defined different grammatical theory direction processing predicate argument goal second argument selected base case following clause defines lookup relation parsing hpsg base case clause instantiated step concatenation difference list operation string base case clause instantiated string follows avoids combination item adjacent input string earley deduction step proving goal perform lookup goal add resulting unit clause chart non unit clause program appear internal node proof tree goal added chart scanning step achieves certain degree goal directedness algorithm clause appear leaf proof tree goal added chart",
        "item normal context free chart parsing regarded pair consisting dotted rule substring item cover pair starting ending position fundamental rule chart parsing make use string position ensure adjacent substring combined result concatenation substring grammar formalism like dcg hpsg complex nonterminals argument feature phon represents covered substring combination substring explicit rule grammar consequence earley deduction need use string position clause pereira warren point use string position known chart parsing inflexible allows concatenation adjacent contiguous substring linguistic theory interest shifted phrase structure rule combine adjacent contiguous constituent toprinciple based approach grammar state general formedness condition describing particular construction hpsg operation string concatenation head wrapping pollard tree adjoining shanker sequence union reape string position known chart parsing inadequate generation pointed shieber generator item position item combined item string position useful indexing item detected combination contribute proof goal important algorithm goal directed processing indexing combination item useless proof goal fact infinitely item termination problem arise example order monotonic grammar formalism us sequence union operation combining string combination item useless result sign word order input string van noord generalize indexing scheme chart parsing order allow different operation combination string improves efficiency detecting combination fail avoiding combination item useless proof goal define item pair clause index idx written example possible indexing scheme indexing scheme needed reuse item useful lcfrs word input string proof generation goal logical form verbalized twice derivation adjacent combination indexing scheme useful order monotonic grammar directional adjacent combination indexing adjacent constituent combined order combination prescribed non directional basic categorial grammar adjacent combination grammar context free backbone combination allows item time proof example non unit clause program represented item form following table summarizes property combination scheme index associated non unit clause index associated unit clause result combining index case non adjacent combination index consist set string position operation union string position provided string position overlap reduction rule augmented handle index combination index use index",
        "constraint based grammar predicate dealt earley deduction example head feature principle subcategorization principle hpsg head feature principle unifies variable executed compile time need called goal runtime subcategorization principle involves operation list different formalization need processing evaluated resolution argument instantiated managing item proof computational overhead proof terminate case consequence derived base case defined relation order deal goal associate goal body clause goal type goal relevant earley deduction called waiting goal wait activated unit clause unifies goal unit clause combined non unit clause goal waiting goal resulting clause proved according goal type new clause added selected goal waiting goal following inference rule clause mixed goal type sequence goal waiting goal sequence goal starting waiting goal general unifier substitution solution result proving sequence goal",
        "order correctness system scanning step add consequence program chart item derived inference rule consequence program clause easy clause added scanning step instance program clause inference rule performs resolution step correctness known logic programming goal type proved resolution potential source incompleteness algorithm scanning step add program clause chart needed proving goal indexing prevent derivation clause needed prove goal order avoid incompleteness scanning step add program clause needed proof goal chart combination index fail inference step useless proof goal lookup relation indexing scheme satisfy property shown particular grammar formalism order search space small finite ensure termination scanning step add item needed proving goal chart indexing chosen way excludes derived item useless proof goal",
        "practical application desirable search strategy follows promising path search space find preferred solution preferred one situation criterion guide search available base case exampleweighted word hypothesis speech recognizerreadings ambigous word probability assigned stochastic tagger brew correction string error delayed erbach clause associated preference value intended model degree confidence particular solution correct clause associated numerical preference value non unit clause formula determines preference value computed preference value goal body clause value need interpreted probability preference value basis giving priority item unit clause priority identified preference value non unit clause preference formula contain uninstantiated variable priority value formula free variable instantiated highest possible preference value case interpretation probability priority equal maximal possible preference value clause implementation search combine new item chart make use agenda kay new item ordered order descending priority following algorithm earley deduction algorithm parametrized respect relation choice indexing scheme specific different grammatical theory direction processing",
        "proposed earley deduction useful alternative method require subsumption checking restriction avoid prediction loop proposed method improved direction lookup predicate specified user inferred program second problem non unit clause program added chart addition non unit clause dependent goal base case order algorithm directed algorithm combine advantage processing noted kay wiren bouma van noord directed method efficient pure method clear directed method applicable grammar depend concatenation unique left corner connected start symbol remains seen earley deduction compare combined improved earley deduction doerre johnson neumann forthcoming head driven method formed substring table bouma van noord method suited kind problem parsing generation noisy input incremental processing etc",
        "describe section criterion selecting tagset following based noticed useful developing tagger basic french morphological analyser designed statistical tagger number different tag combination high size tagset word associated sequence tag number different combination higher possible sequence single french word consider word joined clitics number different combination higher big tagset cause trouble constraint based tagger refer combination tag single tag statistical tagger big tagset major problem principle forming tagset tagset big andthe tagset introduce distinction resolved level analysis distinction resolved level analysis avoided information tense verb information recovered performing lexicon lookup analysis verb tense ambiguous lost information speech tagger resolve ambiguity instance dort present sleep dormira future sleep tag verb sg p singular person form main verb clause needed lexicon lookup word tag verb sg p assign tense disambiguation tagset lexicon finer distinction tagger hand verb form dit person singular present indicative person singular historic pas simple verb dire introduce distinction form tagged verb sg p determining tense selected given context go scope tagger distinction dit finite verb present past participle distinction handled contextual analysis information concerning mood collapsed way large class ambiguity present indicative present subjunctive resolved motivated fact mood determined remote element connector located distance verb instance conjunction quoique requires subjunctive mood polarity main verb subordinate clause attached play role instance compare form chante tagged verb p mood case faire mood information recovered person plural font fassent indicative subjunctive mood person problematic statistical tagger constraint based tagger instance verb pense ambiguous person sentence pense pa think disambiguated statistical tagger fails person pronoun selects common person reading verb choice collapse second person verb person reason collapse person",
        "ambiguity class contains adjective second person verb sentence secteur matires noun pl plastique adj pl noun pl verb p verb reading plastique impossible noun person sequence common collapsing person cause trouble parsing use tag second person verb second person pronoun collapsed system consistent person analysis straightforward personal pronoun ambiguous verb form ambiguous recovered subject pronoun form lexical item collapsed attached different lemma lexical form sharing category peignent derived verb peigner comb peindre paint coincidental situation rare french el bze case suis person singular auxiliary tre verb suivre follow distinction maintained introduced special tag auxiliary introduced gender distinction far noun adjective determiner concerned feminine noun chaise chair masculine noun tabouret stool receive tag noun sg introduced distinction singular noun noun sg plural noun noun pl number invariant noun noun inv taux rate rate distinction apply adjective determiner main reason choice number unlike gender play major role french respect subject verb agreement noun verb ambiguity major case want tagger resolve gender distinction french tagger counter intuitive major objection choice gender information provide better disambiguation gender ambiguous noun resolved anddisplaying gender provides information strong objection leaving gender information information provide better disambiguation context instance diffuseur diffuse word diffuse ambiguous verb feminine adjective category unlikely masculine noun diffuseur observe gender agreement noun adjective involve long distance dependency instance coordination adjunction noun complement envie soleil diffuse feminine adjective diffuse agrees feminine noun envie word introducing relevant information gender tagset fine information relevant context benefit unclear statistical tagger able use relevant context produce extra error gender interesting albeit minor interest introducing gender distinction problem tagging phrase mon allusion allusion masculine form possessive determiner mon precedes feminine singular noun begin vowel euphonic reason position situation gender distinction help rare expected improvement impaired new error context test suite chanod tapanainen extracted newspaper monde word tagged tagger counted error violated gender agreement avoided mean belong class tagging error problematic sentence interdit tagged adjective",
        "finite verb arme feminine noun interdit masculine adjective make noun adjective sequence impossible particular sentence argument favour gender distinction noun masculine feminine possible difference meaning poste garde manche tour page tagger carry distinction provide sense disambiguation word gender ambiguous word frequent word test corpus counted occurrence word different meaning masculine feminine noun reading number reduced rare reading removed lexicon like masculine ombre kind fish feminine reading mean shadow shade feminine litre religious ornament counted occurrence noun proper noun excluded different meaning masculine feminine reading lve camarade jeune reason distinguish gender noun sparsity immediate context suffice resolve ambiguity disambiguation possible unambiguous masculine feminine modifier attached noun poste poste case preposition noun sequence plural form plural determiner ambiguous respect gender instance test corpus find expression page leur tour ce postes pour le postes responsabilit contextual analysis help disambiguate gender head noun carrying gender information increase disambiguation power tagger disambiguator mark gender distinction tagset provide information reasonable way ass disambiguating power tagger consider ratio initial number ambiguous tag final number tag disambiguation instance difference ambiguity class word table feminine noun finite verb noun finite verb case tagger reduces ambiguity ratio information derived disambiguation matter associating tagged word relevant information base form morphological feature gender definition translation language achieved looking disambiguated word appropriate lexicon derived information intrinsic property tagger point objection hold information important argue ignoring level speech tagging measurable effect overall quality tagger test corpus word error violate gender agreement indicates little accuracy tagger improved introducing gender distinction hand know error introduced distinguished gender avoid category small rare word fit existing category collapsed distinction category useful occurrence training sample category misc miscellaneous word fit existing category account word interjection salutation bonjour onomatopoeia miaou wordparts word exist multi word expression priori priori instance",
        "introduced new category word specific syntactic distribution instance introduced word specific tag prep de word de tag prep word aux specific tag preposition considered reading word removed determiner de tag preposition example sequence likedeterminer noun noun verb prepositionis disambiguated wrong way statistical tagger word ambiguous noun verb singular person tagger prefer noun reading singular noun preposition succeeded fixing modifying tagset effect overall accuracy deteriorated main problem preposition comparable english common preposition specific distribution added new tag prep de prep specific preposition preposition remained marked prep got correct result noticeable change overall accuracy",
        "basic french morphological analyser designed statistical tagger number different tag combination high size tagset word associated sequence tag number different combination higher possible sequence single french word consider word joined clitics number different combination higher big tagset cause trouble constraint based tagger refer combination tag single tag statistical tagger big tagset major problem principle forming tagset tagset big andthe tagset introduce distinction resolved level analysis",
        "distinction resolved level analysis avoided information tense verb information recovered performing lexicon lookup analysis verb tense ambiguous lost information speech tagger resolve ambiguity instance dort present sleep dormira future sleep tag verb sg p singular person form main verb clause needed lexicon lookup word tag verb sg p assign tense disambiguation tagset lexicon finer distinction tagger hand verb form dit person singular present indicative person singular historic pas simple verb dire introduce distinction form tagged verb sg p determining tense selected given context go scope tagger distinction dit finite verb present past participle distinction handled contextual analysis information concerning mood collapsed way large class ambiguity present indicative present subjunctive resolved motivated fact mood determined remote element connector located distance verb instance conjunction quoique requires subjunctive mood polarity main verb subordinate clause attached play role instance compare form chante tagged verb p mood case faire mood information recovered person plural font fassent indicative subjunctive mood",
        "person problematic statistical tagger constraint based tagger instance verb pense ambiguous person sentence pense pa think disambiguated statistical tagger fails person pronoun selects common person reading verb choice collapse second person verb person reason collapse person ambiguity class contains adjective second person verb sentence secteur matires noun pl plastique adj pl noun pl verb p verb reading plastique impossible noun person sequence common collapsing person cause trouble parsing use tag second person verb second person pronoun collapsed system consistent person analysis straightforward personal pronoun ambiguous verb form ambiguous recovered subject pronoun",
        "ignoring gender distinction french tagger counter intuitive major objection choice gender information provide better disambiguation gender ambiguous noun resolved anddisplaying gender provides information strong objection leaving gender information information provide better disambiguation context instance diffuseur diffuse word diffuse ambiguous verb feminine adjective category unlikely masculine noun diffuseur observe gender agreement noun adjective involve long distance dependency instance coordination adjunction noun complement envie soleil diffuse feminine adjective diffuse agrees feminine noun envie word introducing relevant information gender tagset fine information relevant context benefit unclear statistical tagger able use relevant context produce extra error gender interesting albeit minor interest introducing gender distinction problem tagging phrase mon allusion allusion masculine form possessive determiner mon precedes feminine singular noun begin vowel euphonic reason position situation gender distinction help rare expected improvement impaired new error context test suite chanod tapanainen extracted newspaper monde word tagged tagger counted error violated gender agreement avoided mean belong class tagging error problematic sentence interdit tagged adjective finite verb arme feminine noun interdit masculine adjective make noun adjective sequence impossible particular sentence argument favour gender distinction noun masculine feminine possible difference meaning poste garde manche tour page tagger carry distinction provide sense disambiguation word gender ambiguous word frequent word test corpus counted occurrence word different meaning masculine feminine noun reading number reduced rare reading removed lexicon like masculine ombre kind fish feminine reading mean shadow shade feminine litre religious ornament counted occurrence noun proper noun excluded different meaning masculine feminine reading lve camarade jeune reason distinguish gender noun sparsity immediate context suffice resolve ambiguity disambiguation possible unambiguous masculine feminine modifier attached noun poste poste case preposition noun sequence plural form plural determiner ambiguous respect gender instance test corpus find expression page leur tour ce postes pour le postes responsabilit contextual analysis help disambiguate gender head noun carrying gender information increase disambiguation power tagger",
        "disambiguator mark gender distinction tagset provide information reasonable way ass disambiguating power tagger consider ratio initial number ambiguous tag final number tag disambiguation instance difference ambiguity class word table feminine noun finite verb noun finite verb case tagger reduces ambiguity ratio information derived disambiguation matter associating tagged word relevant information base form morphological feature gender definition translation language achieved looking disambiguated word appropriate lexicon derived information intrinsic property tagger point objection hold information important argue ignoring level speech tagging measurable effect overall quality tagger test corpus word error violate gender agreement indicates little accuracy tagger improved introducing gender distinction hand know error introduced distinguished gender",
        "instance introduced new category word specific syntactic distribution instance introduced word specific tag prep de word de tag prep word aux specific tag preposition considered reading word removed determiner de tag preposition example sequence likedeterminer noun noun verb prepositionis disambiguated wrong way statistical tagger word ambiguous noun verb singular person tagger prefer noun reading singular noun preposition succeeded fixing modifying tagset effect overall accuracy deteriorated main problem preposition comparable english common preposition specific distribution added new tag prep de prep specific preposition preposition remained marked prep got correct result noticeable change overall accuracy",
        "lexical transducer french karttunen built xerox lexical tool karttunen beesley karttunen work modify corresponding source lexicon employ finite state calculus map lexical transducer new rule map tag sequence tag new tag straightforward redefining source lexicon imply complex time consuming work initial lexicon contains inflectional information instance word danses plural noun danse second person form verb danser dance following analysis form include clitics analysed sequence item separated symbol depending clitics precede follow head word instance vient il come lit come analysed basic morphological transducer derived new lexicon match reduced tagset described involved major operation handling cliticised form tagger need tagsetsin order reduce number tag cliticised item like vient il split independent token tagging application splitting performed early stage tokeniser dictionary lookup track fact token agglutinated reduces overall ambiguity instance word danses derived expression danses tu dance lit dance verb reading form danses tu tokenised danses form chante t il tokenised chante t turn requires form danses chante t introduced new lexicon respect switching tagsets use contextual level rule turn initial tag new tag void symbol old tag disappear instance symbol verb transformed verb p immediate left context consists symbol symbol indp transduced void symbol vient new token vient get analysed verb p indp verb final transformation consists associating given surface form ambiguity class ordered sequence possible tag instance danses associated ambiguity class noun pl verb p plural noun verb form belongs collapsed second person paradigm",
        "word found lexicon analysed separate finite state transducer guesser developed simple compact efficient guesser french based general assumption neologism uncommon word tend follow regular inflectional pattern guesser based productive ending ment adverb ible adjective verb given ending point category identifies infinitive verb noun possible borrowing english instance ambiguity class killer noun sg verb inf ending belong frequent ending pattern lexicon rare word weight frequent word selected according frequency running text frequent word tend irregular ending shown adverb jamais toujours peut tre hier souvent verb neologism belong regular conjugation paradigm characterised infinitive ending dballaduriser respect noun selected productive ending iste eau eur rice realised better choice assign noun tag ending exception assigned class case situation arise prefix shared noun category ment barred list noun ending aient inflectional marking person plural verb fact introduced hierarchy ending ment shared adverb noun iquement assigned adverb based ending offer advantage unknown word result alternation occur beginning word rest remaining derivational prefix isralo jordano palestinienne oral transcription le oreilles ear marking phonological liaison spelling error account unknown word affect ending internal structure word misspelt verb form appellaient geulait emphasise word har mo ni ser leave ending unaltered advantage operate alternation prefix spelling error applies frequent word follow regular ending pattern instance verb construit adverb trs misspelt constuit trs recognised guesser recognise word belonging closed class conjunction preposition etc assumption closed class described basic lexicon possible improvement guesser incorporate frequent spelling error word recognised extracted corpus newspaper article libration list word unknown basic lexicon unknown word capitalised word analysed guesser proper noun accuracy foreign capitalised word proper noun onomatopoeia ooooh test remaining non capitalised unknown word interesting selected word ran guesser tag assigned word guesser give average tag word word required tag missing tag missing word lacking tag misspelt irregular verb",
        "recognised mean word got required tag guesser tag classified irrelevant concerned word mean word irrelevant tag word got required tag combine evaluation capitalised non capitalised word unknown word tagged guesser necessary tag unwanted one test non capitalised word tough counted irrelevant tag acceptable general ground specific word instance misspelt word statisiticiens tagged adj pl noun pl count adj pl tag irrelevant ground underlying correct word statisticiens noun compare adjective platoniciens occurs word ending ement tagged adv noun sg ending iquement recognised make noun sg tag irrelevant missing tag half adjective tag word tagged noun participle reduces importance error syntactic distribution adjective overlap noun past participle remaining word lack tag include misspelt word belonging closed class come trs vavec irregular verb constuit barbarism resulting omission blank proposde adjunction superfluous blank hyphen quand mme cit example compound noun tagged singular noun rencontres tl plural marking appears element compound foreign word represent class problematic word noun found english example born easy spanish levantarse italian one palazzi",
        "extracted corpus newspaper article libration list word unknown basic lexicon unknown word capitalised word analysed guesser proper noun accuracy foreign capitalised word proper noun onomatopoeia ooooh test remaining non capitalised unknown word interesting selected word ran guesser tag assigned word guesser give average tag word word required tag missing tag missing word lacking tag misspelt irregular verb recognised mean word got required tag guesser tag classified irrelevant concerned word mean word irrelevant tag word got required tag combine evaluation capitalised non capitalised word unknown word tagged guesser necessary tag unwanted one test non capitalised word tough counted irrelevant tag acceptable general ground specific word instance misspelt word statisiticiens tagged adj pl noun pl count adj pl tag irrelevant ground underlying correct word statisticiens noun compare adjective platoniciens occurs word ending ement tagged adv noun sg ending iquement recognised make noun sg tag irrelevant missing tag half adjective tag word tagged noun participle reduces importance error syntactic distribution adjective overlap noun past participle remaining word lack tag include misspelt word belonging closed class come trs vavec irregular verb constuit barbarism resulting omission blank proposde adjunction superfluous blank hyphen quand mme cit example compound noun tagged singular noun rencontres tl plural marking appears element compound foreign word represent class problematic word noun found english example born easy spanish levantarse italian one palazzi",
        "previous work manny rayner author samuelsson rayner attempt tailor existing natural language system specific application domain extracting specialized grammar original large set training example training set treebank consisting implicit parse tree specify verified analysis input sentence parse tree implicit sense node tree mnemonic grammar rule resolved point syntactic category lh grammar rule case ordinary parse tree show example implicit parse tree analysis verified sense analysis judged preferred input sentence human evaluator semi automatic evaluation method new grammar created cutting implicit parse tree treebank appropriate point creating set new rule consist chunk original grammar rule lh new rule lh phrase original grammar rule root tree chunk rh rh phrase rule leaf tree chunk example cutting parse tree figure rule yield rule figure idea create specialized grammar retains high coverage allows parsing turned possible speedup compared original grammar median time achieved cost coverage percent samuelsson benefit method decreased error rate system required select preferred analysis experiment scheme applied grammar version sri core language engine alshawi adapted atis domain speech translation task rayner large corpus real user data collected wizard oz simulation resulting specialized grammar compiled parsing table special parser exploited special property samuelsson technical vehicle extract specialized grammar explanation based generalization ebg mitchell briefly consists redoing derivation training example letting implicit parse tree drive rule expansion process aborting expansion specialized rule extracted current node implicit parse tree meet set tree cutting criterion case extraction process invoked extract subrules rooted current node tree cutting criterion local lh original grammar rule dependent rest parse tree dominate string previous choice node cut cut current node labelled np problem explored arrive optimal choice tree cutting criterion previous scheme specified choice left designer intuition article address problem automating process present method node cut selected information theoretical concept entropy known physic concept perplexity known speech recognition natural language community reason review concept entropy point discus relation perplexity measure disorder example physical system state state probability entropy system thenif state equal probability",
        "thenin case entropy logarithm number state system linguistic example assume trying predict word word string previous one word previous word string language model estimate probability possible word conditional previous word string probability possible word entropy measure hard prediction problem word equal probability entropy logarithm branching factor point input string related entropy follows observed perplexity language model respect imaginary infinite test sequence defined formula jelinek denotes probability word stringsince measure infinite limit terminate finite test string arriving measured perplexity rewriting give uslet exponential expectation value local perplexity measure information content initial string expectation value summation carried possible word equation previous section entropy point input string entropy logarithm local perplexity given point word string word probable local perplexity branching factor point probability differ local perplexity viewed generalized branching factor take account turn task calculating entropy node parse tree different way describe different one small test training set figure wish calculate entropy phrase rule prep named training set lh attached rh rule case rh rule case giving entropy rh preposition prep lexical lookup entropy rh case attache lh rule case lh rule case lexical lookup resulting entropy complete table given want calculate entropy particular node parse tree use phrase entropy rh node sum entropy phrase unified node example entropy rh rule unified lh rule case case",
        "entropy measure disorder example physical system state state probability entropy system thenif state equal probability thenin case entropy logarithm number state system linguistic example assume trying predict word word string previous one word previous word string language model estimate probability possible word conditional previous word string probability possible word entropy measure hard prediction problem word equal probability entropy logarithm branching factor point input string",
        "perplexity related entropy follows observed perplexity language model respect imaginary infinite test sequence defined formula jelinek denotes probability word stringsince measure infinite limit terminate finite test string arriving measured perplexity rewriting give uslet exponential expectation value local perplexity measure information content initial string expectation value summation carried possible word equation previous section entropy point input string entropy logarithm local perplexity given point word string word probable local perplexity branching factor point probability differ local perplexity viewed generalized branching factor take account",
        "turn task calculating entropy node parse tree different way describe different one small test training set figure wish calculate entropy phrase rule prep named training set lh attached rh rule case rh rule case giving entropy rh preposition prep lexical lookup entropy rh case attache lh rule case lh rule case lexical lookup resulting entropy complete table given want calculate entropy particular node parse tree use phrase entropy rh node sum entropy phrase unified node example entropy rh rule unified lh rule case case",
        "following scheme desired coverage specialized grammar prescribed parse tree cut appropriate place having specify tree cutting criterion index treebank tree node correspond alternative choice grammar rule expand node correspond rh phrase grammar rule parse tree involve selecting set node tree node cutnodes entropy node cut node entropy exceeds threshold value rationale wish cut parse tree expect lot variation difficult predict rule resolved corresponds node tree exhibit high entropy value node tree partitioned equivalence class dependent choice cutnodes order avoid redundant derivation parse time selecting particular node cutnode cause node cutnodes entropy threshold threshold entropy yield desired coverage example interval bisection training example matching tree cutting determined cutnodes interesting note textbook method constructing decision tree classification attribute value pair minimize weighted average remaining entropy possible choice root attribute quinlan",
        "treebank partitioned training set test set training set indexed tree extract specialized rule test set check coverage set extracted rule set implicit parse tree stored tree parse tree general form rule identifier dominating list subtrees word training sentence current node tree labelled rule identifier corresponding stored parse tree node follow arc labelled add new reach add node indicating rh phrase grammar rule named follow arc leading node turn accommodate subtrees list arc lead node reached point recursion index corresponding subtree recursion terminates special rule identifier lex dominates word training sentence list subtrees training example figure result tree figure find set node entropy exceed threshold value need calculate entropy node describe different way discus question redundancy resulting set specialized rule equate cutnodes correspond type phrase mean cut node corresponding arc incident labelled grammar rule left hand side np allow specialized rule applicable point one rooted node requires transitivity equate node dominated cutnode equivalent way path cutnode node path cutnode node identical sequence label node equated cutnode cutnode low entropy value following iterative scheme accomplishes set cutnodes augmented induced step selecting set cutnodes practice accomplished compiling graph tree set selected cutnodes set equated node constituted vertex graph traversing simplest scheme calculating entropy node rh phrase parent rule dominating node contributes entropy fact need employ tree tree cutting criterion local parse tree cut elaborate scheme sum entropy node parse tree match node tree letting daughter node contribute entropy lh phrase corresponding grammar rule entropy weighted relative frequency use alternative choice grammar rule example entropy node tree figure calculated follows mother rule contribute entropy associated rh referring table choice rule resolve relative frequency referring entropy table find lh phrase rule",
        "entropy result following entropy node following function determines set cutnodes exceed entropy threshold induced structural equivalence n entropy node version scheme relative frequency daughter node calculate node entropy set arc arc entropy quinlan tends promote daughter cutnodes turn cutnodes result problem instability conjunction additional constraint discussed later section entropy node dependent choice cutnodes redefine function entropy node given set cutnodes ensured modifying termination criterion appropriate set metric size symmetric difference norm like function percent sum size little avail interested solution initial assignment cutnodes use simple interval bisection technique finding appropriate threshold value operate range bound give desired coverage bound midpoint range find cutnodes corresponding value threshold check give desired coverage new bound new upper bound lower upper bound close stop return node corresponding bound termination criterion replaced elaborate implemented follows coverage test set specialized grammar determined set cutnodes need handle boundary case assignment cutnodes give required coverage coverage upper bound entropy difference small problem taken care modifying termination criterion solution omitted sake clarity running example weighted sum phrase entropy node entropy threshold value chosen yield desired coverage single test example figure covered retrieving specialized rule match training example tree current node cutnode cut point training example resulting rule set cut training example threshold value example yield set cutnodes result set specialized rule figure let tree determine set specialized rule cut training example arrive larger number rule combination choice tree correspond training example strategy example extra rule figure correspond training example",
        "set implicit parse tree stored tree parse tree general form rule identifier dominating list subtrees word training sentence current node tree labelled rule identifier corresponding stored parse tree node follow arc labelled add new reach add node indicating rh phrase grammar rule named follow arc leading node turn accommodate subtrees list arc lead node reached point recursion index corresponding subtree recursion terminates special rule identifier lex dominates word training sentence list subtrees training example figure result tree figure",
        "find set node entropy exceed threshold value need calculate entropy node describe different way discus question redundancy resulting set specialized rule equate cutnodes correspond type phrase mean cut node corresponding arc incident labelled grammar rule left hand side np allow specialized rule applicable point one rooted node requires transitivity equate node dominated cutnode equivalent way path cutnode node path cutnode node identical sequence label node equated cutnode cutnode low entropy value following iterative scheme accomplishes set cutnodes augmented induced step selecting set cutnodes practice accomplished compiling graph tree set selected cutnodes set equated node constituted vertex graph traversing simplest scheme calculating entropy node rh phrase parent rule dominating node contributes entropy fact need employ tree tree cutting criterion local parse tree cut elaborate scheme sum entropy node parse tree match node tree letting daughter node contribute entropy lh phrase corresponding grammar rule entropy weighted relative frequency use alternative choice grammar rule example entropy node tree figure calculated follows mother rule contribute entropy associated rh referring table choice rule resolve relative frequency referring entropy table find lh phrase rule entropy result following entropy node following function determines set cutnodes exceed entropy threshold induced structural equivalence n entropy node version scheme relative frequency daughter node calculate node entropy set arc arc entropy quinlan tends promote daughter cutnodes turn cutnodes result problem instability conjunction additional constraint discussed later section entropy node dependent choice cutnodes redefine function entropy node given set cutnodes ensured modifying termination criterion appropriate set metric size symmetric difference norm like function percent sum size little avail interested solution initial assignment cutnodes",
        "use simple interval bisection technique finding appropriate threshold value operate range bound give desired coverage bound midpoint range find cutnodes corresponding value threshold check give desired coverage new bound new upper bound lower upper bound close stop return node corresponding bound termination criterion replaced elaborate implemented follows coverage test set specialized grammar determined set cutnodes need handle boundary case assignment cutnodes give required coverage coverage upper bound entropy difference small problem taken care modifying termination criterion solution omitted sake clarity running example weighted sum phrase entropy node entropy threshold value chosen yield desired coverage single test example figure covered",
        "retrieving specialized rule match training example tree current node cutnode cut point training example resulting rule set cut training example threshold value example yield set cutnodes result set specialized rule figure let tree determine set specialized rule cut training example arrive larger number rule combination choice tree correspond training example strategy example extra rule figure correspond training example",
        "mentioned beginning specialized grammar compiled parsing table finding set cutnodes yield desired coverage result grammar suited parsing particular parser parser employing parsing strategy blend production filtering production applicable point input string naive parser loop parsing table constitute type filtering sufficient guarantee termination case lot spurious application production place degrading performance reason allow learned rule rh refrain cutting node parse tree dominate lexical lookup scheme described successful performance good hand coded tree cutting criterion conjectured effect reduction length short reason spurious rule reduction place corresponding rh phrase stack likelihood happen chance decrease increased rule length second reason number state visited decrease increasing reduction length seen noting number state visited deterministic parser equal number shift action number reduction equal number node corresponding parse tree longer reduction shallow parse tree hand coded operationality criterion result average rule length distribution reduction length percent length percent length sharp contrast scheme accomplishes corresponding figure percent length attempted solution problem impose restriction neighbouring cutnodes way tested select rule rh phrase entropy prescribe node corresponding lh rule chosen cutnode node corresponding rh phrase chosen cutnode vice case conflict node class lowest entropy removed set cutnodes modify function handle set node removed avoid violating constraint neighbouring cutnodes modify termination criterion function assume coverage increase decreased entropy modify interval bisection scheme handle proved reasonable assume coverage monotone side maximum simplifies task",
        "module realizing scheme implemented applied setup previous experiment hand coded tree cutting criterion samuelsson verified parse tree constituted training set test set table summarizes result grammar different coverage extracted hand coded tree cutting criterion tree cutting criterion node entropy taken phrase entropy rh phrase dominating grammar rule tree cutting criterion node entropy sum phrase entropy rh phrase dominating grammar rule weighted sum phrase entropy lh alternative choice grammar rule resolve case experiment carried restriction neighbouring cutnodes discussed previous section mixed entropy scheme important include restriction neighbouring cutnodes case rh phrase entropy scheme potential explanation higher average parsing time grammar extracted induced tree cutting criterion general recursive hand coded criterion allow recursion produce grammar generate finite language hand coded tree cutting criterion better induced one remember produce grammar median allows time faster processing original grammar parser mean induced criterion produce grammar factor slower half order magnitude original setup mean closed research issue attempt realize scheme doubt mind improved",
        "article proposes method finding appropriate tree cutting criterion ebg scheme having hand code ebg scheme proved successful tuning natural language grammar specific application domain achieve parsing cost small reduction coverage developed tested controlling coverage avoiding large number short reduction argued main source poor parser performance instrument blunt enable producing grammar high performance hand coded tree cutting criterion sharpened future research particular refined achieve delicate balance high coverage distribution reduction length biased long reduction banning recursion category specialization example distinguishing np dominate np investigated believed important ingredient version scheme employing hand coded tree cutting criterion",
        "issue sentence categorization according language fundamental nlp document processing fact growing multilingual text corpus data available sentence categorization leading multilingual text structure open wide range application multilingual text analysis information retrieval preprocessing multilingual syntactic parser major difficulty sentence categorization convergence textual error convergence dealing short entry involve discarding language clue textual error document coming different electronic way contain spelling grammatical error character recognition error generated ocr describe approach sentence categorization originality based natural property language training set dependency implementation fast small robust textual error tolerant tested french english spanish german discrimination system give interesting result achieving test correct assignment real sentence resolution power based grammatical word common word alphabet having grammatical word alphabet language disposal system computes likelihood selected language having optimum likelihood tag sentence non resolved ambiguity maintained discus reason lead use linguistic fact present direction improve system classification performance categorization sentence linguistic property show difficult problem simple solution",
        "emergence text categorization according language came need processing text coming world goal text categorization tag text language written retrieval main application field job traditionnal way exploit difference letter combination different language cavnar trenkle language system computes training set profile based frequency probability letter sequence given text computes profile select language closer profile text categorization system good result major problem quality based training set require lot data converge building large representative training set real problem method assume text monolingual result affected dealing multilingual text care natural language property considers text stream character linguistic justification problem quiet different multilingual citation tool process sentence language solves problem switching application function language affect nlp information retrieval field concerned syntactic analysis application based concerned making study particular language multilingual text parasitic noise possible previous method possible sentence small unit converge analysis method precise reveal possible change language remark change language text appear change sentence paragraph included segment quote parenthesis dash colon sentence traditionnal sentence segment included",
        "emergence text categorization according language came need processing text coming world goal text categorization tag text language written retrieval main application field job traditionnal way exploit difference letter combination different language cavnar trenkle language system computes training set profile based frequency probability letter sequence given text computes profile select language closer profile text categorization system good result major problem quality based training set require lot data converge building large representative training set real problem method assume text monolingual result affected dealing multilingual text care natural language property considers text stream character linguistic justification",
        "today problem quiet different multilingual citation tool process sentence language solves problem switching application function language affect nlp information retrieval field concerned syntactic analysis application based concerned making study particular language multilingual text parasitic noise possible previous method possible sentence small unit converge analysis method precise reveal possible change language remark change language text appear change sentence paragraph included segment quote parenthesis dash colon sentence traditionnal sentence segment included",
        "studying quantity text try understand possible way discriminate language present section result research implemented section direction promising section going motivate reason lead choose grammatical word discriminant word proper language different language short numerous build exhaustive list word use discriminant language use discriminant sentence word sentence represent average word omitted structure sentence understandable relying grammatical word allows textual error tolerance foreign word import language usual scientific text important note foreign word import concern noun verb adjective grammatical word rule allow categorize sentence grammatical word short sentence word grammatical word way clue introduce new knowledge improve short sentence categorization improve categorization short sentence simple way use alphabet proper language great common sign accent allows discrimination way improve categorization section possible issue interesting knowledge system coherent multilingual syntactic parser rely grammatical word ending categorization system constitute switch parser vergne vergne remark grammatical word different common word fact common word require training set dependency known representative training set difficult number word hold quiet subjective frequency relative text sentence",
        "section going motivate reason lead choose grammatical word discriminant word proper language different language short numerous build exhaustive list word use discriminant language use discriminant sentence word sentence represent average word omitted structure sentence understandable relying grammatical word allows textual error tolerance foreign word import language usual scientific text important note foreign word import concern noun verb adjective grammatical word rule allow categorize sentence grammatical word short sentence word grammatical word way clue introduce new knowledge improve short sentence categorization",
        "level improve sentence categorization level word morphology level text structure improvement implemented object work way explore improve categorization natural language property syllabation idea check good syllabation word language requires distinguish middle syllabs ending possible way sequence voyells consonant idea sequence proper language dealing text use heuristical knowledge text structure paragraph contiguous sentence written languagetitles paragraph written language bodyincluded block sentence parenthesis written language sentence interesting tool build general document structure recognizer issue field progress lucas lucas know implementation",
        "implementation research divided part sentence tokenization language classification tokenization problem itsef document come different electronic way sentence start capitalized letter finish stop email formated miscellaneous character found abbreviation name number increase problem inserting point space following rule rule exist free style text wrote robust sentence parser solves majority case allowing categorize good condition multilingual sentence realization implement previous idea manage possible point change language included segment section language classification procedure us recursive algorithm handle change context classification principle following word sentence checked word belongs grammatical word list language incremented likelihood selected word morphology let think belongs language incremented likelihood selected sentence name language highest likelihood algorithm linear complexity time",
        "test bed set prepared process french english spanish german use dictionnaries grammatical word language table alphabet decided use different kind document test robustness speed precision textual error tolerance collected scientific text email novel table result obtained expected express fact sentence written grammatical word grammatical word discriminant sentence word word total undeterminations fact corpus show processing included segment quote parenthesis grammatical word clue rely start word sentence grammatical word appear quantity allow perfect deduction result alphabet good discriminate short sentence described implemented improve result case table french corpus program succeeds isolating single language sentence containing word word ambiguity total undetermination single language mean isolating right language error rate concern short sentence mail analysed spanish change language quote sentence unexpected language latin orbi urbi",
        "result obtained expected express fact sentence written grammatical word grammatical word discriminant sentence word word total undeterminations fact corpus show processing included segment quote parenthesis grammatical word clue rely start word sentence grammatical word appear quantity allow perfect deduction result alphabet good discriminate short sentence described implemented improve result case table french corpus program succeeds isolating single language sentence containing word word ambiguity total undetermination",
        "recent year common agreement nlp research community importance having extensive coverage selectional restriction tuned domain work seen semantic type constraint word sense imposes word combine process semantic interpretation different application help parser word sense selection w hirst preferring certain structure grammatical one deciding semantic role played syntactic complement basili interested acquisition sr defining context approach lexical semantics work levin aim work explore feasibility statistical method extracting sr line corpus developed method extracting class based sr line corpus performed experiment basic technique drew limitation corresponding result paper describe substantial modification basic technique report corresponding experimental evaluation outline paper follows section summarize basic methodology ribas analyzing limitation section explore alternative statistical measure ranking hypothesized sr section propose evaluation measure sr learning problem use test experimental result obtained different technique section draw final conclusion establish future line research",
        "technique functionality summarized inputthe training set list complement co occurrence triple verb lemma syntactic relationship noun lemma extracted corpus knowledge useda semantic hierarchy wordnet word clustered semantic class semantic class organized word represented instance different class set syntactic sr verb lemma syntactic relationship semantic class weight final sr disjoint weighted according statistical evidence found corpus stage creation space candidate class appropriateness candidate mean statistical measure appropriate subset candidate space convey sr appropriateness class expressing sr stage quantified strength co occurrence verb class noun corpus resnik verb syntactic relationship candidate class association score assoc defined term assoc try capture different property mutual information ratio measure strength statistical association given verb candidate class given syntactic position compare prior distribution posterior distribution strength association frequency relationship estimated maximum likelihood estimation counting relative frequency event corpus obvious calculate class frequency training corpus tagged case simplistic approach calculate following manner constant factor normalize probabilitieswhen creating space candidate class learning process stage use thresholding technique ignore possible noise introduced training set consider class higher number occurrence threshold selection appropriate class stage based global search candidate way final class disjoint related hyperonymy reported experimental result obtained application technique learn sr performed evaluation sr obtained training set word wall street journal section summarize result conclusion reached paper instance table show sr acquired subject position verb seek indicates manual diagnosis class appropriateness correct ab generalization sens erroneous sens corresponds association score higher value appear induced class incorrect sens suit wsj article sense algorithm considered sens assoc score ranked higher appropriate sense notice ab class general example noun daughter fit data result obtained different experimental evaluation method ribas drew conclusion technique achieves good coverage class acquired result accumulation incorrect sens clear co relation assoc manual diagnosis found slight tendency generalization exists incorrect sens performance presented technique good think detected flaw addressed polysemy noun involved main obstacle practicality technique make association score prefer",
        "technique functionality summarized inputthe training set list complement co occurrence triple verb lemma syntactic relationship noun lemma extracted corpus knowledge useda semantic hierarchy wordnet word clustered semantic class semantic class organized word represented instance different class set syntactic sr verb lemma syntactic relationship semantic class weight final sr disjoint weighted according statistical evidence found corpus stage creation space candidate class appropriateness candidate mean statistical measure appropriate subset candidate space convey sr appropriateness class expressing sr stage quantified strength co occurrence verb class noun corpus resnik verb syntactic relationship candidate class association score assoc defined term assoc try capture different property mutual information ratio measure strength statistical association given verb candidate class given syntactic position compare prior distribution posterior distribution strength association frequency relationship estimated maximum likelihood estimation counting relative frequency event corpus obvious calculate class frequency training corpus tagged case simplistic approach calculate following manner constant factor normalize probabilitieswhen creating space candidate class learning process stage use thresholding technique ignore possible noise introduced training set consider class higher number occurrence threshold selection appropriate class stage based global search candidate way final class disjoint related hyperonymy",
        "evaluation nlp crucial fostering research particular area learning task provide ground compare different technique try sr corpus wordnet section permit measuring utility sr obtained wordnet comparison framework kind knowledge powerful tool detecting flaw particular technique ribas analysis related crucial issue linguistic task reference useful lexicography nlp hand point view lexicography goal evaluation measure quality sr induced resulting class correspond noun corpus hand point view nlp sr evaluated utility help performing reference task lexicography quality concerned think main criterion sr acquired corpus meet correct categorization inferred class correspond correct sens word generalized appropriate generalization level andgood coverage majority noun occurrence corpus generalized induced sr method use assessing accomplishment criterion introspectiona lexicographer check sr accomplish criterion manual diagnosis table intrinsic difficulty approach appropriate comparing different technique learning sr qualitative flavor generalization level appropriatenessa possible measure percentage sense occurrence included induced sr correct called abstraction ratio technique higher abstraction ratio learns class fit set example better manual assessment ratio confirmed behavior testing set lower ratio inducing ab case coverageit measured proportion triple correct sense belongs sr nlp task sr utility evaluated diverse introduced section recent literature grishman sterling resnik task oriented scheme test selectional restriction syntactic ambiguity resolution proposed tested sr w task following scheme testing set algorithm selects appropriate noun sense hyperonym class highest association score sense belongs highest random selection performed acquired algorithm remains undecided result w procedure checked testing sample analyzed precision recall ratio calculated calculated ratio manual automatic match number noun occurrence disambiguated procedure computed ratio manual automatic match total number noun occurrence order evaluate different variant association score impact thresholding performed experiment section analyze result training set word wsj material provided acl dci version penn treebank testing set consisted triple corresponding average common verb treebank rise report seek present considered triple extracted treebank noun",
        "correct sense included wordnet triple called testing sample evaluation measure coverage abstraction ratio recall precision ratio w task section addition performed evaluation hand comparing sr acquired different technique different technique shown table higher coverage better technique succeeds generalizing input example label referring different technique follows corresponds basic association measure section technique introduced section local normalization section log likelihood relative entropy mutual information ratio technique discussed section abstraction ratio different technique shown table principle higher abstraction ratio better technique succeeds filtering incorrect sens ab precision recall ratio noun w task different technique shown table principle higher precision recall ratio better technique succeeds inducing appropriate sr disambiguation task evaluation measure try account different phenomenon goodness particular technique quantified trade result similar difference significative cautious extrapolating result conclusion table worse result measure abstraction good local normalizing technique uniform distribution help local weighting misinform algorithm problem reduced weight polysemous noun informative informed kind local weight section improve technique version assoc local normalization good result technique exploit simpler prior distribution improve basic technique likelihood worse result assoc technique result similar interested measuring impact thresholding sr acquired figure different evaluation measure basic technique varying threshold recall coincide candidate class refused threshold expected threshold increase case classified ratio diverge precision increase recall diminishes show impact thresholding coverage abstraction ratio decrease threshold increase rejecting threshold low small class fit data induced learning general incomplete sr precision abstraction ratio inverse co relation precision grows abstraction decrease term w general class performing class fit data relationship explored future work",
        "section consider different variation association score order robust different technique evaluated section considering prior probability independent context better measure actual association sensible modification measure consider prior distribution chain rule mutual information cover thomas relate different version assoc advantage assoc come information theoretical relationship assoc take account preference selection syntactic position particular class intuitive term typical subject person individual preferred atypical subject suit clothes sr subject contrast assoc second advantage prior probability involve simpler event assoc estimation easier accurate ameliorating data sparseness subsequent modification estimate prior count noun appearing corpus syntactic position restricted head verbal complement way estimation easier accurate global weighting technique presented equation polysemous noun provide evidence sense non ambiguous noun ambiguous noun informative correct class carry ambiguity weight introduced found local manner way polysemous noun evidence sens ambiguous one weight obtained good estimation probability problematic lack tagged training material absence better estimator use poor uniform distribution resnik us local normalization technique normalizes total number class hierarchy scheme present problematic feature ribas detail dependency relationship introduced hyperonymy account noun categorized lower level taxonomy provide weight class higher noun section propose application measure assoc learning sr log likelihood ratio dunning relative entropy cover thomas mutual information ratio church hank gale church section experimental evaluation presented statistical measure detect association distribution defined random variable work measuring deviation conditional distribution expected distribution variable considered independent marginal distribution good approximation association measure low deviating show cross table formed conditional marginal distribution case association measure use information provided cross table different extent assoc mutual information ratio consider deviation conditional probability corresponding marginal hand log likelihood ratio measure association considering deviation conditional cell table corresponding marginals plausible deviation cell taken account assoc help extracting useful sr interesting use information related selectional behavior comparing conditional probability given corresponding marginals entropy job",
        "considering prior probability independent context better measure actual association sensible modification measure consider prior distribution chain rule mutual information cover thomas relate different version assoc advantage assoc come information theoretical relationship assoc take account preference selection syntactic position particular class intuitive term typical subject person individual preferred atypical subject suit clothes sr subject contrast assoc second advantage prior probability involve simpler event assoc estimation easier accurate ameliorating data sparseness subsequent modification estimate prior count noun appearing corpus syntactic position restricted head verbal complement way estimation easier accurate",
        "global weighting technique presented equation polysemous noun provide evidence sense non ambiguous noun ambiguous noun informative correct class carry ambiguity weight introduced found local manner way polysemous noun evidence sens ambiguous one weight obtained good estimation probability problematic lack tagged training material absence better estimator use poor uniform distribution resnik us local normalization technique normalizes total number class hierarchy scheme present problematic feature ribas detail dependency relationship introduced hyperonymy account noun categorized lower level taxonomy provide weight class higher noun",
        "section propose application measure assoc learning sr log likelihood ratio dunning relative entropy cover thomas mutual information ratio church hank gale church section experimental evaluation presented statistical measure detect association distribution defined random variable work measuring deviation conditional distribution expected distribution variable considered independent marginal distribution good approximation association measure low deviating show cross table formed conditional marginal distribution case association measure use information provided cross table different extent assoc mutual information ratio consider deviation conditional probability corresponding marginal hand log likelihood ratio measure association considering deviation conditional cell table corresponding marginals plausible deviation cell taken account assoc help extracting useful sr interesting use information related selectional behavior comparing conditional probability given corresponding marginals entropy job",
        "evaluation nlp crucial fostering research particular area learning task provide ground compare different technique try sr corpus wordnet section permit measuring utility sr obtained wordnet comparison framework kind knowledge powerful tool detecting flaw particular technique ribas analysis related crucial issue linguistic task reference useful lexicography nlp hand point view lexicography goal evaluation measure quality sr induced resulting class correspond noun corpus hand point view nlp sr evaluated utility help performing reference task lexicography quality concerned think main criterion sr acquired corpus meet correct categorization inferred class correspond correct sens word generalized appropriate generalization level andgood coverage majority noun occurrence corpus generalized induced sr method use assessing accomplishment criterion introspectiona lexicographer check sr accomplish criterion manual diagnosis table intrinsic difficulty approach appropriate comparing different technique learning sr qualitative flavor generalization level appropriatenessa possible measure percentage sense occurrence included induced sr correct called abstraction ratio technique higher abstraction ratio learns class fit set example better manual assessment ratio confirmed behavior testing set lower ratio inducing ab case coverageit measured proportion triple correct sense belongs sr nlp task sr utility evaluated diverse introduced section recent literature grishman sterling resnik task oriented scheme test selectional restriction syntactic ambiguity resolution proposed tested sr w task following scheme testing set algorithm selects appropriate noun sense hyperonym class highest association score sense belongs highest random selection performed acquired algorithm remains undecided result w procedure checked testing sample analyzed precision recall ratio calculated calculated ratio manual automatic match number noun occurrence disambiguated procedure computed ratio manual automatic match total number noun occurrence",
        "far lexicography quality concerned think main criterion sr acquired corpus meet correct categorization inferred class correspond correct sens word generalized appropriate generalization level andgood coverage majority noun occurrence corpus generalized induced sr method use assessing accomplishment criterion introspectiona lexicographer check sr accomplish criterion manual diagnosis table intrinsic difficulty approach appropriate comparing different technique learning sr qualitative flavor generalization level appropriatenessa possible measure percentage sense occurrence included induced sr correct called abstraction ratio technique higher abstraction ratio learns class fit set example better manual assessment ratio confirmed behavior testing set lower ratio inducing ab case coverageit measured proportion triple correct sense belongs sr",
        "nlp task sr utility evaluated diverse introduced section recent literature grishman sterling resnik task oriented scheme test selectional restriction syntactic ambiguity resolution proposed tested sr w task following scheme testing set algorithm selects appropriate noun sense hyperonym class highest association score sense belongs highest random selection performed acquired algorithm remains undecided result w procedure checked testing sample analyzed precision recall ratio calculated calculated ratio manual automatic match number noun occurrence disambiguated procedure computed ratio manual automatic match total number noun occurrence",
        "order evaluate different variant association score impact thresholding performed experiment section analyze result training set word wsj material provided acl dci version penn treebank testing set consisted triple corresponding average common verb treebank rise report seek present considered triple extracted treebank noun correct sense included wordnet triple called testing sample evaluation measure coverage abstraction ratio recall precision ratio w task section addition performed evaluation hand comparing sr acquired different technique different technique shown table higher coverage better technique succeeds generalizing input example label referring different technique follows corresponds basic association measure section technique introduced section local normalization section log likelihood relative entropy mutual information ratio technique discussed section abstraction ratio different technique shown table principle higher abstraction ratio better technique succeeds filtering incorrect sens ab precision recall ratio noun w task different technique shown table principle higher precision recall ratio better technique succeeds inducing appropriate sr disambiguation task evaluation measure try account different phenomenon goodness particular technique quantified trade result similar difference significative cautious extrapolating result conclusion table worse result measure abstraction good local normalizing technique uniform distribution help local weighting misinform algorithm problem reduced weight polysemous noun informative informed kind local weight section improve technique version assoc local normalization good result technique exploit simpler prior distribution improve basic technique likelihood worse result assoc technique result similar interested measuring impact thresholding sr acquired figure different evaluation measure basic technique varying threshold recall coincide candidate class refused threshold expected threshold increase case classified ratio diverge precision increase recall diminishes show impact thresholding coverage abstraction ratio decrease threshold increase rejecting threshold low small class fit data induced learning general incomplete sr precision abstraction ratio inverse co relation precision grows abstraction decrease term w general class performing class fit data relationship",
        "coverage different technique shown table higher coverage better technique succeeds generalizing input example label referring different technique follows corresponds basic association measure section technique introduced section local normalization section log likelihood relative entropy mutual information ratio technique discussed section abstraction ratio different technique shown table principle higher abstraction ratio better technique succeeds filtering incorrect sens ab precision recall ratio noun w task different technique shown table principle higher precision recall ratio better technique succeeds inducing appropriate sr disambiguation task evaluation measure try account different phenomenon goodness particular technique quantified trade result similar difference significative cautious extrapolating result conclusion table worse result measure abstraction good local normalizing technique uniform distribution help local weighting misinform algorithm problem reduced weight polysemous noun informative informed kind local weight section improve technique version assoc local normalization good result technique exploit simpler prior distribution improve basic technique likelihood worse result assoc technique result similar",
        "interested measuring impact thresholding sr acquired figure different evaluation measure basic technique varying threshold recall coincide candidate class refused threshold expected threshold increase case classified ratio diverge precision increase recall diminishes show impact thresholding coverage abstraction ratio decrease threshold increase rejecting threshold low small class fit data induced learning general incomplete sr precision abstraction ratio inverse co relation precision grows abstraction decrease term w general class performing class fit data relationship explored future work",
        "paper presented variation affecting association measure thresholding basic technique learning sr line corpus proposed evaluation measure sr learning task experimental result variation reported conclude variation improve result obtained basic technique technique practical application nlp task useful providing experimental insight lexicographer line research concentrate improving local normalization technique solving noun sense ambiguity foreseen application following technique simple technique decide best sense given target noun estimate n gram obtained supervised un supervised corpus different n gram mean smoothing technique combining applying algorithm improve model wordnet hierarchy source backing knowledge way n gram composed decide best sense equal tri gram ancestor class",
        "extragrammatical sentence include ungrammatical construction utterance acceptable syntactic coverage parser difficult one encountered parsing carbonell hayes example people write meaningful sentence addition people prone mistake writing sentence bulk written sentence open extragrammaticality penn treebank tree tagged corpus marcus instance percent rule concerned peculiar sentence include inversive elliptic parenthetic emphatic phrase example drive rule comma comma following sentence robust parser analyze extragrammatical sentence failure try preserve robustness adding rule encounter extragrammatical sentence rulebase grow processing maintaining excessive number rule inefficient impractical extragrammatical sentence handled recovery mechanism set additional rule researcher attempted technique deal extragrammatical sentence augmented transition network atn kwasny sondheimer network based semantic grammar partial pattern matching hayes mouradian conceptual case frame schank multiple cooperating method hayes carbonell mentioned technique account semantic factor depending specific domain question recovering extragrammatical sentence provide better solution ad hoc lack extensibility important recover extragrammatical sentence syntactic factor independent particular system particular domain introduced chart based technique syntactic information extragrammatical sentence technique advantage repeating work chart prevent parser generating edge existed edge recovery process run normal parser terminates performance normal parser decrease case handling grammatical sentence experiment based error running text artificial one generated human word error considered word error occur running text general algorithm error recognition lyon proposed lyon find number error necessary successful parsing recover algorithm oriented based chart advantage mellish parser original parsing algorithm terminates algorithm begin assume error insertion deletion mutation word input including grammatical extragrammatical sentence algorithm generate resultant parse tree cost complete robustness algorithm degrades efficiency parsing generates intermediate edge paper present robust parser recovery mechanism extend general algorithm error recognition adopt recovery mechanism robust parser robust parser handle extragrammatical sentence syntactic information oriented recovery mechanism independent particular system particular domain present heuristic reduce number edge upgrade performance parser paper organized follows review general algorithm error recognition present extension algorithm heuristic adopted",
        "general algorithm error recognition lyon based earley algorithm assumes sentence insertion deletion mutation error terminal symbol objective algorithm parse input string number error state algorithm production number grammar mark position start position state input string error value final state denotes recognition phrase error number component rule stateset position input ordered set state stateset ordered ascending value take descending value adding statesets state candidate admission stateset similar member rejected replaced algorithm work follows procedure scan carried state check correspondence input token terminal symbol rh rule scan completer substitute final state analysis use component handle state checking input terminal requirement state error hypothesis show scan process j th component th word input string match add possible error hypothesis add possible cost insertion error terminal symbol error hypothesis terminal add possible cost deletion error terminal symbol error hypothesis terminal equal add possible cost mutation error terminal symbol handle substitution final state original earley algorithm final state mean recognition nonterminal algorithm section analyze input string number error algorithm handle error terminal symbol consider error nonterminal node real text insertion deletion inversion phrase nonterminal node occurs extend original algorithm order handle error nonterminal symbol extended algorithm scan original algorithm completer modified extended show processing extended completer figure denotes final state rule lh word mean recognition noun phrase completerif final state phrase perfect matchif exists state add insertion error hypothesisif exists state add possible cost insertion error nonterminal symbol deletion error hypothesisif exists state nonterminal add possible cost deletion error nonterminal symbol mutation error hypothesisif exists state nonterminal equal add possible cost mutation error nonterminal symbol extended error recognition algorithm handle terminal error nonterminal error robust parser extended error recognition algorithm overgenerates error hypothesis edge parsing process cope problem adjust error value according following heuristic error value regarded important one edge processed error value error analysis sentence penn treebank corpus wsj show sentence phrase deletion sentence phrase insertion assign error value deletion error hypothesis edge",
        "insertion mutation error error cost terminal symbol error cost nonterminal symbol fiducial nonterminalpeople mistake writing english mistake place small constituent verbal phrase adverbial phrase noun phrase small constituent possibility error occurrence noun phrase lower noun phrase verbal phrase preposition phrase adverbial phrase assume phrase example noun phrase fiducial nonterminals mean error free nonterminals handling sentence robust parser assings error value error hypothesis edge occurring fiducial nonterminal kind terminal symbolssome terminal symbol punctuation symbol conjunction particle misused robust parser assigns error value error hypothesis edge symbol terminal symbol inserted phrase comma inserted phrase surrounded comma parenthesis example assign error value insertion error hypothesis edge nonterminals embraced comma parenthesis weight error terminal node weight error nonterminal node error value edge calculated follows error value additive error value rule terminal node nonterminal node iswhere error value child edge heuristic robust parser process plausible edge processing generated edge time enhance performance robust parser result great reduction number resultant tree",
        "general algorithm error recognition lyon based earley algorithm assumes sentence insertion deletion mutation error terminal symbol objective algorithm parse input string number error state algorithm production number grammar mark position start position state input string error value final state denotes recognition phrase error number component rule stateset position input ordered set state stateset ordered ascending value take descending value adding statesets state candidate admission stateset similar member rejected replaced algorithm work follows procedure scan carried state check correspondence input token terminal symbol rh rule scan completer substitute final state analysis use component handle state checking input terminal requirement state error hypothesis show scan process j th component th word input string match add possible error hypothesis add possible cost insertion error terminal symbol error hypothesis terminal add possible cost deletion error terminal symbol error hypothesis terminal equal add possible cost mutation error terminal symbol handle substitution final state original earley algorithm final state mean recognition nonterminal",
        "algorithm section analyze input string number error algorithm handle error terminal symbol consider error nonterminal node real text insertion deletion inversion phrase nonterminal node occurs extend original algorithm order handle error nonterminal symbol extended algorithm scan original algorithm completer modified extended show processing extended completer figure denotes final state rule lh word mean recognition noun phrase completerif final state phrase perfect matchif exists state add insertion error hypothesisif exists state add possible cost insertion error nonterminal symbol deletion error hypothesisif exists state nonterminal add possible cost deletion error nonterminal symbol mutation error hypothesisif exists state nonterminal equal add possible cost mutation error nonterminal symbol extended error recognition algorithm handle terminal error nonterminal error",
        "robust parser extended error recognition algorithm overgenerates error hypothesis edge parsing process cope problem adjust error value according following heuristic error value regarded important one edge processed error value error analysis sentence penn treebank corpus wsj show sentence phrase deletion sentence phrase insertion assign error value deletion error hypothesis edge insertion mutation error error cost terminal symbol error cost nonterminal symbol fiducial nonterminalpeople mistake writing english mistake place small constituent verbal phrase adverbial phrase noun phrase small constituent possibility error occurrence noun phrase lower noun phrase verbal phrase preposition phrase adverbial phrase assume phrase example noun phrase fiducial nonterminals mean error free nonterminals handling sentence robust parser assings error value error hypothesis edge occurring fiducial nonterminal kind terminal symbolssome terminal symbol punctuation symbol conjunction particle misused robust parser assigns error value error hypothesis edge symbol terminal symbol inserted phrase comma inserted phrase surrounded comma parenthesis example assign error value insertion error hypothesis edge nonterminals embraced comma parenthesis weight error terminal node weight error nonterminal node error value edge calculated follows error value additive error value rule terminal node nonterminal node iswhere error value child edge heuristic robust parser process plausible edge processing generated edge time enhance performance robust parser result great reduction number resultant tree",
        "robust parsing system composed module module normal parser chart parser robust parser error recovery mechanism proposed input sentence processed normal parser sentence grammatical coverage system normal parser succeed analyze normal parser fails robust parser start execute edge generated normal parser result robust parser parse tree grammatical coverage system overview system shown figure usefulness robust parser proposed paper experiment derive rule frequency sentence penn treebank tree tagged corpus wall street journal average frequency rule time corpus rule remove rule occurs fewer time average frequency corpus rule left removed rule peculiar sentence left rule general rule robust parser compensate lack rule rule recovery mechanism heuristic setfirst sentence selected wsj corpus referred proposing robust parser sentence failed normal parsing processed robust parser validity heuristic compare result robust parser heuristic heuristic adaptability robust parser experiment carried sentence atis corpus penn treebank referred propose robust parser sentence atis sentence processed robust parser failure normal parsing adjustmentwe chose best parameter heuristic executing experiment measured percentage constituent test sentence cross penn treebank constituent black show result robust parser wsj table raw mean percentage sentence crossing constituent crossing crossing heuristic robust parser enhance processing time reduce number edge accuracy improved heuristic differentiate edge prefer edge show proposed heuristic valid parsing real sentence experiment say robust parser heuristic recover sentence sentence failed normal parsing percentage crossing sentence result robust parser atis refer accuracy result atis lower wsj parameter heuristic adjusted atis wsj percentage sentence constituent crossing higher wsj sentence atis simple experimental result robust parser high accuracy recovery total rule removed impossible construct complete grammar rule real parsing system succeed analyzing real sentence parsing system extragrammatical sentence",
        "usefulness robust parser proposed paper experiment derive rule frequency sentence penn treebank tree tagged corpus wall street journal average frequency rule time corpus rule remove rule occurs fewer time average frequency corpus rule left removed rule peculiar sentence left rule general rule robust parser compensate lack rule rule recovery mechanism heuristic setfirst sentence selected wsj corpus referred proposing robust parser sentence failed normal parsing processed robust parser validity heuristic compare result robust parser heuristic heuristic adaptability robust parser experiment carried sentence atis corpus penn treebank referred propose robust parser sentence atis sentence processed robust parser failure normal parsing adjustmentwe chose best parameter heuristic executing experiment measured percentage constituent test sentence cross penn treebank constituent black show result robust parser wsj table raw mean percentage sentence crossing constituent crossing crossing heuristic robust parser enhance processing time reduce number edge accuracy improved heuristic differentiate edge prefer edge show proposed heuristic valid parsing real sentence experiment say robust parser heuristic recover sentence sentence failed normal parsing percentage crossing sentence result robust parser atis refer accuracy result atis lower wsj parameter heuristic adjusted atis wsj percentage sentence constituent crossing higher wsj sentence atis simple experimental result robust parser high accuracy recovery total rule removed impossible construct complete grammar rule real parsing system succeed analyzing real sentence parsing system extragrammatical sentence analyzed system robust parser recover extragrammatical sentence accuracy interesting parameter heuristic reflect characteristic test corpus example people tend write sentence inserted phrase parameter increase better result parameter fitted characteristic corpus",
        "paper presented robust parser extended error recognition algorithm recovery mechanism robust parser scaled applied domain parser depends syntactic factor enhance performance robust parser extragrammatical sentence proposed heuristic heuristic assign error value error hypothesis edge edge error value processed generated edge processed robust parser plausible parse tree generated accuracy recovery robust parser parser suitable system real application area short term goal propose automatic method learn parameter value heuristic analyzing corpus expect learned value parameter upgrade performance parser",
        "paper report investigation problem assigning tone pitch contour proposed model intended serve tool phonologist working obtained pitch data tone language motivation exemplification model provided data taken fieldwork bamileke dschang cameroon following recent work liberman provide parametrised prediction function generates value tone sequence explore asymptotic behaviour observe transcribing sequence pitch value amount finding tone sequence combinatorial optimisation problem non deterministic search technique provided genetic algorithm simulated annealing algorithm implementation technique described compared artificial real data sequence tone program adapted tone language adjusting prediction function",
        "wealth literature tone intonation demonstrated voice pitch speech independent linguistic control english voice pitch signal distinction statement question tone language voice pitch signal tense verb describe pitch contour describe speech sequence discrete unit transcription illustrated figure indicates low tone indicates downstepped high tone question addressed paper concern relate pitch contour tone sequence paper divided main section summarised turn section present problem relating sequence value tone transcription argue hidden markov model unsuited task demonstrate importance having computational tool allows phonologist experiment scaling parameter scalingthis section give mathematical basis general approach scaling hoped applicable tone language derive prediction function principle model liberman nigerian language igbo special case bamileke dschanghere present data fieldwork statistical analysis technique liberman general model previous section instantiated language demonstrates versatility general model applied different tone language section provides non deterministic technique transcribing string method us genetic algorithm second method us simulated annealing performance implementation evaluated compared range artificial real data example multiple generated transcription data",
        "promising way generating contour tone sequence specify pitch target tone interpolate target task providing suitable sequence target pierrehumbert beckman clear recognising tone sequence pitch contour markov model hmms huang offer powerful statistical approach problem unclear recognise unit interest phonologist encode timing information way allow output tone syllable vowel section pitch contour correspond tone example look like principled bound context need inspected order resolve ambiguity leading multiplication state information required hmm problem training present context emphasis automatic speech recognition tool support phonologist working tone shall section phonologist identified salient location measure value syllable phonological unit task map string value string tone ladd devised set heuristic identifying key point contour record value connell ladd absence program enshrines heuristic decided develop system producing tone transcription sequence value obvious benefit automating process speed accuracy case possible tone transcription different parameter setting scaling function set tone transcription compatible utterance considerable value analyst searching invariance tonal assignment individual morpheme exemplify point worth considering recent example alternative transcription data proved valuable providing fresh analysis data analysis tone bamileke dschang hyman give transcription stewart give phrase meaning machete dog possibility exist different scaling parameter parameter determine way different tone scaled relative speaker pitch range illustrated adapting hyman earlier notation hyman display kind phonetic interpretation function row tone row number corresponding tone hyman stewart hyman example rising tone symbolised wedge modelled sequence keeping standard practice african tone analysis second row number corresponds downstep upstep hyman model row begin increased downstep encountered stewart model row begin increased downstep encountered decreased upstep encountered row summed row number row stewart hyman model identical parameter distinguishes approach partial total downstep treat dschang partial downstep language appears mid tone respect material left treat total downstep language appears tone respect material left hyman stewart present different analysis different looking transcription",
        "promising way generating contour tone sequence specify pitch target tone interpolate target task providing suitable sequence target pierrehumbert beckman clear recognising tone sequence pitch contour markov model hmms huang offer powerful statistical approach problem unclear recognise unit interest phonologist encode timing information way allow output tone syllable vowel section pitch contour correspond tone example look like principled bound context need inspected order resolve ambiguity leading multiplication state information required hmm problem training present context emphasis automatic speech recognition tool support phonologist working tone shall section phonologist identified salient location measure value syllable phonological unit task map string value string tone",
        "connell ladd devised set heuristic identifying key point contour record value connell ladd absence program enshrines heuristic decided develop system producing tone transcription sequence value obvious benefit automating process speed accuracy case possible tone transcription different parameter setting scaling function set tone transcription compatible utterance considerable value analyst searching invariance tonal assignment individual morpheme exemplify point worth considering recent example alternative transcription data proved valuable providing fresh analysis data analysis tone bamileke dschang hyman give transcription stewart give phrase meaning machete dog possibility exist different scaling parameter parameter determine way different tone scaled relative speaker pitch range illustrated adapting hyman earlier notation hyman display kind phonetic interpretation function row tone row number corresponding tone hyman stewart hyman example rising tone symbolised wedge modelled sequence keeping standard practice african tone analysis second row number corresponds downstep upstep hyman model row begin increased downstep encountered stewart model row begin increased downstep encountered decreased upstep encountered row summed row number row stewart hyman model identical parameter distinguishes approach partial total downstep treat dschang partial downstep language appears mid tone respect material left treat total downstep language appears tone respect material left hyman stewart present different analysis different looking transcription analyzing data given interpretation function phonologist wish limit transcription result certain parameter setting phonetic interpretation function better working number sequence row paper describes tool let",
        "consider contour figure particular note decay non asymptote appear different asymptote symbolise observation clearer figure speaking display peak valley figure artificial example remains true principled upper limit number downsteps occur utterance clements asymptotic behaviour scaling need addressed suppose sequence tone ith tone sequence value value corresponding like formula predicts given express follows question function look suppose sake argument ratio preceding figure constant respect baseline suppose sequence arbitrary tone possibility downstep present static tone system sequence level sequence like hlhlhl realised simple oscillation pitch write following formula situation interesting allow downdrift downstep automatic lowering second tone intervenes hlh realised downstep lowering second tone intervening lost realised hyman schuh dschang downstep downdrift igbo downdrift define equation following factor called transition ratio shall general equation relates equation igbo liberman reproduced instantiated set equation setting follows introduce level generality adjacent value like relate non adjacent value given sequence intervening tone tone sequence value shall write value repeated application write following expression suppose tone sequence straightforward result useful section worth comparing hyman stewart interpretation function illustrated pointed hyman partial downstep model stewart total downstep model total downstep visualised follows dotted line indicate register tone scaled downstep corresponds lowering register partial downstep necessary downsteps high tone level preceding low total downstep necessary single downstep high tone level preceding low express observation partial total downstep model follows partial downstep total downstep equation forced justifiable view data figure argued indicates flaw model presented partial total downstep attested literature tone language possible general provide model partial total downstep permit distinct asymptote",
        "extent figure typical tone language having different asymptote conclude total partial downstep qualitative term emerge model different guise shall effect distinction partial total downstep allow different transcription string saw general following mapping transcription view clear changing view downstep amount adding deleting leaving tone unchanged model admits transcription scheme result view shown concludes discussion prediction function section shall investigate phonetic interpretation tone bamileke dschang determine value language",
        "recent field trip western cameroon study bamileke dschang noun associative construction able collect small data relating scaling particular informant pitch range liberman voice pitch varied getting informant speak different volume adjusting recording level asking informant imagine speaking subject different distance controlled volume having informant wear headphone played white noise detuned radio set informant voice pitch volume control radio hypothesis technique produce consistent volume pitch scaling long utterance informant self conscious speaking asking imagine speaking subject distance taken following data data available isolated disyllable sequence available data utterance hypothesise behaviour unseen sequence tested subsequent empirical work result utterance involving sequence displayed figure result displayed figure regression equation obtained data displayed number occurrence tone sequence given parenthesis sequence column give standard error gradient intercept conclude sequence intercept different sequence conclude value referred quantity figure sufficient determine value pair tone bamileke dschang observation bamileke dschang downdrift difference hlh lhl sequence evident figure write result showed follows downstep shall assume magnitude downstep independent tone separate instrumental study support hypothesis bird stegen tone important consider upstep analysis bamileke dschang stewart upstep downstep intended inverse identity complete table observe symmetry table configuration value find downstepped column reproduced column downstep multiplied column upstep divided table dependent data transcribed repetition transcription scheme based partial downstep scheme based total downstep table follows fact possible table cause alarm transition tone involves factor factor manifested tone transition according following pattern conclude presence table indicates interplay value ratio raise interesting question tone sequence interpretation function based circumstance phonetic interpretation sequence",
        "respective interpretation function sufficient condition reader check condition met mapping table given observation hold model general specialised version model applied bamileke dschang shown determined specified possible characterisation total partial downstep arises total downstep partial downstep interpretation term different standard interpretation shown standard interpretation compatible present model concludes discussion scaling bamileke dschang shall present implementation",
        "section possible program produce sequence tone tone transcription given sequence value program crucial use prediction function evaluating candidate tone transcription program involve search general aim searching discover value optimise value specified evaluation function local optimum deterministic method hill climbing perform terminate local optimum particular found depends starting point search way choosing good starting point search global optimum option search space large present context sequence tone search space contains possible tone transcription thousand possible parameter setting large search space exhaustive search reasonable computation time deterministic search method devised way tackling large scale combinatorial optimisation problem problem involve finding optimum function discrete variable method designed yield approximate solution reasonable computation time known method genetic search goldberg annealing search van laarhoven aarts annealing search applied learning phonological constraint expressed finite state automaton ellison following section describe genetic algorithm annealing algorithm tone transcription problem cogent introduction genetic search explanation work reader referred south presenting version algorithm implementation shall define key data type us standard operation type linear encoding solution present setting array tone tone gene contains bit encoding parameter encoding scaled floating point number range poolan array gene search parameter size known population gene pool renewed generation number generation search parameter measure fitness gene solution problem sequence value wish transcribe particular gene evaluation function follows operation take gene produce single gene result crossover function defined follows selected crossover point word gene cut position determined spliced second create new gene build idea good gene tend produce good offspring suppose transcription contained good rest poor transcription contained poor rest good offspring containing second improvement possible offspring worse survive generation program performs kind crossover parameter employing independent crossover point randomising argument order high order bit offspring come parent extension crossover allows crossing point",
        "current model permit arbitrary number crossing point crossover transcription string resulting gene optimal choose crossing point way minimise position developing system exploiting decomposability evaluation function way caused significant improvement system performance version simple crossover generation create new gene pool previous new gene created mating best chosen gene best chosen gene order maintain genetic diversity element randomness search initial configuration operation applied gene generation certain probability known mutation probability gene tone tone set possible tone parameter encoding mutated mutation rate set raised single generation evaluation best gene improvement evaluation best gene generation best gene mutated building block genetic search discussed structured following algorithm expressed pseudo pascal main loop executed generation time loop program check performance generation performance good mutation rate stay low changed high copy best gene new pool reach inner loop selects gene performs crossover mutates result current pool updated evaluation performed program continues generation generation completed program display best gene final population terminates genetic algorithm simulated annealing van laarhoven aarts combinatorial optimisation technique based analogy natural process heating slow cooling solid allows formation regular crystalline structure having minimum excess energy early stage temperature annealing search resembles random search free energy system transition higher energy state probable temperature decrease search begin resemble hill climbing free energy transition higher energy state likely follows explain parameter annealing search current implementation start search temperature set search temperature reduced rate set cooling rate parameter reach value step search current state perturbed depends temperature temperature determines fraction search space covered single perturbation step tone sequence length reset worst tone according parameter proceed follows exemplified set add random number range check result range temperature system required reach thermal equilibrium temperature lowered present context equilibrium reached perturbation yielded new state accepted energy functionthis available energy transition higher energy state current system distribution uniform",
        "random variable range energy difference old new state available energy transition accepted factor intended scale energy distribution typical value evaluation function algorithm presented program loop outer loop iterates temperature range beginning temperature decreasing get close nested loop performs task reaching thermal equilibrium temperature step perturb previous transcription new temperature parameter perturb function difference old new evaluation calculated new transcription better evaluation old negative program accepts new transcription ifis negative oris positive sufficient free energy system allow worse transcription accepted check new transcription best transcription found besttrans set besttrans new transcription equilibrium reached current transcription set best transcription found search continues genetic annealing search algorithm implemented section performance implementation compared statistic based execution program parameter set execution took second sun sparc performance trial undertaken trial program generated random sequence tone computed corresponding sequence set transcribing sequence sequence ideal best possible evaluation transcription performance program measured came finding optimal solution program tested sequence length length program transcribed generated sequence result displayed figure pair bar corresponds given transcription length left member pair genetic search program right member annealing search program shaded bar corresponding evaluation important indicate number time program found transcription evaluation evaluation mean average squared difference predicted value actual value hz observe annealing search program performs case mutation operation genetic search program treat bit parameter encoding perturbation operation annealing search program sensitive distinction significant significant bit explain better convergence behaviour annealing search figure performance degrade transcription length length double generated sequence contain downsteps second tone average causing general downtrend value limiting combinatorial explosion possible transcription trial time upstep permitted result displayed figure annealing program fare better genetic program bar corresponding evaluation program observe performance degrades",
        "trial inclusion upstep increase number possible transcription number local optimum final trial involved real data including data utterance given figure trial involved subtrials sequence length fourth length second sequence taken extracting initial value fourth sequence avoiding asymptotic behaviour longer sequence data tabulated come sentence result given figure interpretation shading figure different previous figure evaluation likely real data fact annealing program found evaluation genetic program found evaluation program performed finding transcription evaluation shall display transcription indication time program found transcription genetic annealing transcription occurred program execution result trial deserve special attention trial transcription found program best evaluation found given striking note transcription hyman stewart given transcription sequence demonstrated transcription point possibility given encouraging sign program living promise producing alternative acceptable transcription desired analytical standpoint seen transcription given sequence inconvenient required run program time order solution found program designed caught local optimum problem interesting alternative transcription local optimum program set report best solution user specifies number solution desired program ensures area search space explored subsequent search defining distance metric transcription count number tone transcription changed order identical transcription search space distance found solution explored program finding solution generated transcription fall distance previous solution consider following generated sequence tone annealing program set task finding transcription tone sequence program run reported following solution evaluation equal running program found solution order note transcription taken begin initial upstep downstep effect phonetic interpretation following display predicted value given solution facilitate comparison input sequence execution point based table value decided try test second table value performance different solution execution found new solution found",
        "value solution cluster clustering occurring ratio h l analysis relationship kind solution found table parameter value attempted unsatisfying performance program dependent setting search parameter combinatorial optimisation problem find good parameter setting trial error approach found optimal parameter value conclude performance comparison annealing search genetic search problem tone transcription thoroughgoing comparison approach problem need undertaken parameter continuous variable evaluation function write continuous function worthwhile try deterministic search method optimising candidate tone transcription found interesting integrate system one presented speech workstation phonologist identifies salient point cursor system transcription",
        "cogent introduction genetic search explanation work reader referred south presenting version algorithm implementation shall define key data type us standard operation type linear encoding solution present setting array tone tone gene contains bit encoding parameter encoding scaled floating point number range poolan array gene search parameter size known population gene pool renewed generation number generation search parameter measure fitness gene solution problem sequence value wish transcribe particular gene evaluation function follows operation take gene produce single gene result crossover function defined follows selected crossover point word gene cut position determined spliced second create new gene build idea good gene tend produce good offspring suppose transcription contained good rest poor transcription contained poor rest good offspring containing second improvement possible offspring worse survive generation program performs kind crossover parameter employing independent crossover point randomising argument order high order bit offspring come parent extension crossover allows crossing point current model permit arbitrary number crossing point crossover transcription string resulting gene optimal choose crossing point way minimise position developing system exploiting decomposability evaluation function way caused significant improvement system performance version simple crossover generation create new gene pool previous new gene created mating best chosen gene best chosen gene order maintain genetic diversity element randomness search initial configuration operation applied gene generation certain probability known mutation probability gene tone tone set possible tone parameter encoding mutated mutation rate set raised single generation evaluation best gene improvement evaluation best gene generation best gene mutated building block genetic search discussed structured following algorithm expressed pseudo pascal main loop executed generation time loop program check performance generation performance good mutation rate stay low changed high copy best gene",
        "genetic algorithm simulated annealing van laarhoven aarts combinatorial optimisation technique based analogy natural process heating slow cooling solid allows formation regular crystalline structure having minimum excess energy early stage temperature annealing search resembles random search free energy system transition higher energy state probable temperature decrease search begin resemble hill climbing free energy transition higher energy state likely follows explain parameter annealing search current implementation start search temperature set search temperature reduced rate set cooling rate parameter reach value step search current state perturbed depends temperature temperature determines fraction search space covered single perturbation step tone sequence length reset worst tone according parameter proceed follows exemplified set add random number range check result range temperature system required reach thermal equilibrium temperature lowered present context equilibrium reached perturbation yielded new state accepted energy functionthis available energy transition higher energy state current system distribution uniform random variable range energy difference old new state available energy transition accepted factor intended scale energy distribution typical value evaluation function algorithm presented program loop outer loop iterates temperature range beginning temperature decreasing get close nested loop performs task reaching thermal equilibrium temperature step perturb previous transcription new temperature parameter perturb function difference old new evaluation calculated new transcription better evaluation old negative program accepts new transcription ifis negative oris positive sufficient free energy system allow worse transcription accepted check new transcription best transcription found besttrans set besttrans new transcription equilibrium reached current transcription set best transcription found search continues",
        "genetic annealing search algorithm implemented section performance implementation compared statistic based execution program parameter set execution took second sun sparc performance trial undertaken trial program generated random sequence tone computed corresponding sequence set transcribing sequence sequence ideal best possible evaluation transcription performance program measured came finding optimal solution program tested sequence length length program transcribed generated sequence result displayed figure pair bar corresponds given transcription length left member pair genetic search program right member annealing search program shaded bar corresponding evaluation important indicate number time program found transcription evaluation evaluation mean average squared difference predicted value actual value hz observe annealing search program performs case mutation operation genetic search program treat bit parameter encoding perturbation operation annealing search program sensitive distinction significant significant bit explain better convergence behaviour annealing search figure performance degrade transcription length length double generated sequence contain downsteps second tone average causing general downtrend value limiting combinatorial explosion possible transcription trial time upstep permitted result displayed figure annealing program fare better genetic program bar corresponding evaluation program observe performance degrades trial inclusion upstep increase number possible transcription number local optimum final trial involved real data including data utterance given figure trial involved subtrials sequence length fourth length second sequence taken extracting initial value fourth sequence avoiding asymptotic behaviour longer sequence data tabulated come sentence result given figure interpretation shading figure different previous figure evaluation likely real data fact annealing program found evaluation genetic program found evaluation program performed finding transcription evaluation shall display transcription indication time program found transcription genetic annealing transcription occurred program execution",
        "trial program generated random sequence tone computed corresponding sequence set transcribing sequence sequence ideal best possible evaluation transcription performance program measured came finding optimal solution program tested sequence length length program transcribed generated sequence result displayed figure pair bar corresponds given transcription length left member pair genetic search program right member annealing search program shaded bar corresponding evaluation important indicate number time program found transcription evaluation evaluation mean average squared difference predicted value actual value hz observe annealing search program performs case mutation operation genetic search program treat bit parameter encoding perturbation operation annealing search program sensitive distinction significant significant bit explain better convergence behaviour annealing search figure performance degrade transcription length length double generated sequence contain downsteps second tone average causing general downtrend value limiting combinatorial explosion possible transcription",
        "final trial involved real data including data utterance given figure trial involved subtrials sequence length fourth length second sequence taken extracting initial value fourth sequence avoiding asymptotic behaviour longer sequence data tabulated come sentence result given figure interpretation shading figure different previous figure evaluation likely real data fact annealing program found evaluation genetic program found evaluation program performed finding transcription evaluation shall display transcription indication time program found transcription genetic annealing transcription occurred program execution result trial deserve special attention trial transcription found program best evaluation found given striking note transcription hyman stewart given transcription sequence demonstrated transcription point possibility given encouraging sign program living promise producing alternative acceptable transcription desired analytical standpoint",
        "seen transcription given sequence inconvenient required run program time order solution found program designed caught local optimum problem interesting alternative transcription local optimum program set report best solution user specifies number solution desired program ensures area search space explored subsequent search defining distance metric transcription count number tone transcription changed order identical transcription search space distance found solution explored program finding solution generated transcription fall distance previous solution consider following generated sequence tone annealing program set task finding transcription tone sequence program run reported following solution evaluation equal running program found solution order note transcription taken begin initial upstep downstep effect phonetic interpretation following display predicted value given solution facilitate comparison input sequence execution point based table value decided try test second table value performance different solution execution found new solution found value solution cluster clustering occurring ratio h l analysis relationship kind solution found table parameter value attempted",
        "unsatisfying performance program dependent setting search parameter combinatorial optimisation problem find good parameter setting trial error approach found optimal parameter value conclude performance comparison annealing search genetic search problem tone transcription thoroughgoing comparison approach problem need undertaken parameter continuous variable evaluation function write continuous function worthwhile try deterministic search method optimising candidate tone transcription found interesting integrate system one presented speech workstation phonologist identifies salient point cursor system transcription",
        "paper began discussion problem relating tone transcription physical counterpart trace showed desirable phonologist working tone use sequence value primary data impressionistic transcription implicit assumption scaling provided prediction function estimated value tone given value previous tone identity tone presented instrumental data bamileke dschang showed function specialised language function incorporated evaluation function implemented non deterministic search algorithm performance result encouraging demonstrate promise automated tone transcription",
        "head driven generation method combine search combination ideal way proposed define head constituent phrase category semantic ground semantic representation identical put strong restriction shape semantic analysis rule leaf share semantic form root node composition rule semantic representation violate restriction schema construction underspecified discourse representation structure frank reyle general root tree associated larger semantic structure leaf order generation method available grammar follow strict notion semantic head syntactic head driven generation algorithm presented specialized generate udrss second step method extended order handle movement syntactic head defined manner tactical generation problem task generate string semantic representation according syntax semantics relation defined given grammar assume relation stated pair tree left tree state local syntactic dependency dominance relation root node set leaf node linear precedence relation leaf right tree defines relation semantic representation root semantic representation leaf assume map nonterminal leaf node local syntax tree leaf node local semantic derivation tree assumes pairwise linking left link tree omitted pair tree reminiscent synchronous tree tag shieber schabes simpler way particular use adjunction operation essence pair tree graphical notation rule rule hypothesis gazdar fact grammar syntax rule related semantic analysis rule long run tree notation suggests general relation internal structure additional terminal leaf node local syntax tree obvious way implement generation procedure fig relate input semantics start symbol grammar try expand node manner according rule specified grammar node expansion corresponds application predict rule following specification generator terminates leaf node labeled terminal success question method complex symbol equal sake simplicity assume open leaf resp matched feature term unification corresponding mother node grammar rule semantic form decidable variant higher order unification order include reduction expression course necessary precaution taken order avoid confusion object meta level variable shieber depth realization algorithm work semantic representation leaf smaller size semantic form root node actual semantic decomposition take place lexicon semantic representation subgoals variable stand semantic representation size strict left right depth expansion",
        "semantic structure driven generation algorithm defined give basis dynamic subgoal reordering guided semantic input proposal subgoal reordering compile time minnen elaborating work strzalkowski helpful subgoal reordering rule semantic head recursion component required solution strategy breadth search kohl fair delay access lexicon pure depth strategy adopt pure strategy proposed shieber presented fig schematic manner lexical entry qualifies potential leaf node semantic form non trivial substructure input semantics rule lex derivation tree built complete rule succeeds root node current syntax tree labeled start symbol grammar root semantic analysis tree input semantics exclusion phrase semantics trivial substructure input semantics method terminates lack guidance lead general lot non determinism strong substructure condition mean algorithm incomplete grammar cover void phrase like expletive expression particle subphrases idiom head corner generator van noord illustrative instance sophisticated combination prediction structure building fig rule lex restricts selection lexical entry linked local goal category visualized dotted line van noord syntax semantics pair linkable semantic form identical rule complete performs head corner completion step linked phrase link marking removed linked category resp linked semantic form identical rule local success succeeds leaf syntax tree labeled terminal link marking exist rule global success order obtain completeness general case inference schema head corner generator executed breadth interpreter depth interpreter loop semantic analysis rule admit subtrees associated semantic form proper substructure input semantics subtrees composed extreme case recursive rule particle semantics represented list symbol assume structure kind occur depth interpreter sufficient inference rule algorithm encoded interpreted prolog van noord method restricted grammar phrase lexical semantic head algorithm shieber relaxes condition",
        "following present semantic representation formalism corresponding set analysis rule resist definition semantic head required van noord head corner algorithm developed inference system underspecified discourse representation structure udrs discourse representation structure kamp reyle underspecified respect scope following udrs represents reading sentence woman love man leaving exact structural embedding quantified phrase underspecified arrow pointing called subordination constraint mean formula wider scope reyle proposed rule construction udrs hpsg style syntax cf pollard sag shown fig adapted manner composition performed coindexing feature dref re subj etc serve interface value sem feature actual semantic representation phrase structure tree rooted leaf fulfill definition semantic head given shieber van noord head corner generator fig link relation based semantic head applicable",
        "define weak notion semantic head requires semantic form semantic head substructure root semantics meaningless leaf qualify semantic head notion syntactic head serve pivot generation process syntactic head leaf local syntax tree defined grammar writer following preliminary version syntax based link relation kind link relation parsing general work lexical lookup input structure input string consumed order reduce number non terminating case generation similar precaution added input structure taken account final version syntax based link relation incorporates test weak notion semantic head substructure check make sense semantics current goal instantiated case proper semantic head syntactic head differ sister goal semantic head expanded head general sister goal reordered according degree instantiation semantic representation addition improved termination property condition semantic representation help filter useless candidate lexicon lexical entry final derivation semantic representation fit order simplify representation following assume syntax tree grammar isomorphic corresponding semantic analysis tree mean tree merged tree labeling node syntax semantics pair shieber ad hoc solution proposed enforce termination semantic head moved adopting syntactic head driven strategy head movement cause problem landing site head syntactic head main functor category clause categorial grammar terminology superordinate clause postulated syntactic description likewhere mean derivation vp node include v leaf example fig syntactic head c position visited derived exact information verb trace available time movement vorfeld verb second configuration described single structurethe algorithm run deadlock vp node processed semantics xp trace unknown expansion xp filler position delayed reason syntactic description preferred link relation modified substructure test wrt semantics current goal replaced substructure test wrt global input semantics lead loss flexibility discussed connection pure approach algorithm implemented cuf language includes wait mechanism reodering subgoals delegated cuf blown substructure test complicated graph udrs predicate name essential semantic keywords lexical entry mapped current goal semantics map feasible lexical entry dropped restrict grammar lexicalized one grammar lexicalized local syntax tree preterminal leaf cf schabes",
        "water lexicalization affect expressibility grammar bar hillel schabes water generation algorithm turn simpler efficient need transitive link relation goal match mother node preterminal lexicon access head corner completion step merged rule schema version non local feature principle hpsg integrated algorithm non head nonterminal leaf local tree come multiset syntax semantics pair value bind slash feature feature abbreviated example static value dynamic inherited slash value feature abbreviated calculated generation rule lex fig lexical entry head current goal substructure condition hold corresponding semantic form value choose element value current head value associated string lexicalized tree connects goal chosen head value split disjoint set value new subgoals disjoint set union value local tree given grammar version non local feature principle corresponds hypothetical reasoning mechanism provided lambek categorial grammar lambek koenig illustrated fact left tree example rendered categorial grammar notation algorithm fig clear logical basis",
        "define weak notion semantic head requires semantic form semantic head substructure root semantics meaningless leaf qualify semantic head notion syntactic head serve pivot generation process syntactic head leaf local syntax tree defined grammar writer following preliminary version syntax based link relation kind link relation parsing general work lexical lookup input structure input string consumed order reduce number non terminating case generation similar precaution added input structure taken account final version syntax based link relation incorporates test weak notion semantic head substructure check make sense semantics current goal instantiated case proper semantic head syntactic head differ sister goal semantic head expanded head general sister goal reordered according degree instantiation semantic representation addition improved termination property condition semantic representation help filter useless candidate lexicon lexical entry final derivation semantic representation fit",
        "order simplify representation following assume syntax tree grammar isomorphic corresponding semantic analysis tree mean tree merged tree labeling node syntax semantics pair shieber ad hoc solution proposed enforce termination semantic head moved adopting syntactic head driven strategy head movement cause problem landing site head syntactic head main functor category clause categorial grammar terminology superordinate clause postulated syntactic description likewhere mean derivation vp node include v leaf example fig syntactic head c position visited derived exact information verb trace available time movement vorfeld verb second configuration described single structurethe algorithm run deadlock vp node processed semantics xp trace unknown expansion xp filler position delayed reason syntactic description preferred link relation modified substructure test wrt semantics current goal replaced substructure test wrt global input semantics lead loss flexibility discussed connection pure approach",
        "algorithm implemented cuf language includes wait mechanism reodering subgoals delegated cuf blown substructure test complicated graph udrs predicate name essential semantic keywords lexical entry mapped current goal semantics map feasible lexical entry dropped restrict grammar lexicalized one grammar lexicalized local syntax tree preterminal leaf cf schabes water lexicalization affect expressibility grammar bar hillel schabes water generation algorithm turn simpler efficient need transitive link relation goal match mother node preterminal lexicon access head corner completion step merged rule schema version non local feature principle hpsg integrated algorithm non head nonterminal leaf local tree come multiset syntax semantics pair value bind slash feature feature abbreviated example static value dynamic inherited slash value feature abbreviated calculated generation rule lex fig lexical entry head current goal substructure condition hold corresponding semantic form value choose element value current head value associated string lexicalized tree connects goal chosen head value split disjoint set value new subgoals disjoint set union value local tree given grammar version non local feature principle corresponds hypothetical reasoning mechanism provided lambek categorial grammar lambek koenig illustrated fact left tree example rendered categorial grammar notation algorithm fig clear logical basis",
        "paper give syntactic head driven generation algorithm includes defined treatment moved constituent relies notion syntactic head semantic head work grammar semantic head available general like grammar includes semantic decomposition rule underspecified discourse representation structure notion head parsing generation technique closer effect specification generation algorithm gave read parsing algorithm modulo change success condition link relation underspecified drs mean sentence generated meaning representation disambiguated regard quantifier scope particular importance application machine translation want avoid resolution scope relation underspecified meaning rendered source target language work consider strategic generation problem try find heuristic strategy handle situation scope mismatch language precise regard scope",
        "dalrymple dsp equational treatment ellipsis higher order unification thing provides insightful analysis interaction ellipsis quantification suffers number drawback viewed computational perspective precise order quantifier scoped ellipsis resolved determines final interpretation elliptical sentence dalrymple analysis implemented system employing pipelined architecture separate quantifier scoping reference resolution operation preclude generation legitimate reading system good practical reason employ kind architecture additional constraint overgenerate reading sentence likekehler argued problem arises distinguish co referential co indexed terminology role linked expression higher order unification going second order matching required resolving ellipsis involving quantification increase computational complexity ellipsis resolution task paper present treatment ellipsis avoids difficulty having coverage dalrymple treatment implementable form basis ellipsis resolution component core language engine alshawi interpretation represented simple set substitution semantic representation antecedent substitution built order independent way scoping recourse higher order unification treatment similar discourse copying analysis kehler substitutional treatment suggested kamp discourse representation theory described gawron peter extend notion strict sloppy identity deal pronoun deal phenomenon scope parallelism treatment ellipsis value right general conclusion drawn concerning requirement computational theory semantics standard view formal semantics dalrymple inherit identifies semantic interpretation composition interpretation process taking meaning constituent composing form meaning make semantic interpretation order dependent affair order functor composed argument affect resulting meaning reflected order sensitive interleaving scope ellipsis resolution dalrymple account addition composition sensitive meaning component mapping composition meaning example term identical meaning co referential co indexed kind information lost difference amount way composing meaning alternative proposed view semantic interpretation process building partial description intended semantic composition partial description meaning constituent composed order composition operation performed affect outcome order description built unimportant case ellipsis extra layer descriptive indirection permit equational treatment ellipsis order independent account compositional distinction result meaning difference alsodoes require use higher order unification dealing quantifier paper organised follows describes substitutional treatment ellipsis way example presented simplified version quasi logical form qlf alshawi crouch",
        "section illustrates substitutional treatment ellipsis small number example presentation purpose sketch intended semantics simplified qlf notation detailed discussion deferred section simple uninteresting example fix notation represent sentence ignoring tense resolved noun phrase john give rise quantified term identified index expression argument index determiner quantifier explicit restriction additional derived restriction case quantifier range object named john restricted identical salient individual denoted smith reference resolution contextual restriction term uninstantiated meta variable resolution consists instantiating meta variable appropriate value scope term indicated scope node prefixing formula prior resolution scope node uninstantiated meta variable generalized quantifier representation equivalent isthe index scope node mean evaluate qlf hold quantifier restriction contextual restriction corresponding term form generalized quantifier expression body obtained discharging occurrence term index variable abstracting variable index dischargeable manner lead uninterpretable qlfs alshawi crouch represent elliptical sentence abbreviated resolved qlf unresolved meta variable resolve ellipsis need instantiated salient predicate similar line dalrymple set equation determine possible value looking predicate applied subject term ellipsis return antecedent interpretation ellipsis given applying predicate subject ellipsis equation solved setting take term argument substitute index ellipsis antecedent rh form abstraction harm view form abstraction accurate substitution represented notation value ellipsis getellipsis resolution amount selecting antecedent determining set substitution apply reason explained important resolution carry application substitution particular case antecedent resolved capture intended interpretation ellipsis note substitution applied conventional order viz replace replace substitution ensure second substitution replace order substitution apply depends order expression occur making pas applying semantic evaluation rule formula term index substitution applies scope node replaced ensures term mary ellipsis get parallel scope term john antecedent parallelism significant proper name concerned important come quantificational term section meaning ellipsis composed way component meaning antecedent change need order accommodate new material introduced ellipsis substitution specify change example discussed",
        "meaning ellipsis built way antecedent encounter term corresponding john dependent co indexed treated term mary dependent co indexed mean substitution act directive controlling way qlf expression scope evaluated syntactic operation qlf expression qlf object language reason substitution applied ellipsis resolution follows time deciding ellipsis substitution precise composition antecedent determined instance scope quantifier contextual restriction pronoun antecedent resolved correspond presence uninstantiated meta variable antecedent qlf ellipsis follow modulo substitution composition antecedent composition determined make sense apply substitution antecedent resolved sense decide appropriate substitution practical term amount exploiting entrancy qlfs elliptical qlf contain predicate formed antecedent qlf substitution uninstantiated meta variable antecedent entrant ellipsis resolution antecedent imposed ellipsis case substitution treated syntactic operation qlf applied entrant meta variable substituted ellipsis remaining subject substitution applied instantiated noted substitution term index scope node ensures scope parallelism illustrated interesting example adapted hirshbhler cited dalrymple antecedent possible scopings single canadian flag house house flag scoping given antecedent parallel scoping given ellipsis simplified qlf iswhere index mnemonic canadian flag american flag house conjunct antecedent set solution elliptical conjunct equivalent tothe scope node resolved house take wide scope canadian flag take wide scope resolution substitution ensures parallel scoping ellipsis american flag substitution case havethere scoping option instantiates give house wide scope antecedent ellipsis case term ellipsis antecedent discharged bound scope node bound copy equivalent truth condition illustrating scope parallelism example dalrymple resort higher order unification second order matching increase complexity required present treatment notion strict sloppy identity confined pronominal item occurring antecedent ellipsis standard example ison strict reading simon john love john mother implicit pronoun identified pronoun antecedent pick referent john sloppy reading simon love simon mother implicit pronoun identified antecedent refer matching similar description",
        "subject agent loving relation simon sentencehas reading john simon read book john simon read book belonging john john read john book simon read simon book reading arises identifying elliptical book antecedent book second arises identifying pronoun identifying book identifying book pronoun literature reading viewed case strict identity view emerges treatment substitution natural characterisation phenomenon need distinguish parallel non parallel term ellipsis antecedent term john example correspond term appearing ellipsis parallel term explicit parallel ellipsis determining term parallel non parallel touched section parallel term choice ellipsis substitution replace term index corresponding term index ellipsis non parallel term choice strict sloppy substitution sloppy substitution involves substituting new term index old effect reindexing version term occurring ellipsis refers kind thing antecedent term linked strict substitution substitute term index way version term occurring ellipsis linked antecedent term illustrate abbreviated qlf antecedent john read book owned ishere left scope node uninstantiated meta variable pronominal term occurs restriction book term pronoun resolved contextual restriction co index subject term function applied entity denoting expression variable constant return property identical entity applies term index return e type property linked term ellipsis represented conjoined antecedent reading book illustrated listing substitution applied antecedent cashing result application omitting scope reference term removed strict substitution term occurs make difference pronoun given strict sloppy substitution substitution book leaf occurrence index ellipsis qlf interpretable necessary antecedent book term wide scope ellipsis order discharge index antecedent pronoun constrained given wide scope ellipsis pain index undischargeable pronoun like proper name treated restricted quantifier contextual restriction limit domain quantification individual index substitution primary term index contextual restriction pronoun coindexed account reading different account reading reading involves scoping book quantifier ellipsis resolution reading scope quantifier resolution differ giving pronoun sloppy interpretation account choice strict sloppy substitution secondary",
        "term constrain permissible quantifier scopings making choice interleaved precise order scoping quantifier difference strict sloppy reading depend able distinguish primary secondary occurrence term meaning dalrymple representation antecedent np john rise occurrence term constant qlf representation able distinguish primary secondary pronominal reference john precludes illustrating substitutional approach example discussed alshawi cooper coverage dalrymple antecedent contained deletion sloppy substitution person simon sentence john greeted person simon result introducing ellipsis resolution lead uninterpretable cyclic qlf way dalrymple obtain violation occurs check sound unification ellipsis number reading obtained john revised paper teacher simon benchmark dalrymple approach get reading identified plausible slight modification get fifth reading marginal plausibility modification allow strict substitution term appearing ellipsis implicit paper second ellipsis resolving ellipsis sixth implausible reading provided clause resolved coindexed john john refer individual block reading similar manner block reading artificial restriction depth embedding expression logical form lack mean distinguishing coindexed co referential expression ellipsismultiple ellipsis gardent pose problem level determining antecedent ellipsis level incorporating elliptical material antecedent determined appears offer special problem form ellipsis form ellipsis vp ellipsis handled example np ellipsis slept john accommodated ellipsis left tuesday wednesday requires substitution construction qlf described representing prepositional phrase use term index parallel proposal kehler kamp kehler gawron peter adopts analysis referential argument verb represented related davidsonian event thematic role function refer function ellipsis strict identity corresponds copying entire role assignment antecedent identity corresponds copying function applying event ellided clause kamp strict identity involves copying discourse referent antecedent identifying ellided pronoun identity copy condition antecedent discourse referent applies discourse referent ellided pronoun kamp kehler extend copying substitution mechanism pronoun kehler case role assignment function extended deal non referential term desired manner use discourse referent indicate scope suggests kamp treatment extended manner list discourse referent drs box reminiscent index list scope node",
        "simple uninteresting example fix notation represent sentence ignoring tense resolved noun phrase john give rise quantified term identified index expression argument index determiner quantifier explicit restriction additional derived restriction case quantifier range object named john restricted identical salient individual denoted smith reference resolution contextual restriction term uninstantiated meta variable resolution consists instantiating meta variable appropriate value scope term indicated scope node prefixing formula prior resolution scope node uninstantiated meta variable generalized quantifier representation equivalent isthe index scope node mean evaluate qlf hold quantifier restriction contextual restriction corresponding term form generalized quantifier expression body obtained discharging occurrence term index variable abstracting variable index dischargeable manner lead uninterpretable qlfs alshawi crouch represent elliptical sentence abbreviated resolved qlf unresolved meta variable resolve ellipsis need instantiated salient predicate similar line dalrymple set equation determine possible value looking predicate applied subject term ellipsis return antecedent interpretation ellipsis given applying predicate subject ellipsis equation solved setting take term argument substitute index ellipsis antecedent rh form abstraction harm view form abstraction accurate substitution represented notation value ellipsis getellipsis resolution amount selecting antecedent determining set substitution apply reason explained important resolution carry application substitution particular case antecedent resolved capture intended interpretation ellipsis note substitution applied conventional order viz replace replace substitution ensure second substitution replace order substitution apply depends order expression occur making pas applying semantic evaluation rule formula term index substitution applies scope node replaced ensures term mary ellipsis get parallel scope term john antecedent parallelism significant proper name concerned important come quantificational term section",
        "meaning ellipsis composed way component meaning antecedent change need order accommodate new material introduced ellipsis substitution specify change example discussed meaning ellipsis built way antecedent encounter term corresponding john dependent co indexed treated term mary dependent co indexed mean substitution act directive controlling way qlf expression scope evaluated syntactic operation qlf expression qlf object language reason substitution applied ellipsis resolution follows time deciding ellipsis substitution precise composition antecedent determined instance scope quantifier contextual restriction pronoun antecedent resolved correspond presence uninstantiated meta variable antecedent qlf ellipsis follow modulo substitution composition antecedent composition determined make sense apply substitution antecedent resolved sense decide appropriate substitution practical term amount exploiting entrancy qlfs elliptical qlf contain predicate formed antecedent qlf substitution uninstantiated meta variable antecedent entrant ellipsis resolution antecedent imposed ellipsis case substitution treated syntactic operation qlf applied entrant meta variable substituted ellipsis remaining subject substitution applied instantiated",
        "noted substitution term index scope node ensures scope parallelism illustrated interesting example adapted hirshbhler cited dalrymple antecedent possible scopings single canadian flag house house flag scoping given antecedent parallel scoping given ellipsis simplified qlf iswhere index mnemonic canadian flag american flag house conjunct antecedent set solution elliptical conjunct equivalent tothe scope node resolved house take wide scope canadian flag take wide scope resolution substitution ensures parallel scoping ellipsis american flag substitution case havethere scoping option instantiates give house wide scope antecedent ellipsis case term ellipsis antecedent discharged bound scope node bound copy equivalent truth condition illustrating scope parallelism example dalrymple resort higher order unification second order matching increase complexity required present treatment",
        "notion strict sloppy identity confined pronominal item occurring antecedent ellipsis standard example ison strict reading simon john love john mother implicit pronoun identified pronoun antecedent pick referent john sloppy reading simon love simon mother implicit pronoun identified antecedent refer matching similar description subject agent loving relation simon sentencehas reading john simon read book john simon read book belonging john john read john book simon read simon book reading arises identifying elliptical book antecedent book second arises identifying pronoun identifying book identifying book pronoun literature reading viewed case strict identity view emerges treatment substitution natural characterisation phenomenon need distinguish parallel non parallel term ellipsis antecedent term john example correspond term appearing ellipsis parallel term explicit parallel ellipsis determining term parallel non parallel touched section parallel term choice ellipsis substitution replace term index corresponding term index ellipsis non parallel term choice strict sloppy substitution sloppy substitution involves substituting new term index old effect reindexing version term occurring ellipsis refers kind thing antecedent term linked strict substitution substitute term index way version term occurring ellipsis linked antecedent term illustrate abbreviated qlf antecedent john read book owned ishere left scope node uninstantiated meta variable pronominal term occurs restriction book term pronoun resolved contextual restriction co index subject term function applied entity denoting expression variable constant return property identical entity applies term index return e type property linked term ellipsis represented conjoined antecedent reading book illustrated listing substitution applied antecedent cashing result application omitting scope reference term removed strict substitution term occurs make difference pronoun given strict sloppy substitution substitution book leaf occurrence index ellipsis qlf interpretable necessary antecedent book term wide scope ellipsis order discharge index antecedent pronoun constrained given wide scope ellipsis pain index undischargeable pronoun like proper name treated restricted quantifier contextual restriction limit domain quantification individual",
        "index substitution primary term index contextual restriction pronoun coindexed account reading different account reading reading involves scoping book quantifier ellipsis resolution reading scope quantifier resolution differ giving pronoun sloppy interpretation account choice strict sloppy substitution secondary term constrain permissible quantifier scopings making choice interleaved precise order scoping quantifier difference strict sloppy reading depend able distinguish primary secondary occurrence term meaning dalrymple representation antecedent np john rise occurrence term constant qlf representation able distinguish primary secondary pronominal reference john",
        "space precludes illustrating substitutional approach example discussed alshawi cooper coverage dalrymple antecedent contained deletion sloppy substitution person simon sentence john greeted person simon result introducing ellipsis resolution lead uninterpretable cyclic qlf way dalrymple obtain violation occurs check sound unification ellipsis number reading obtained john revised paper teacher simon benchmark dalrymple approach get reading identified plausible slight modification get fifth reading marginal plausibility modification allow strict substitution term appearing ellipsis implicit paper second ellipsis resolving ellipsis sixth implausible reading provided clause resolved coindexed john john refer individual block reading similar manner block reading artificial restriction depth embedding expression logical form lack mean distinguishing coindexed co referential expression ellipsismultiple ellipsis gardent pose problem level determining antecedent ellipsis level incorporating elliptical material antecedent determined appears offer special problem form ellipsis form ellipsis vp ellipsis handled example np ellipsis slept john accommodated ellipsis left tuesday wednesday requires substitution construction qlf described representing prepositional phrase",
        "use term index parallel proposal kehler kamp kehler gawron peter adopts analysis referential argument verb represented related davidsonian event thematic role function refer function ellipsis strict identity corresponds copying entire role assignment antecedent identity corresponds copying function applying event ellided clause kamp strict identity involves copying discourse referent antecedent identifying ellided pronoun identity copy condition antecedent discourse referent applies discourse referent ellided pronoun kamp kehler extend copying substitution mechanism pronoun kehler case role assignment function extended deal non referential term desired manner use discourse referent indicate scope suggests kamp treatment extended manner list discourse referent drs box reminiscent index list scope node",
        "figure defines valuation relation qlf fragment derived alshawi crouch cooper qlf expression contains uninstantiated meta variable valuation relation associate value expression case formula given value true false corresponding formula true possible resolution false subsumption ordering qlfs employed evaluation rule effect propose possible instantiation meta variable rule fragment allows scope meta variable cooper describes general case kind meta variable permitted instantiated qlf specifies set possible evaluation semantic composition qlf instantiated set possible evaluation narrow singleton possible qlf uninterpretable specify possible evaluation rule given evaluating term index isolation discharged scoping rule substitute term index bound variable scoping leave undischarged uninterpretable term index account called free variable vacuous quantification constraint scope alshawi crouch non deterministic nature evaluation role substitution draw conclude ellipsis substitution operate description semantic composition result composition",
        "selecting ellipsis antecedent parallel element open problem pruest pruest kehler grover approach parallelism handed absence clear solution flexible qlfs shown omitted category information present term form set feature value equation containing syntactic information relevant determining uninstantiated meta variable resolved vp ellipsis illustrates category workinthe ellipsis contained form expression category state syntactic tense aspect polarity marked ellipsis underscore indicate lack specification category constrains resolution look verb phrase sentence source come wrapped category likeheuristics similar described hardt category say kind match term antecedent category identifies subject treated parallel explicit term ellipsis example illustrates tense aspect ellipsis antecedent agree antecedent ellipsis category determine substituted antecedent comprises restriction antecedent new category constructed taking feature antecedent category overridden ellipsis kind monotonic priority union grover skeptical opposed credulous default unification carpenter new category constructed antecedent tense resolution need undone original one appropriate revised category merges category information source antecedent determine verb phrase form substituted original case category general question ellipsis involve recompositions variant linguistic antecedent case degree inference required apply knowledge austrian speak german interpret ellipsis equational treatment context dependency suggests method dealing case remains seen equation ellipsis integrated pulman framework",
        "substitutional treatment ellipsis presented coverage higher order unification treatment computational advantage requiring order sensitive interleaving different resolution operation andnot requiring greater second order matching dealing quantifier addition cure slight overgeneration problem dalrymple account claimed advantage arise viewing semantic interpretation process building description semantic composition conclude argument view independent particular proposal dealing ellipsis independence reason computational success unification based syntactic formalism order independence parser generator operation permit look order sensitive nature operation semantic composition provide poor starting point treatment semantics enjoying similar computational success semantic interpretation viewed building description intended composition better prospect sensitivity truth value sentence depend context dependence enter interpretive mapping sentence meaning evaluative mapping meaning world truth value context dependence enters interpretive mapping meaning context independent maintain principle strict compositionality interpretation syntactic structure underspecifies intended composition meaning constituent pronoun mode combination quantifier specified contextual information required fill gap interpretation seen description building sits information focusing result semantic composition meaning ignore difference meaning derived significant co referential co indexed term information lost way referring structure composition result required",
        "consider following sentence entail basic message borrowed car addition basic message carry information borrow possession say borrow car say car word focus marker indicated underlining stressed word combine add extra message similar phenomenon appears taking place set sentence say steal car carry extra message steal belongs say stole car say car borrowed entailed argues number situation focus marker extract interpretation called focussed negation combine extracted element interpretation left carry complex message kind discussed current paper show implement general notion following krifka analysis detail crucial point provision way storing extracted interpretation making available required interpretation focussed negation straightforward long treatment focussed item coherent",
        "general aim paper use focus decompose interpretation phrase part interpretation focussed item object combine example thought ate peach interpreted word abstraction event ate peach want object corresponding interpretation ate peach like extracted denotation peach property peach converted interpretation abstraction combine property reproduce original interpretation phenomenon kind consider following phrase property man combine property stole bike construct minimal unique characterisation relevant individual achieve need interpret relative pronoun relative clause leaving hole interpretation clause abstracting respect hole clear hold want interpret sentence like man stole bike quantifier introduced subject fact maximal scope analysis argued ramsay treatment requires mechanism require want deal focus outlined hold nlp system include way dealing interpretation case like mechanism open adaptation deal focus suggested line approach outlined",
        "expect interpret relative clause phrase focussed constituent abstraction interpretation simple sentence order construct interpretation kind object interested start looking simple sentence analysis presented paper start following observation orthodox indefinite np viewed way introducing item set item discourse quantified np item specified type satisfy property viewed way introducing event set event discourse construct interpretation paraphrasing sentence formal language extends predicate calculus realise scope quantifier paraphrase determined simple structural property source text np presuppositional construction place constraint discourse sentence containing phrase man uninterpretable context containing unique man version point barwise perry kamp groenendijk stokhof interaction scope definite np type expression man kill thing love presuppositional construct thing love requires existence single target affection man standard way deal potential discrepancy phrase appears width scope storing quantifier quantifier stack entire sentence interpreted explicit information priority quantifier sort thing cooper vestre work reported follows treatment extends introducing quantifier like entity dealing presuppositional item definite np ramsay ramsay formal account constraint sentence meaningful respect situation example sentence woman stole bike interpreted asthis say relationship simple hold past instant property certain sort event sort event bike stolen singleton set bike stolen contain occurrence say hold unique individual satisfies restriction woman singleton set woman restriction fails pick unique individual expression meaningless context analysis orthodox main point require defence analysis aspect term relationship temporal object event type discussed ramsay treatment definite reference term constraint meaningfulness crucial remainder paper like replace better unlikely find better simpler analysis obtained framework quantifier scope determined basis information associated form cooper storage cooper abstraction operator form applied formula bind free variable framework easy deal case allowing relative pronoun add expression quantifier store annotated specify expression maximal scope expression applied formula containing free occurrence return abstraction respect want requirement maximal scope ensure free",
        "variable use mechanism construct abstraction interpretation relative clause use construct abstraction interpretation phrase containing focussed item extra work perform find interpretation focussed item needed extra feature focus description linguistic item value focus focussed item behaves gpsg foot feature daughter item non vacuous value focus item daughter non vacuous value feature item share value daughter standard feature slash dealing left extraposition foot feature value item position",
        "mechanism use construct interpretation sentence instance example extracted displayed formula available space fact formula say relationship hold property car object fine go worth spell condition relationship hold following meaning postulate word hold satisfies present case consequence mean borrow car extracted save space obtained meaning postulate substituting second consequence say category item satisfies abstraction thing borrowed car focus interpretation say hold description type event borrows abstraction situation car consequence say car borrowed substituting description event type abstracted variable produce second argument simple reduces want second say happen place relation backed following say form negation hold hold hold entity present case mean stealing contrast simple negation focussed item say case stealing event involving choice forced presence absence focussed item final example consider sentence contains focussed item operator analysis abstraction kind individual ate focus set description including semantic analysis focussed phrase man kind object required discourse operator contrast elaboration operator appropriate depends factor visible require pair argument kind",
        "discussion show achieved treating focus syntactic marker make information available variety operator mechanism involves introducing foot feature carry focussed item constructing appropriate abstraction standard quantifier scoping mechanism required phenomenon nlp system deal syntax semantics phenomenon left right extraposition different way argued approach phenomenon adapted deal focus example section showed combine analysis focus variety operator convey range interpretation sequence word important recall point interpretation language intensional logic permit quantification arbitrary kind individual including quantification property proposition argued language required wide variety phenomenon interpretation focus example",
        "year common agreement natural language processing research community importance having extensive coverage surface lexical semantics domain work typical context use knowledge expressed different level abstraction depending phenomenon involved selectional restriction sr lexical preference col location etc interested sr expressed semantic type constraint word sense imposes word combine process semantic interpretation include information syntactic position word restricted instance sens verb drink restricts subject animal object liquid help parser prefer par grammatical one sr help parser deciding semantic role played syntactic complement interested acquisition sr hand sr interesting information included dictionary defining context approach hand church hank remark effort involved analyzing classifying linguistic material provided concordance use word labor intensive possible represent sr word studied classify concordance different word us lexicographer analysis possible source sr introspection lexicographer machine readable dictionary line corpus main advantage provide experimental evidence word us approach acquiring different kind lexical information corpus developed basili church church hank resnik paper interested exploring amenability method extracting sr textual data line work aim proposed technique learn sr word imposing analysis example use word contained corpus illustration learning shown figure system departing example use knowing prosecutor buyer lawmaker noun belonging semantic class indictment assurance legislation member induce verb seek imposes sr constraint subject member semantic type object kind system extract word complement having number occurrence use corpus syntactic complement list alternative sr word imposing order detect sr word imposes context mean statistical technique distinct approach proposed word based church class based basili resnik based approach infers sr collection word co occur syntactic context studied word class based technique gather different noun mean semantic class advantage clear hand meaningful data gathered small corpus frequent word hand sr generalized new example present training set acquired sr independent lexical choice training corpus developed implemented method extracting class based sr line corpus section describe discussing approach section analyze data",
        "sr express semantic constraint holding different syntactic functional configuration paper focus selectional restriction holding verb complement method exported configuration wo distinguish sr imposed verb argument adjunct believe adjunct going provide evidence corpus creating sr following paragraph describe functional specification system setthe input learning process list co occurrence triple codifying co occurrence verb complement head corpus verb syntactic relationship noun noun lemma inflected form appearing text relationship code kind complement subject object preposition case method draw co occurrence triple corpus proposed subsection result learning process set syntactic sr verb syntactic relationship semantic class class represented set noun acquired case corpus gather statistical evidence distinct us verb different sr permit extract class syntactic position disjoint related hyperonymy knowledge usedin process learning sr system need know word clustered semantic class semantic class organized word represented having different hyperonym class subsection defend use broad coverage taxonomy process computational process divided stage guessing possible semantic class creation space candidate principle hyperonyms level noun appearing training set candidate appropriateness candidate order compare different candidate statistical measure summarizing relevance occurrence candidate class appropriate subset candidate space convey sr taking account final class disjoint subsection statistical measure fulfill stage presented stage discussed process learning example accuracy training set base system correct prediction case semantic class hypothesized example accuracy fundamental approach obtain lexical co occurrence proposed literature basili church church approach inappropriate tackling need detect local co occurrence church church hank extract spurious co occurrence triple basili church hank hand system intends learn sr kind verb complement hand fact approach extract co occurrence reliability verb complement violates accuracy requirement co occurrence extracted corpus annotated structural syntactic information speech skeletal tree result higher degree accuracy representativity way detect relationship verb complement non related co occurrence extracted objection approach task producing syntactic analyzed corpus expensive growing interest produce analyzed corpus parser simple heuristic meet",
        "requirement representativeness accuracy introduced hand useful represent co occurrence triple holding lemma order gather evidence possible simple morphological analyzer lemma big percentage word appearing corpus suffice class based approach presented section resnik technique us wide coverage semantic taxonomy basili consists hand tagging fixed set semantic label advantage drawback approach diverse hand basili approach semantic class relevant domain chosen adjustment class corpus nice resnik system constrained able induce appropriate level sr hand basili implies hand coding relevant word semantic tag resnik need broad semantic taxonomy available taxonomy wordnet resnik approach better result obtained lower cost involved trying choose measure appropriateness semantic class consider feature problem robustness noise andconservatism order able generalize positive example having tendency generalize statistical measure accomplish requirement proposed literature basili church resnik adopt resnik approach quantifies statistical association verb class noun co occurrence adapt taking account syntactic position relationship set verb noun syntactic position possible noun class association score assoc syntactic position defined bewhere conditional probability estimated counting number observation joint event dividing frequency given event term assoc try capture different property expressed candidate class information measure strength statistical association given verb candidate class given syntactic position real relationship hand conditional probability favor class occurrence noun existence noise training set introduces class candidate space considered expressing sr common technique ignoring possible noise consider event higher number occurrence certain threshold erroneous class persist exceed threshold candidate class ordered significance assoc verb likely appropriate class introduced noise ranked position candidate list algorithm learn sr based search class instance training set given threshold different iteration candidate class operation performed class having best assoc best class extracted final result remaining candidate class filtered class hyper hyponym best class step definitive class disjoint iteration repeated candidate space run performed similar learning process looking preferred class object noun interested possible class sr",
        "process learning example accuracy training set base system correct prediction case semantic class hypothesized example accuracy fundamental approach obtain lexical co occurrence proposed literature basili church church approach inappropriate tackling need detect local co occurrence church church hank extract spurious co occurrence triple basili church hank hand system intends learn sr kind verb complement hand fact approach extract co occurrence reliability verb complement violates accuracy requirement co occurrence extracted corpus annotated structural syntactic information speech skeletal tree result higher degree accuracy representativity way detect relationship verb complement non related co occurrence extracted objection approach task producing syntactic analyzed corpus expensive growing interest produce analyzed corpus parser simple heuristic meet requirement representativeness accuracy introduced hand useful represent co occurrence triple holding lemma order gather evidence possible simple morphological analyzer lemma big percentage word appearing corpus suffice",
        "class based approach presented section resnik technique us wide coverage semantic taxonomy basili consists hand tagging fixed set semantic label advantage drawback approach diverse hand basili approach semantic class relevant domain chosen adjustment class corpus nice resnik system constrained able induce appropriate level sr hand basili implies hand coding relevant word semantic tag resnik need broad semantic taxonomy available taxonomy wordnet resnik approach better result obtained lower cost involved",
        "trying choose measure appropriateness semantic class consider feature problem robustness noise andconservatism order able generalize positive example having tendency generalize statistical measure accomplish requirement proposed literature basili church resnik adopt resnik approach quantifies statistical association verb class noun co occurrence adapt taking account syntactic position relationship set verb noun syntactic position possible noun class association score assoc syntactic position defined bewhere conditional probability estimated counting number observation joint event dividing frequency given event term assoc try capture different property expressed candidate class information measure strength statistical association given verb candidate class given syntactic position real relationship hand conditional probability favor class occurrence noun",
        "existence noise training set introduces class candidate space considered expressing sr common technique ignoring possible noise consider event higher number occurrence certain threshold erroneous class persist exceed threshold candidate class ordered significance assoc verb likely appropriate class introduced noise ranked position candidate list algorithm learn sr based search class instance training set given threshold different iteration candidate class operation performed class having best assoc best class extracted final result remaining candidate class filtered class hyper hyponym best class step definitive class disjoint iteration repeated candidate space run performed similar learning process looking preferred class object noun interested possible class sr performed search candidate space function maximize monotone behavior case assoc search guarantee global optimals local one fact decide global search candidate space big",
        "order experiment methodology presented implemented system unix machine corpus extracting co occurrence triple fragment parsed material penn treebank corpus word sentence consisting article wall street journal tagged parsed wordnet verb noun lexicon lemmatizer semantic taxonomy clustering noun semantic class section evaluate performance methodology implemented looking performance technique extracting triple considering coverage wordnet taxonomy noun sens appearing treebank andanalyzing performance learning process total number co occurrence triple extracted amount triple discarded lemmatizing process surface head noun remaining triple processed lemmatizer mapped corresponding lemma form addition analyzed result obtained subset extracted triple looking sentence corpus occurred subset contains example average common verb treebank rise report seek present testing sample hand triple considered extracted parser lemmatizer triple testing set considered extracted lemmatized analyzing coverage wordnet taxonomy considered different ratio hand noun occurrence sens included taxonomy extracted triple hand noun occurrence testing sample correct sense introduced taxonomy extracted triple figure positive evaluation coverage wordnet order evaluate performance learning process inspected sr acquired testing sample assessing corresponded actual sr imposed way evaluation mean measuring precision recall ratio testing sample case define precision proportion triple appearing syntactic position acquired sr fulfill sr amount remaining triple belong class induced syntactic position correct sense included wordnet taxonomy correct class induced evidence hand define recall proportion triple fulfill sr acquired corresponding syntactic position amount second way evaluating performance abstraction process diagnose reason system deduce sr obtained show sr corresponding subject position verb seek indicates diagnostic class appropriateness value association score number noun appearing corpus contained class indicates number actual noun sens corpus contained class table example type manual diagnostic okthe acquired correct according noun sens contained corpus best level stating induced",
        "lower happens erroneous sens metonymy accumulate evidence higher class sr gathered unique class find case class cropped accumulates evidence provided erroneous sens class accumulates evidence provided extracted triple show incidence diagnostic type testing sample row show type diagnostic number percentage class accomplish number percentage noun occurrence contained class testing sample result obtained testing sample shown table draw positive negative conclusion correct semantic class syntactic position sample acquired technique achieves good coverage co occurrence triple class acquired result accumulation incorrect sens size tends smaller class category contain sens clear co relation assoc manual diagnostic class considered correct ranked higher position assoc table generalization produced little difference noun included rival class situation rare impact noise provided erroneous extraction co occurrence triple acquisition wrong semantic class moderate different verb sens occur corpus sr acquired appear mixed",
        "performance technique presented good detected problem solved way explore order reduce problem stated point measure assoc mean mutual information pair v s way syntactic position provide information statistical evidence measuring appropriate class modify assoc way based likelihood ratio test dunning kind test better performance mutual information count small case estimate probability class frequency noun member correcting evidence number sens noun way estimated function probability distribution interesting noun provide evidence occurrence hyperonyms proportional degree ambiguity collect bigger number example verbal complement projecting complement internal argument diathesis sub categorization rule assoc better performance estimated bigger population hand way possible detect sr holding internal argument order solve point foreseen possibility consideration statistical significance alternative involved generalization step climbing use pps corpus attached complement main verb source implicit negative example way constrain generalization interesting investigate solution point possible way disambiguate sens verb appearing corpus sr acquired gathering evidence pattern corresponding sense mean technique similar yarowsky disambiguated verb sens split set sr acquired future line research outlined investigated result included ribas",
        "keyboard user proficient frustrating time spent backtracking pick mi typed mistaken input described paper started idea error processor sit editor detecting correcting error entry user continued text relieved tedious backtracking co operative error processing program catch error entered operate complete sentence underway focus shallow processing far error detection correction proceed system purview set stretch text admit complete sentential analysis date grammar checker program deal illformed input step spelling consideration scale sentence parse treating sentence basic unit loses meaning sentence incomplete illformed processing interesting cheaper complete analysis sentence investigate issue involved shallow processing cooperative error handling pet processing error text system built focus issue attempt produce complete product operates shifting window text attached emacs editor word purview focus time response word accept word suggest correction indicate found error correct follow outline discussion linguistic component pet discussion testing evaluation system",
        "word focus passed level morphological analysis stage based adaption pulman purpose served checking word lexical lexicon permissible inflection word lexicon collecting possible category represented set feature specification grover morphological lookup operates character trie compressed directed graph ending shared category information stored unique transition advantage compression word morpheme recognised category affixation rule grover checked initial letter allow uniqueness end word immense saving space reduction transition trie formed alvey lexicon word unknown system reconsiders analysis point broke added possibility error rule error rule corresponding damerau transformation omission insertion transposition substitution damerau considered order pollock error rule level format integrate morphological analysis say letter inserted asterisk indicating occur context compare pulman right hand represents error surface left hand surface error removed succeed backtracks try error rule point analysis present apply error rule word keeping finding error frequency pollock alternative program developed us positional binary trigram riseman spot error position check candidate correction generated reverse damerau transformation advantage level error rule us good method calculating likely error position set correction possibility generated cheaply correction possibility ranked frequency information damerau error giving preference common word initial test small file constructed error showed error rule better fact choosing correct correction error rule applied ordinary morphological rule fail place mark error rule ignore error location accept allowable letter combination error rule operate letter graph lexicon consider lexical word unknown letter instantiated letter associated transition option disadvantage remains generating correction possibility sicstus backtracking time consuming present phase postulate grapheme time possible category passed later stage category fail analysis backtracking alternative correction candidate different grapheme occur alvey feature mapped claw lob corpus garside transition checked occurrence matrix tagged lob corpus positional binary trigram similar spelling check mentioned check current set category stop category pass backtrack continue parsing fails core language engine cle application independent unification based general purpose device mapping natural language sentence logical form",
        "representation alshawi intermediate syntactic stage involve phrasal parsing followed syntactic analysis left corner stage fails cle invokes partial parsing phrasal phase partial parsing extracted adapted present purpose mapping cle tag application phrasal phase implement parsing straightforward partial parsing left corner analysis combined prediction result phrasal phase look complete phrase break wordstring maximal segment example produce segment produce segment ate nice friendly cat produce segment parsing need adapted support idea pet purview partial parsing accepts string constitute sentence achieve end wordstring delimited purview need treated right hand end start rule possibility word considered prediction facility built parsing process left hand treated end possibility better idea purview remember derived constituent involve current word phase added detection tag partial parsing error processing backtrack intraword correction level correction pet consider possibility simple phrase error word doubling omission common function word transformation involving space character splitting word implemented deletion space substitution character space straightforward addition morphological process space dealt setting expectation discovering deletion character word deleted character attached beginning word space trickier focus word processing unit correction include red present system generate possibility present word focus newest word purview provide right hand context information thing facilitate handling space addition change necessitate complex backtracking mechanism focus lag morphological processing phase sensible reference wider context able refer earlier detection correction respect editor pet attached correspond log error encountered file edited recent microsoft product keep record personal habitual mistake valuable aid choosing correct correction system better use graph state lexicon transformation implies implicit explicit string comparison advantage graph trie allows comparison end word beginning",
        "word focus passed level morphological analysis stage based adaption pulman purpose served checking word lexical lexicon permissible inflection word lexicon collecting possible category represented set feature specification grover morphological lookup operates character trie compressed directed graph ending shared category information stored unique transition advantage compression word morpheme recognised category affixation rule grover checked initial letter allow uniqueness end word immense saving space reduction transition trie formed alvey lexicon word unknown system reconsiders analysis point broke added possibility error rule error rule corresponding damerau transformation omission insertion transposition substitution damerau considered order pollock error rule level format integrate morphological analysis say letter inserted asterisk indicating occur context compare pulman right hand represents error surface left hand surface error removed succeed backtracks try error rule point analysis present apply error rule word keeping finding error frequency pollock alternative program developed us positional binary trigram riseman spot error position check candidate correction generated reverse damerau transformation advantage level error rule us good method calculating likely error position set correction possibility generated cheaply correction possibility ranked frequency information damerau error giving preference common word initial test small file constructed error showed error rule better fact choosing correct correction error rule applied ordinary morphological rule fail place mark error rule ignore error location accept allowable letter combination error rule operate letter graph lexicon consider lexical word unknown letter instantiated letter associated transition option disadvantage remains generating correction possibility sicstus backtracking time consuming present phase postulate grapheme time possible category passed later stage category fail analysis backtracking alternative correction candidate different grapheme occur",
        "alvey feature mapped claw lob corpus garside transition checked occurrence matrix tagged lob corpus positional binary trigram similar spelling check mentioned check current set category stop category pass backtrack continue parsing fails core language engine cle application independent unification based general purpose device mapping natural language sentence logical form representation alshawi intermediate syntactic stage involve phrasal parsing followed syntactic analysis left corner stage fails cle invokes partial parsing phrasal phase partial parsing extracted adapted present purpose mapping cle tag application phrasal phase implement parsing straightforward partial parsing left corner analysis combined prediction result phrasal phase look complete phrase break wordstring maximal segment example produce segment produce segment ate nice friendly cat produce segment parsing need adapted support idea pet purview partial parsing accepts string constitute sentence achieve end wordstring delimited purview need treated right hand end start rule possibility word considered prediction facility built parsing process left hand treated end possibility better idea purview remember derived constituent involve current word phase added detection tag partial parsing error processing backtrack intraword correction level correction pet consider possibility simple phrase error word doubling omission common function word",
        "damerau transformation involving space character splitting word implemented deletion space substitution character space straightforward addition morphological process space dealt setting expectation discovering deletion character word deleted character attached beginning word space trickier focus word processing unit correction include red present system generate possibility present word focus newest word purview provide right hand context information thing facilitate handling space addition change necessitate complex backtracking mechanism focus lag morphological processing phase sensible reference wider context able refer earlier detection correction respect editor pet attached correspond log error encountered file edited recent microsoft product keep record personal habitual mistake valuable aid choosing correct correction system better use graph state lexicon transformation implies implicit explicit string comparison advantage graph trie allows comparison end word beginning",
        "aim evaluating effectiveness shallow processing test carried proportion different type error dealt examination number error missed caught corrected component configuration system compared example error rule system varied example breadth purview position purview focus number correction candidate timing generation shallow processing miss error cooperative error processing aimed significant difficulty collecting test data central difficulty finding representative sample genuine error native speaker context correct version text attached representative hard decide spectrum error distribution error corpus text contains error left undetected text processing deal error backtracks catch different class range different distribution error type ideal data record people keystroke interacting editor creating editing piece text allow measure linguistic feasibility cooperative error processing effectiveness shallow processing error revealed keystroke record data appear english source kind planned compile comparison variety data collected test generated error program produce random damerau slip according observed distribution pollock confusion matrix appropriate kernighan data includes birkbeck corpus mitton multifarious misspelling list context look low frequency word corpus news mail archive longmans learner corpus native speaker",
        "infant face difficult problem segmenting continuous speech word benefit developed lexicon source information speech help infant solve problem including prosody semantic correlation phonotactics research date focused determining source infant sensitive little work determine potential usefulness source computer simulation reported attempt measure usefulness distributional phonotactic information segmenting phoneme sequence algorithm hypothesize different segmentation input word select best hypothesis according minimum description length principle result indicate useful information phoneme distribution phonotactic rule combination source useful",
        "infant learn recognize certain sound sequence word difficult problem normal speech contains obvious acoustic division word source information aid speech segmentation distribution phoneme sequence cat appears context including thecat cat catnap sequence catn rare appears restricted context phonotactics cat acceptable syllable english pcat evidence exists infant sensitive information source know measurement usefulness paper attempt quantify usefulness distribution phonotactics segmenting speech found source provided useful information speech segmentation combination source provided substantial information found child directed speech easier segment adult directed speech source date psychologist focused aspect speech segmentation problem problem parsing continuous speech word given developed lexicon incoming sound matched psychologist cutler carter cutler butterfield designer speech recognition system church examined problem problem examined different want know infant segment speech knowing phonemic sequence form word second aspect psychologist focused problem determining information source infant sensitive source examined prosody word stress suggest parent exaggerate prosody child directed speech highlight important word fernald mazzie aslin press infant sensitive prosody hirsh pasek stress english predicts location word beginning cutler norris cutler butterfield jusczyk demonstrated month old month old sensitive common strong weak word stress pattern english native language phonotactics month old reported jusczyk study demonstrated infant perceptive ability demonstrating usefulness infant perception child combine information perceive different source aslin speculate infant learn word heard isolation use distribution prosody refine expand vocabulary jusczyk suggests sound sequence learned isolation differ context useful go information sound structure input bootstrap acquisition level linguistic organization remains determined paper measure potential role distribution phonotactics combination computer simulated learning algorithm simulation based bootstrapping model phonotactic knowledge constrain distributional analysis speech sample work motivated research developmental research support certain assumption input system represented sequence phoneme assume infant able convert acoustic input phoneme sequence research grieser kuhl suggests assumption reasonable sentence boundary provide information word boundary end sentence end word input contains sentence boundary study bernstein ratner hirsh pasek kemler jusczyk shown infant perceive sentence boundary prosodic cue fisher tokura press found evidence prosody predict word boundary task finding word remains question infant",
        "ability trying model identify word embedded sentence jusczyk aslin submitted found month old gain intuitive understanding model consider following speech sample transcription ipa different way break sample putative word particular segmentation called segmentation hypothesis hypothesis listing word segmentation hypothesis yield following lexicon note segmentation correct hypothesis yield compact lexicon frequent word segmentation yield larger lexicon infrequent word note lexicon contains word sample word known system priori carried hypothesis lexicon sample encoded replacing word respective index lexicon simulation attempt find hypothesis minimizes combined size lexicon encoded sample approach called minimum description length mdl paradigm domain analyze distributional information vitnyi rissanen ellison ellison brent reason explained section system convert character based representation compact binary representation number bit binary string measure size rule restrict segmentation hypothesis space preventing word boundary certain place instance cat paw internal segmentation point tspz tspz etc allowed spz kt evaluate usefulness phonotactic knowledge compared result constrained unconstrained simulation",
        "gain intuitive understanding model consider following speech sample transcription ipa different way break sample putative word particular segmentation called segmentation hypothesis hypothesis listing word segmentation hypothesis yield following lexicon note segmentation correct hypothesis yield compact lexicon frequent word segmentation yield larger lexicon infrequent word note lexicon contains word sample word known system priori carried hypothesis lexicon sample encoded replacing word respective index lexicon simulation attempt find hypothesis minimizes combined size lexicon encoded sample approach called minimum description length mdl paradigm domain analyze distributional information vitnyi rissanen ellison ellison brent reason explained section system convert character based representation compact binary representation number bit binary string measure size rule restrict segmentation hypothesis space preventing word boundary certain place instance cat paw internal segmentation point tspz tspz etc allowed spz kt evaluate usefulness phonotactic knowledge compared result constrained unconstrained simulation",
        "use mdl principle introduced search sized hypothesis defined method measuring hypothesis size method work simple intuitive way measuing size hypothesis count number character represent example counting character excluding space introductory example hypothesis us character hypothesis us simplistic method inefficient instance length lexical index arbitrary respect property word hypothesis reason assigned index length length system improves simple size metric computing size based compact representation motivated information theory imagine hypothesis represented string one zero binary string represent lexical entry index called code word coded sample overhead information specifying number item coded arrangement string information given spacing spatial placement introductory example string component self delimiting decoder identify endpoint component section describes binary representation length formul derived detail reader satisfied intuitive description presented skip phonotactics sub section representation scheme described based information theory example coding system vitnyi quinlan rivest representation derive formula describing length bit discrete form formula work practice simulation use continuous approximation discrete formula approximation involves dropping ceiling function length computation example use self delimiting representation integer described vitnyi pp representation number bit needed code integer given use following approximation discrete formula difference difference bit continuous formula difference difference found easier interpret result continuous function following discussion present approximate formula lexicon list word represented phoneme sequence paired code word example binary representation column represented column called word inventory column second column called code word inventory column word inventory column figure schematic list lexical item represented continuous string phoneme separator word ktktisi mark boundary lexical item phoneme string preceded list integer representing length phoneme word length represented fixed length padded binary number list single integer denoting length length field integer represented unary length need known advance entire column number lexical entry coded self delimiting integer length representation integer given functionwe define number phoneme word total unique phoneme sample represent phoneme fixed length bit string length length representation word lexicon number phoneme",
        "word time length phoneme total length word lexicon sum formula lexical item stated length field divide phoneme string fixed length field integer number phoneme longest word representing integer take bit length field self delimiting width field represented self delimiting way use unary representation write extra field consisting bit followed terminating field word unary prefix combined length field prefix terminating total length word inventory column representation sum term code word inventory column lexicon figure schematic identical representation previous column code word listed phonemic word length field unary prefix serve purpose marking division code word sample represented assigning short code word frequent word reserving longer code word infrequent word satisfy property code word assigned length frequency based length code word word frequency greater total length code word list sum code word length lexical entry word inventory column described length code word represented fixed length field frequent word longest code word property formula longest possible code word come word frequency field contains integer number define length field represent width field unary total element size field unary representation field width combined length field prefix terminating total length code word inventory column representation sum term sequence word form sample figure schematic represented number word sample followed list code word code word compact index lexicon original sample reconstructed looking code word list replacing phoneme sequence lexicon code word assigned lexical item self delimiting set code known need represent boundary code word length representation integer given functionthe length representation sample computed summing length code word represent sample simplify description noting combined length occurrence particular code word occurrence code word sample length encoded sample sum formula word lexicon total length sample given adding term total length representation entire hypothesis sum representation length word inventory column code word inventory column sample system computing hypothesis size efficient sense element thought represented",
        "code word assigned based relative frequency word final evaluation given hypothesis estimate minimal number bit required transmit hypothesis permit direct comparison competing hypothesis shorter representation hypothesis distributional information extracted better hypothesis knowledge given system list licit initial final consonant cluster english word list checked sample list permissive underlined consonant cluster explore divided ek splore eks plore simulation phonotactic knowledge word boundary inserted create word initial final consonant cluster list create word vowel example actual sample corresponds utterance want help baby second line word boundary legal marked dot boundary illegal legal word english boundary illegal valid word initial consonant cluster boundary illegal valid word final consonant cluster boundary legal valid word final cluster valid word initial cluster phonotactic constraint reduces number potential word boundary example system insert new word boundary update list remaining valid insertion point adding point cause nearby point unusable restriction word vowel example corresponding utterance green segmentation potential boundary invalid inserting word produce word vowel speech sample subject simulation sample mother speaking daughter mother speaking researcher sample taken childes database macwhinney snow study reported sample checked consistent word spelling t changed transcribed ascii based phonemic representation transcription system based ipa character consonant vowel diphthong r colored vowel syllabic consonant represented character example boy written bird brd label lebl purpose phonotactic constraint syllabic consonant treated vowel length selected number available segmentation point equal phonotactic constraint applied child directed sample token type adult directed sample token type sample fed simulation division word sentence removed space possible hypothesis vast method finding minimum length hypothesis considering hypothesis necessary following method evaluate input sample segmentation point added evaluate hypothesis obtained adding segmentation point shortest hypothesis found previous step evaluate hypothesis obtained adding",
        "segmentation point continue way sample segmented smallest possible unit report shortest hypothesis found variant simulation dist free free phonotactic restriction hypothesis form dist refers measurement distributional information whereasdist phono phonotactic restriction described simulation run sample total dist run simulation run sample measure chance performance rand free inserted random segmentation point reported resulting hypothesis rand phono inserted random segmentation point permitted phonotactic constraint rand simulation given number segmentation point add equal number segmentation point needed produce natural english segmentation performance upper bound chance performance contrast dist simulation determine number segmentation point add mdl evaluation result rand simulation average trial input sample",
        "representation scheme described based information theory example coding system vitnyi quinlan rivest representation derive formula describing length bit discrete form formula work practice simulation use continuous approximation discrete formula approximation involves dropping ceiling function length computation example use self delimiting representation integer described vitnyi pp representation number bit needed code integer given use following approximation discrete formula difference difference bit continuous formula difference difference found easier interpret result continuous function following discussion present approximate formula lexicon list word represented phoneme sequence paired code word example binary representation column represented column called word inventory column second column called code word inventory column word inventory column figure schematic list lexical item represented continuous string phoneme separator word ktktisi mark boundary lexical item phoneme string preceded list integer representing length phoneme word length represented fixed length padded binary number list single integer denoting length length field integer represented unary length need known advance entire column number lexical entry coded self delimiting integer length representation integer given functionwe define number phoneme word total unique phoneme sample represent phoneme fixed length bit string length length representation word lexicon number phoneme word time length phoneme total length word lexicon sum formula lexical item stated length field divide phoneme string fixed length field integer number phoneme longest word representing integer take bit length field self delimiting width field represented self delimiting way use unary representation write extra field consisting bit followed terminating field word unary prefix combined length field prefix terminating total length word inventory column representation sum term code word inventory column lexicon figure schematic identical representation previous column code word listed phonemic word length field unary prefix serve purpose marking division code word sample represented assigning short code word frequent word reserving longer code word infrequent word satisfy property code word assigned length frequency based length code word word frequency",
        "greater total length code word list sum code word length lexical entry word inventory column described length code word represented fixed length field frequent word longest code word property formula longest possible code word come word frequency field contains integer number define length field represent width field unary total element size field unary representation field width combined length field prefix terminating total length code word inventory column representation sum term sequence word form sample figure schematic represented number word sample followed list code word code word compact index lexicon original sample reconstructed looking code word list replacing phoneme sequence lexicon code word assigned lexical item self delimiting set code known need represent boundary code word length representation integer given functionthe length representation sample computed summing length code word represent sample simplify description noting combined length occurrence particular code word occurrence code word sample length encoded sample sum formula word lexicon total length sample given adding term total length representation entire hypothesis sum representation length word inventory column code word inventory column sample system computing hypothesis size efficient sense element thought represented code word assigned based relative frequency word final evaluation given hypothesis estimate minimal number bit required transmit hypothesis permit direct comparison competing hypothesis shorter representation hypothesis distributional information extracted better hypothesis",
        "phonotactic knowledge given system list licit initial final consonant cluster english word list checked sample list permissive underlined consonant cluster explore divided ek splore eks plore simulation phonotactic knowledge word boundary inserted create word initial final consonant cluster list create word vowel example actual sample corresponds utterance want help baby second line word boundary legal marked dot boundary illegal legal word english boundary illegal valid word initial consonant cluster boundary illegal valid word final consonant cluster boundary legal valid word final cluster valid word initial cluster phonotactic constraint reduces number potential word boundary example system insert new word boundary update list remaining valid insertion point adding point cause nearby point unusable restriction word vowel example corresponding utterance green segmentation potential boundary invalid inserting word produce word vowel",
        "speech sample subject simulation sample mother speaking daughter mother speaking researcher sample taken childes database macwhinney snow study reported sample checked consistent word spelling t changed transcribed ascii based phonemic representation transcription system based ipa character consonant vowel diphthong r colored vowel syllabic consonant represented character example boy written bird brd label lebl purpose phonotactic constraint syllabic consonant treated vowel length selected number available segmentation point equal phonotactic constraint applied child directed sample token type adult directed sample token type sample fed simulation division word sentence removed space possible hypothesis vast method finding minimum length hypothesis considering hypothesis necessary following method evaluate input sample segmentation point added evaluate hypothesis obtained adding segmentation point shortest hypothesis found previous step evaluate hypothesis obtained adding segmentation point continue way sample segmented smallest possible unit report shortest hypothesis found variant simulation dist free free phonotactic restriction hypothesis form dist refers measurement distributional information whereasdist phono phonotactic restriction described simulation run sample total dist run simulation run sample measure chance performance rand free inserted random segmentation point reported resulting hypothesis rand phono inserted random segmentation point permitted phonotactic constraint rand simulation given number segmentation point add equal number segmentation point needed produce natural english segmentation performance upper bound chance performance contrast dist simulation determine number segmentation point add mdl evaluation result rand simulation average trial input sample",
        "simulation scored number correct segmentation point inserted compared natural english segmentation scoring value computed recall percent correct segmentation point found andaccuracy percent hypothesized segmentation point correct term hit false alarm miss result given table trade recall accuracy possible segmentation point added recall accuracy low segmentation point added word accuracy recall low goal segment speech accuracy important finding correct segmentation example deciding littlekitty word disastrous deciding tle word assigning meaning littlekitty reasonable try learning word meaning pair trying assign separate meaning tle problematic performance dist phono child directed speech show system go long way solving segmentation problem comparing average performance simulation useful effect phonotactic information seen comparing average performance rand free rand phono difference addition phonotactic constraint segmentation phonotactic constraint useful recall accuracy improve similar comparison rand free dist free show distributional information improves performance result free distributional information favor recall accuracy fact segmentation hypothesis produced dist free word broken single phoneme unit handful word remaining intact comparison needed combination distributional phonotactic information performs source dist phono compared rand phono effect adding distributional analysis phonotactic constraint dist phono compared dist free effect adding phonotactic constraint distributional analysis comparison show source combined useful phonotactic information comparison obvious trade recall accuracy reversed clear winner discovered word type help comparison dist free found word accuracy dist phono found word accuracy segmentation point data inconclusive word type data demonstrate combining information source useful distributional information obvious difference performance child adult directed speech dist phono combined information source difference striking accuracy remains high recall rate triple child directed speech difference supported word type data recall accuracy adult directed speech recall accuracy child directed speech",
        "technique segment continuous speech word distributional phonotactic information expect recall segmentation point accuracy sample yield recall word type accuracy low type accuracy mitigated fact incorrect word meaningful concatenation correct word thekitty finding confirms idea distribution phonotactics useful source information infant use discovering word jusczyk fact help explain infant ability learn word parental speech source useful infant prosody word stress pattern available suggests semantics isolated word need play central role think jusczyk downplayed utility word isolation difficult impossible given available method determine source information necessary infant segment speech learn word sort indirect evidence available result difference adult child directed speech easier segment given distribution phonotactics lends quantitative support research suggests motherese differs normal adult speech way useful language learning infant aslin fact factor making motherese learnable elucidated technique compare result different model containing different factor combination factor looking substantial performance difference exists child adult directed speech model us phonotactic constraint absolute requirement structure individual word implies phonotactics learned attempt segmentation phonotactics learned access lexicon demonstration trapped circular reasoning brent demonstrate phonotactics learned high accuracy unsegmented utterance simulation general method exist combining information source mdl paradigm absolute requirement plausible hypothesis phonotactic constraint requirement learnable method combination include information source internal representation hypothesis distributional information component representation learned ellison example multiple component representation like extend system detailed transcription system expect help system find word boundary reason detailed church brief allophonic variation useful predicting word boundary simpler extension research increase length speech sample try current system sample language sure method generalizes research program provide complementary evidence supporting hypothesis source information infant use learning native language research focused demonstration infant sensitivity source begun provide quantitative measure usefulness source",
        "paper describes lhip left head corner island parser parser designed broad coverage handling unrestricted text system interprets extended dcg formalism produce robust analyser find par input island terminal corresponding terminal consumed successful grammar rule use processing dialogue transcript hcrc map task corpus anderson expect eventual application wider natural speech contains number frequent characteristic ungrammatical phenomenon filled pause repetition restarts etc analysis conversation account purpose represent significant obstacle analysis provides processing method allows selected portion input ignored handled chief modification standard prolog grammar rule format type right hand rh item marked head rh item marked ignorable expand point introduce difference behaviour lhip understood term notion island span cover threshold island input string consisting terminal island subsequence length span grammar rule length longest island terminal consumed rule said cover item terminal consumed island described coverage threshold rule minimum value ratio coverage span hold order rule succeed rule covered span consuming terminal implied rule need cover input order succeed constraint applied creating island island adjacent separated non covered input island contain input unaccounted grammar overlap multiple analysis exist involve different segmentation input island notion non coverage input sanctioned unsanctioned non coverage case arises grammar account terminal non coverage mean number special ignore rule applied simulate coverage input material lying island effect making island contiguous part input ignored considered consumed ignore rule invoked class capability distinguishes ignore rule regular rule equivalent serving notational aid grammar writer adjacency rh clause specified grammar possible define global local threshold proportion spanned input covered rule way user lhip grammar exercise fine control required accuracy completeness analysis chart kept success failure rule improve efficiency provide mean identifying unattached constituent addition feedback given grammar writer degree grammar able cope given input context grammar development serve notification area coverage grammar extended notion head employed connected processing control linguistics particular requires head rule",
        "section describe lhip system define constitutes acceptable lhip grammar second describe process converting grammar prolog code describe analysis input grammar grammar extended form prolog dcg grammar extension summarized follows rh clause nominated head rh clause marked optional ignore rule invoked adjacency constraint imposed rh clause global threshold level set determine minimum fraction spanned input covered parse anda local threshold level set rule override global threshold rule provide syntactic definition lhip grammar rule notation syntactic rule form indicates category form optional item form denoted surrounding square bracket category italicised terminal underlined lhip grammar rule form value present value defines local threshold fraction rule local threshold value overrules global threshold symbol rule mark ignore rule rule defined way invoked ignore rule rh clause connective precedence prolog precedence resolve ambiguity connective indicate string subsumed rh clause ordered adjacent input indicates precedes input intervening material stronger constraint immediate precedence marked indicates span precedes uncovered input expressed optional rh clause surrounded symbol indicate head clause rule prolog term rule terminal item act head rule body symbol introduces terminal string said purpose ignore rule consume input terminal intended use facilitating repair analysing input contains false start restarts filled pause etc rule referred preceding symbol referred class rule body special rh clause rule body indicate input ignored problem ignore rule intended repair occur case rule succeed consuming input semantic restriction body rule contain clause cover input optional clause ignore rule cover input following example lhip rule sub rule conjunction conj marked head evaluated rule converted prolog code lhip system rule read rh clause partitioned marked head record kept original ordering record allows clause constrained respect clause precedes respect head clause follows code added maintain chart known success failure rule rule turned prolog clause additional argument added argument input start",
        "end point area input rule succeed start end point actual input fact succeeds number terminal item covered island reference point chart result stored list pointer sub result converted form rule given code chart maintenance important point note converted form following conjunction clause searched clause region input searched conjunction clause rule lh b c island extends cover item search region clause b o start lh search region start conjunction island island start cover item search region second clause p c end conjunction island end lh search region island end cover item island associated rule extends cover item unifies argument current global threshold value current implementation lhip compiled rule interpreted depth left right standard prolog theorem prover giving analyser proceeds left head corner fashion reordering carried compilation situation left recursion subtle conventional dcg conjunct rule shown case point sight appears left recursive inspection converted form show true leftmost subrule conjunction compilation induce left recursion eliminating case lhip suffer termination problem ordinary dcg formalism interpreted way ordinary dcg formalism possible apply different parsing method lhip order circumvent problem pereira shieber related issue concern interpretation embedded prolog code rh clause result code precedes head lhip rule evaluated judicious freezing goal avoidance unsafe cut required provides number way applying grammar input simplest allows enumerate possible analysis input grammar order result produced reflect lexical ordering rule converted lhip threshold level set analysis possible grammar deletion input terminal generated supposing suitable grammar sentence john saw mary mark saw analysis corresponding sentence john saw mary john saw mark john saw mary saw mary mark saw etc setting threshold partial analysis unaccounted terminal span succeed mark saw receive valid analysis mary mark saw provided grammar contains rule conjoined np john saw hand example illustrates partial analysis kind fact correspond true sub parse input mary mark conjoined subject original care taken interpreting result number built predicate provided allow user constrain behaviour",
        "parser way based notion coverage span threshold input parsed instance category input covered bind beginning island described application bind position following end cov bind number terminal covered maximal coverage cov set par coverage cov set par span seq sequence par non overlapping consumes input precedes consumed set par highest threshold value backtracking return set highest threshold value addition predicate search chart constituent identified attached parse tree include successful rule indicating island position coverage list specific successful rule succeeded result list successful instance rule sentence receives complete analysis contain parsable substring result recorded position input predicate partial useful information extracted sentence despite global failure parse section conversion grammar prolog code mean user system develop analysis tool apply different constraint tool provided building block",
        "mentioned lhip facilitates cyclic approach grammar development writing english grammar map task corpus following attempt rule noun phrase appropriate rule determiner noun rule analyse simple np map missionary camp right hand corner analysis right hand corner long sentence join determiner start sentence noun occur half sentence number superfluous analysis reduced imposing local threshold level looking analysis sentence corpus rule give skeleton noun phrase fraction coverage par leaf important feature adjective found noun phrase rule handle phrase left hand corner banana tree rule applied corpus rule applied local threshold level looking item parsed case second identify feature noun phrase found corpus covered current rule include instance phrase form dipping line grammar noun phrase rule need changed account certain type modifier adjective adverbial modifier possible set local threshold making use prolog code facility way strictness rule varied according information originating particular run time invocation rule current parse example possible providing suitable definition set optional adjective found given rule set rule stable writer satisfied performance grammar local threshold value assigned superfluous par interfere work use chart store known result failure allows user develop hybrid parsing technique relying default depth strategy given analysing respect category instance possible analyse input layer linguistic category starting analysing noun phrase preposition verb relative clause clause conjuncts complete sentence strategy allows user perform processing result layer useful trying find best analysis",
        "discussion built predicate mentioned facility recovering partial par illustrate process indicate use information obtained following example chart inspected reveal constituent built failed parse truncated sentenceeach rule listed identifier lexical rule island analysed beginning ending position coverage representation constructed output seen grammar manages noun phrase unable deal question initial auxiliary remains unattached interested successful application rule represent maximal constituent displayed unattached lexical item identified instance rule combine postmodifying analysed island tree brook ignoring tree second analysed tree brook consuming terminal second analysis tree brook rule obtained ignoring sequence tree information user wish rank result according respective span coverage ratio preferring second",
        "ability deal large amount formed text principal objective current nlp research proposal include use probabilistic method briscoe carroll large robust deterministic system hindle fidditch hindle suggests system lhip right circumstance provide alternative approach combine advantage prolog interpreted dcgs ease modification parser output suitable direct use program etc ability relax adjacency constraint formalism flexible dynamic manner based assumption partial result useful useful result approximation complete coverage useful come indication approximate point important case grammar usable degree early stage development example case development grammar map task corpus near future expect apply lhip different problem defining restricted language specialized parsing rationale distinction sanctioned unsanctioned non coverage input ignore facility permit different category unidentified input distinguished example interesting separate material occurs start input appearing rule similar functionality normal rule particular argument assign structure unidentified input flagged overall parse setting threshold value lhip perform like interpreted prolog dcg use chart number possible extension system envisaged present rule compiled preferable enhance preprocessing order compute certain kind global information grammar improvement determine possible linking root head sequence rule index terminal item use oracle analysis second identify item early analysis reduce search space subsequent processing scan input begin parsing point proceeding left suggests possibility parallel approach parsing expect measure increase efficiency lhip result returned order determined order rule grammar preferable arrange matter cooperative fashion best highest coverage span ratio displayed bidirectional parsing satta stock appear candidate inclusion later version appear longer term research goal",
        "computational model discourse based analysis intention speaker cohen perrault allen perrault grosz sidner agent certain goal communication result planning process achieve goal speaker form intention based goal act intention producing utterance hearer reconstruct model speaker intention hearing utterance approach strong point provide satisfactory account adherence discourse convention dialogue instance consider simple phenomenon question followed answer explicit statement inability refusal answer intentional story account go follows production question agent agent recognizes agent goal find answer adopts goal tell answer order co operative plan achieve goal generating answer provides elegant account simple case requires strong assumption co operativeness adopt agent goal result explain say know answer predisposed adopting goal approach suggested account behavior allen introduced intentional analysis discourse level addition domain level assumed set conventional multi agent action discourse level tried account kind behavior social intentional construct joint intention cohen levesque shared plan grosz sidner account help explain discourse phenomenon require strong degree co operativity account dialogue coherence provide easy explanation agent act case support high level mutual goal stranger approaching agent asking time unlikely joint intention shared plan met strategic point view agent interest stranger goal met agent respond situation example consider case agent goal prefers interrogating agent find requested information block formation intention inform inspires agent respond example illustrate account question answering recognition speaker intention provide evidence speaker goal adoption goal interlocutor involved formulating response question researcher mann kowtko assume library discourse level action called dialogue game encode common communicative interaction co operative agent participating game question asked fixed number activity introduced question co operative response provide better explanation coherence require agent recognize intention perform dialogue game result work viewed special case intentional view interesting model described airenti separate conversational game task related game similar way litman allen separation assume co operation task agent performing require recognition intention co operation conversational level left unexplained goal",
        "motivate conversational co operation problem system impose co operativity form automatic goal adoption make impossible reason case want violate rule case conversational co operation conflict agent personal goal developing alternate approach take step strong plan based approach strong plan based account mean model set personal goal motivates behavior agent intuition underlying approach close right claim mistake attempt analyze behavior arising agent high level goal believe people complex set motivation action particular behavior arises sense obligation behave limit set society agent model based obligation differs intention based approach obligation independent shared plan intention recognition obligation result rule agent life interaction enabled sufficient compatibility rule affecting interacting agent responds question social behavior encouraged grows instilled agent",
        "model propose agent behavior determined number factor including agent current goal domain set obligation induced set social convention planning agent considers goal obligation order determine action address extent possible prior intention obligation conflict agent delay pursuit intention order satisfy obligation agent behave cost violating obligation given time agent obligation different goal planning involves complex tradeoff different factor example question agent asked question creates obligation respond agent adopt goal answering question personal goal order explain behavior constraint action agent plan fact agent explicit goal answer question obliged offer response consider politician press conference planning task satisfy obligation responding question revealing answer possible case agent know answer obligation respond discharged explicit statement inability answer",
        "obligation represent agent according set norm notion obligation studied century formal aspect examined deontic logic need simple require extensive survey complexity arise literature intuition underlying work help clarify obligation obligation defined term modal operator called permissible action obligatory permissible action forbidden permissible informal semantics operator given positing set rule behavior action obligatory occurrence follows forbidden non occurrence follows action occur occur according obligatory forbidden action obligatory respect set rule mean agent perform action adopt model suggested shoham tennenholtz agent behavior violate defined social law obligation satisfied mean rule broken assume agent plan action violate rule possible obligated action occur conflict agent personal goal agent choose violate different reduced intention goal particular agent obliged action contrary goal example consider child apologize hitting younger brother reduced simple expectation obligation act source expectation guide action interpretation plan recognition process proposed carberry expectation provide sufficient motivation agent perform expected action case wrong unexpected performing expected action interpretation utterance clear coherence prior expectation need allow possibility agent performed action violates expectation agent violates obligation agent held accountable obligation arise variety source conversational setting accepted offer promise incur obligation command request party bring obligation perform requested action obligation discourse obligation model obligation simple use set rule encode discourse convention new conversation act determined performed future action inferred conventional rule obligation use simple chaining technique introduce obligation obligation rule based performance conversation act summarized table agent performs promise perform action performs acceptance suggestion request agent perform action agent obliges achieve action question agent request action performed request brings obligation address request accept reject decision known requester requestee permitted ignore request question establishes obligation answer question utterance understood believed deficient way brings obligation repair utterance belief agent obligation form important reasoning process",
        "deliberative agent architecture proposed bratman addition considering belief world govern possibility performing action likelyhood success desire goal govern utility desirability action social agent consider obligation govern permissibility action large number strategy incorporate obligation deliberative process based weight given compared agent goal castelfranchi present strategy moving obligation action including performing obligated action adopting obligation goal adopting obligated action goal performing action result state desired agent case goal conflict goal agent guaranteed performed general want allow action based obligation supersede performance intended action instance consider agent intention possible obligation imposed perform intended action behaved agent need delay performance obligation dealt example intention perform series inform act listener request repair behaved agent repair inform proceeding initiate intended",
        "obligation belief agent obligation form important reasoning process deliberative agent architecture proposed bratman addition considering belief world govern possibility performing action likelyhood success desire goal govern utility desirability action social agent consider obligation govern permissibility action large number strategy incorporate obligation deliberative process based weight given compared agent goal castelfranchi present strategy moving obligation action including performing obligated action adopting obligation goal adopting obligated action goal performing action result state desired agent case goal conflict goal agent guaranteed performed general want allow action based obligation supersede performance intended action instance consider agent intention possible obligation imposed perform intended action behaved agent need delay performance obligation dealt example intention perform series inform act listener request repair behaved agent repair inform proceeding initiate intended",
        "built system us discourse obligation communicative intention partake natural dialogue system play role dialogue manager train dialogue system act intelligent planning assistant transportation domain domain assumption co operation valid obligation model provides simpler analysis discourse behavior plan based account example dialogue train system engage shown figure describe part discourse model detail account aspect dialogue train system allen schubert large integrated natural language conversation plan reasoning system concentrate system discourse actor drive action dialogue manager module illustrates system viewpoint dialogue manager dialogue manager responsible maintaining flow conversation making sure conversational goal met system main goal executable plan meet user goal constructed agreed system user plan executed dialogue manager track current state dialogue determine effect observed conversation act generate utterance send command domain plan reasoner domain plan executor appropriate action represented theory conversation act traum hinkelman augments traditional core speech act level act turn taking grounding clark schaefer argumentation utterance contain act partial act level representing general obligation temporal logic represent general knowledge system maintains stack conversant pending discourse obligation obligation stack represented obligation type paired content stack structure appropriate general respond imposed obligation explained section system attend obligation considering part discourse context obligation result formation intention communicate user intention formed obligation removed stack met reason system dropped intention satisfying obligation current system place stack riding goal train domain construct execute plan shared participant lead goal accepting proposal agent suggested performing domain plan synthesis proposing agent plan domain plan reasoner constructed executing completed plan designing agent control behavior dialogue manager choose reactive approach system deliberate add new intention performed action intended shown new obligation need addressed performing intended action agent deliberative behavior characterized sense deciding agent considers obligation decides update intentional structure add new goal intention based obligation lead immediate",
        "action obligation agent consider intention perform action satisfy intention performable intention system deliberate overall goal adopt new intention performed iteration discourse actor special consideration given extra constraint participation conversation imposes includes weak general obligation acknowledging utterance interrupting extra goal coming domain setting maintain shared view world domain plan executed prioritize source deliberation actor follows discourse obligation tableweak obligation interrupt user turnintended speech actsweak grounding coordinate mutual belief discourse goal domain plan negotiationhigh level discourse goalsthe implemented actor serializes consideration source algorithm figure updating conversational state perceived conversation act action module system progress operation discourse actor discourse actor active decide task attempt according priority given figure work task completing particular task run loop searching task context changed observance new utterance user actor running decides iteration speak according turn taking convention system need wait user utterance observed invoke actor need respond user utterance utterance utterance fashion algorithm figure indicate actor priority fulfilling obligation actor think best meet obligation obligation address request actor evaluate request reasonable accept reject sufficient information decide attempt clarify parameter case meeting obligation form intention tell user decision acceptance rejection clarification intention acted utterance produced obligation discharged obligation type repair uninterpretable utterance presupposition violated answer question question answering actor query belief answer depending result system know answer case actor form intention produce appropriate utterance waiting chance according turn taking convention generate utterance certain case repair system try control turn produce utterance motivation obligation system adopts relaxed conversational style try turn given user user pause conversation start lag line system turn conversational state updated actor try deliberate act system turn actor checking obligation examines intended conversation act call generator produce utterance line discourse actor algorithm utterance",
        "produced reinterpreted indicated figure conversational state updated end releasing turn generate intended act utterance case remain intended act left future utterance care subsequent situation merit dropping intention intended speech act argumentation act uttered kept intention revert caused intention formed subsequent deliberation cause intention adopted intended conversation act thing actor considers grounding situation line actor try believed grounded particular speech act performed involve acknowledging repairing user utterance repairing requesting acknowledgement system utterance grounding considered urgent acting based communicative intention grounding act performed basis obligation arise interpreting prior utterance accessible utterance grounded actor considers negotiation domain belief intention line actor try work shared domain plan adding intention perform appropriate speech act work goal includes accepting rejecting requesting retraction user proposal requesting acceptance retracting system proposal initiating new system proposal counterproposal actor look user proposal shared found add intention accept proposal proposal deficient way help goal system come better alternative case system reject user proposal present argue proposal actor look proposal accepted requesting user accept acknowledged retracting reformulating rejected actor check private plan part plan proposed find adopt intention suggestion user local conversational structure constraint described require attention actor concern actual high level goal train system include making call domain plan reasoner domain executor return material update system private view plan initiate new proposal point actor control conversation pursuing objective responding user system unmet goal work achieving line hand turn user try end conversation belief user goal met functioning actor illustrated behavior dialogue figure discussion informal skip detail dialogue processed manner implemented system detail dialogue manager operation example found traum interpreted performing core speech act interpreted initiation inform obligation perform domain",
        "action shipping orange utterance seen initiation indirect suggestion action goal shared domain plan achieve performance action addition utterance release turn system show relevant part discourse state interpretation utterance interpreting utterance system decides acknowledge utterance line actor algorithm moving suggestion unacknowledged unaccepted accept proposal line system act intention produced deliberation line produce combined acknowledgement acceptance utterance acceptance make goal shared satisfies discourse goal getting domain goal work interpreted responded user keep turn case following subsequent utterance system chance act invokes discourse obligation system respond user assertion give turn system resulting discourse context system decides acknowledge shown figure system query domain knowledge base decides user correct orange corning decides meet obligation line answering affirmative result forming intention inform realized acknowledgement utterance production utterance consideration hold system response reasoning leading utterance similar leading utterance user suggesting domain action help lead goal system get turn acknowledges accepts suggestion interpreted request imperative surface structure discourse obligation address request incurred system decides acknowledge utterance ground decision acknowledge obligation incurred system address request deciding accept adding intention perform accept speech act produced interpreted request evaluation plan system decided acknowledge creates discourse obligation address request system considers invoking domain plan reasoner search plan problem incomplete part decides plan work decides perform requested action evaluation speech act generated discourse state decision acknowledge shown figure user assent system check goal having come suitable plan executes plan domain sending completed plan domain plan executor example illustrates small fraction capability dialogue model dialogue system needed follow initiative user architecture handle varying degree initiative remaining responsive default behavior allow user maintain initiative plan construction phase dialogue user stop asks help give initiative continuing suggestion system switch plan recognition plan elaboration devise plan satisfy goal plan plan constructed dialogue illustrate system behaving basis goal obligation modification",
        "previous example user release turn system utterance deliberation proceeds follows system obligation communicative intention ungrounded unaccepted proposal system start high level goal goal form shared plan fact current plan consisting single commodity action executable actor domain plan reasoner elaborate plan return list augmentation plan assumed including engine event generates commodity given condition orange boxcar attached engine choice point possibility added choice particular engine boxcar use user taken turn system propose new item user choice resolved way domain executor queried preference based prior experience system matter user form alternative question arbitrary choice suggest user user expected acknowledge react proposal system acknowledgement request acknowledgement time considers grounding situation proposal accepted rejected system request acceptance proposal rejected system negotiate offer counterproposal accept counter proposal user domain plan reasoner ferguson performs plan recognition plan elaboration incremental fashion proposal system user integrated mixed initiative fashion termination condition shared executable plan achieves goal action collaborative planning process based local consideration",
        "representing general obligation temporal logic represent general knowledge system maintains stack conversant pending discourse obligation obligation stack represented obligation type paired content stack structure appropriate general respond imposed obligation explained section system attend obligation considering part discourse context obligation result formation intention communicate user intention formed obligation removed stack met reason system dropped intention satisfying obligation current system place stack riding goal train domain construct execute plan shared participant lead goal accepting proposal agent suggested performing domain plan synthesis proposing agent plan domain plan reasoner constructed executing completed plan",
        "designing agent control behavior dialogue manager choose reactive approach system deliberate add new intention performed action intended shown new obligation need addressed performing intended action agent deliberative behavior characterized sense deciding agent considers obligation decides update intentional structure add new goal intention based obligation lead immediate action obligation agent consider intention perform action satisfy intention performable intention system deliberate overall goal adopt new intention performed iteration discourse actor special consideration given extra constraint participation conversation imposes includes weak general obligation acknowledging utterance interrupting extra goal coming domain setting maintain shared view world domain plan executed prioritize source deliberation actor follows discourse obligation tableweak obligation interrupt user turnintended speech actsweak grounding coordinate mutual belief discourse goal domain plan negotiationhigh level discourse goalsthe implemented actor serializes consideration source algorithm figure updating conversational state perceived conversation act action module system progress operation discourse actor discourse actor active decide task attempt according priority given figure work task completing particular task run loop searching task context changed observance new utterance user actor running decides iteration speak according turn taking convention system need wait user utterance observed invoke actor need respond user utterance utterance utterance fashion algorithm figure indicate actor priority fulfilling obligation actor think best meet obligation obligation address request actor evaluate request reasonable accept reject sufficient information decide attempt clarify parameter case meeting obligation form intention tell user decision acceptance rejection clarification intention acted utterance produced obligation discharged obligation type repair uninterpretable utterance presupposition violated answer question question answering actor query belief answer depending result system know answer case actor form intention produce appropriate utterance waiting chance according turn taking convention generate utterance certain case repair system try control turn produce utterance motivation obligation system adopts relaxed",
        "conversational style try turn given user user pause conversation start lag line system turn conversational state updated actor try deliberate act system turn actor checking obligation examines intended conversation act call generator produce utterance line discourse actor algorithm utterance produced reinterpreted indicated figure conversational state updated end releasing turn generate intended act utterance case remain intended act left future utterance care subsequent situation merit dropping intention intended speech act argumentation act uttered kept intention revert caused intention formed subsequent deliberation cause intention adopted intended conversation act thing actor considers grounding situation line actor try believed grounded particular speech act performed involve acknowledging repairing user utterance repairing requesting acknowledgement system utterance grounding considered urgent acting based communicative intention grounding act performed basis obligation arise interpreting prior utterance accessible utterance grounded actor considers negotiation domain belief intention line actor try work shared domain plan adding intention perform appropriate speech act work goal includes accepting rejecting requesting retraction user proposal requesting acceptance retracting system proposal initiating new system proposal counterproposal actor look user proposal shared found add intention accept proposal proposal deficient way help goal system come better alternative case system reject user proposal present argue proposal actor look proposal accepted requesting user accept acknowledged retracting reformulating rejected actor check private plan part plan proposed find adopt intention suggestion user local conversational structure constraint described require attention actor concern actual high level goal train system include making call domain plan reasoner domain executor return material update system private view plan initiate new proposal point actor control conversation pursuing objective responding user system unmet goal work achieving",
        "functioning actor illustrated behavior dialogue figure discussion informal skip detail dialogue processed manner implemented system detail dialogue manager operation example found traum interpreted performing core speech act interpreted initiation inform obligation perform domain action shipping orange utterance seen initiation indirect suggestion action goal shared domain plan achieve performance action addition utterance release turn system show relevant part discourse state interpretation utterance interpreting utterance system decides acknowledge utterance line actor algorithm moving suggestion unacknowledged unaccepted accept proposal line system act intention produced deliberation line produce combined acknowledgement acceptance utterance acceptance make goal shared satisfies discourse goal getting domain goal work interpreted responded user keep turn case following subsequent utterance system chance act invokes discourse obligation system respond user assertion give turn system resulting discourse context system decides acknowledge shown figure system query domain knowledge base decides user correct orange corning decides meet obligation line answering affirmative result forming intention inform realized acknowledgement utterance production utterance consideration hold system response reasoning leading utterance similar leading utterance user suggesting domain action help lead goal system get turn acknowledges accepts suggestion interpreted request imperative surface structure discourse obligation address request incurred system decides acknowledge utterance ground decision acknowledge obligation incurred system address request deciding accept adding intention perform accept speech act produced interpreted request evaluation plan system decided acknowledge creates discourse obligation address request system considers invoking domain plan reasoner search plan problem incomplete part decides plan work decides perform requested action evaluation speech act generated discourse state decision acknowledge shown figure user assent system check goal having come suitable plan executes plan domain sending completed plan domain plan executor example illustrates small fraction capability dialogue model dialogue system needed follow initiative user architecture handle varying degree initiative remaining responsive default behavior allow user maintain initiative plan construction phase dialogue user",
        "stop asks help give initiative continuing suggestion system switch plan recognition plan elaboration devise plan satisfy goal plan plan constructed dialogue illustrate system behaving basis goal obligation modification previous example user release turn system utterance deliberation proceeds follows system obligation communicative intention ungrounded unaccepted proposal system start high level goal goal form shared plan fact current plan consisting single commodity action executable actor domain plan reasoner elaborate plan return list augmentation plan assumed including engine event generates commodity given condition orange boxcar attached engine choice point possibility added choice particular engine boxcar use user taken turn system propose new item user choice resolved way domain executor queried preference based prior experience system matter user form alternative question arbitrary choice suggest user user expected acknowledge react proposal system acknowledgement request acknowledgement time considers grounding situation proposal accepted rejected system request acceptance proposal rejected system negotiate offer counterproposal accept counter proposal user domain plan reasoner ferguson performs plan recognition plan elaboration incremental fashion proposal system user integrated mixed initiative fashion termination condition shared executable plan achieves goal action collaborative planning process based local consideration",
        "argued obligation play important role accounting interaction dialog replace plan based model augment resulting model account discourse behavior adversarial situation situation implausible agent adopt goal obligation learned social norm guide agent behavior need intention recognition use shared plan discourse level complex intention recognition required complex interaction needed handle typical interaction everyday discourse requirement agreed rule create obligation agent agree rule smoother interaction rule universal agent set individual rule need appeal shared knowledge account local discourse behavior argued architecture us obligation provides simpler implementation strong plan based approach particular local discourse behavior arise reactive manner need complex planning coin new set problem arise planning action satisfy multiple constraint arise agent personal goal perceived obligation model presented allows mixed initiative conversation varying level co operativity initiative seen obligation driven process leading conversation goal driven obligation goal allows system shift mode co operative domain train system subordinate working goal working concern user having shared discourse plan co operative situation architecture allow system adhere conversational convention respond different way rejecting proposal refusing answer question",
        "generation machine translation ultimate goal natural language processing conventional word frequency based generation system kuhn lacking inter sentential discourse structural analysis liable generate incoherent abstract hand conventional knowledge script based generation system lehnert fum owe success limitation domain applied document varied subject popular scientific magazine realize domain independent generation system computational theory analyzing linguistic discourse structure practical procedure established developed theory arranged kind relationship sentence text coherency viewpoint hobbs sidner proposed theory accounted interaction notion discourse linguistic structure intention attention grosz sidner allen described model discourse structure conversation built recognizing participant plan litman allen theory depend extra linguistic knowledge accumulation present problem realization practical analyzer proposed framework analyzing structure argumentative discourse cohen provide concrete identification procedure evidence relationship sentence linguistic clue indicate relationship relationship successive sentence considered scope relationship cover analyzed explicit connective detected thompson proposed linguistic structure text describing relationship sentence relative importance mann thompson method extracting relationship superficial linguistic expression described paper developed computational model discourse japanese expository writing implemented practical procedure extracting discourse structure sumita model discourse structure defined rhetorical structure compound rhetorical relation sentence text generation realized suitable application extracted rhetorical structure paper describe discourse model discus generation system based",
        "rhetorical structure represents relation chunk sentence body section paper rhetorical structure represented layer intra paragraph inter paragraph structure intra paragraph structure structure representation unit sentence inter paragraph structure structure representation unit paragraph text rhetorical pattern clarify principle argument connective expression state inter sentence relationship significant typical grammatical category connective expression connective sentence predicate divided category exemplified table rhetorical relation sentence relationship preceding text extracted accordance connective expression sentence sentence explicit connective expression extension relation set sentence relation exemplified table representing rhetorical structure show paragraph article titled crossing rate estimate frequency speech signal underlined word indicate connective expression fourth fifth sentence exemplification sentence sixth sixth sentence concluding sentence rhetorical structure text represented binary tree shown fig structure represented follows rhetorical structure represented binary tree analogy syntactic tree natural language sentence sub tree rhetorical structure form argumentative constituent sub tree syntactic tree form grammatical constituent sub tree rhetorical structure sub categorized relation parent node syntactic tree",
        "rhetorical structure represents logical relation sentence block sentence section document rhetorical structure analysis determines logical relation sentence based linguistic clue connective anaphoric expression idiomatic expression input text recognizes argumentative chunk sentence structure extraction consists major sub process sentence analysis accomplishes morphological syntactic analysis sentence relation extraction detects rhetorical relation construct sequence sentence identifier relation detects rhetorical expression distant sentence define rhetorical structure added sequence produced step form restriction generating structure step example expression reason course extracted structural constraint added sequence form chunk expression generation generates possible rhetorical structure described binary tree violate segmentation restriction judgement selects structure candidate lowest penalty score value determined based preference rule neighboring relation candidate process selects structure candidate lowest penalty score value determined based preference rule neighboring relation candidate preference rule process represents heuristic local preference consecutive rhetorical relation sentence sequence arbitrary block sentence premise discussion considered structure preferable score imposed structure candidate violating preference rule example text fig structure candidate contain substructure say sentence entailment sentence penalized author investigated pair rhetorical relation derived preference rule system analyzes inter paragraph structure analysis intra paragraph structure system us rhetorical relation sentence paragraph analysis executes step intra paragraph analysis",
        "system generates section document examining rhetorical structure process consists following stage evaluationstructure reductionin sentence evaluation stage system calculate importance sentence original text based relative importance rhetorical relation categorized type shown table relation categorized rightnucleus right node important point view generation left node case leftnucleus relation situation node bothnucleus relation equivalent importance example right node serial relation yotte conclusion left node relation categorized rightnucleus right node important left node actual sentence evaluation carried demerit marking way order determine important text segment system imposes penalty node rhetorical relation according relative importance system imposes penalty left node rightnucleus relation right node leftnucleus relation add penalty root node terminal node turn calculate penalty node structure reduction stage system cut node terminal node imposed highest penalty list terminal node final structure original document expected length case system cut terminal node sentence given penalty score text written rhetorical structure contains bothnucleus relation parallel mata system gradate penalty reduce sentence sentence paragraph reduced inter paragraph structure reduction carried way based relative importance judgement inter paragraph rhetorical structure penalty calculation mentioned accomplished rhetorical structure shown fig penalty score calculated shown fig fig italic number penalty system imposed node structure broken line boundary node imposed different penalty score figure show sentence penalty score sentence sentence sentence penalty score case system selects sentence longest select sentence shorter select sentence shorter sentence included determined system arranges sentence connective relation extracted realizes text important feature generated abstract composed rhetoricaly consistent unit consist sentence form rhetorical substructure contain fragmentary sentence understood example generation mentioned sentence appear appears sentence sentence appeared sentence understand text",
        "generated abstract evaluated point view key sentence coverage editorial article asahi shinbun japanese newspaper technical paper toshiba review journal toshiba corp publishes short expository paper page selected subject judged key sentence important key sentence text editorial article average correspondence rate key sentence important key sentence subject technical paper abstract generated compared selected key sentence result shown table technical paper average length ratio original coverage key sentence important key sentence editorial average length ratio original coverage key sentence important key sentence reason compression rate key sentence coverage technical paper higher editorial considered follows technical paper contains rhetorical expression general expository provide linguistic clue system extract rhetorical structure structure reduced length get shorter omitting key sentence hand editorial relation sentence supposed understood expressed lack linguistic clue system extract rhetorical structure",
        "developed automatic generation system japanese expository writing based rhetorical structure extraction rhetorical structure provides natural order importance sentence text determine sentence extracted according desired length rhetorical structure provides rhetorical relation extracted sentence generate appropriate connective generation based rhetorical structure extraction merit conventional word frequency based generation system kuhn generated consistent original text connective sentence reflect relation original text rhetorical structure obtained length generated abstract generated repeating reduction process get desired length conventional knowledge script based generation system lehnert fum rhetorical structure extraction need prepared knowledge script related original text text domain contain rhetorical expression expository writing generated composed rhetoricaly consistent unit consist sentence form rhetorical substructure contain fragmentary sentence understood limitation system error rhetorical structure analysis sentence selection type generation evaluation accuracy rhetorical structure analysis carried sumita showed length necessary utilize inner sentence analysis realize phrase selection type generation based anaphora resolution topic supplementation realized analysis system utilized text browser prototypical interactive document retrieval system",
        "resource based approach semantic composition lexical functional grammar lfg obtains interpretation phrase logical deduction beginning interpretation part premise dalrymple resource sensitive system linear logic compute meaning accordance relationship manifest lfg f structure property system ensure meaning allowing coherence completeness condition f structure kaplan bresnan maintained case single constituent appears yield contribution meaning utterance obvious sentence involving coordination example instance object different verb hallmark linear logic approach ensure f structure contribution utilized derivation construction glance appear problematic approach argue resource sharing manifest treatment coordination approach handled exploiting structure sharing lfg f structure refine previous analysis account case f structure reached multiple path enclosing f structure provides account lfg semantics represents meaning lexical item linear logic formula formula manipulate basic assertion form f structure meaning logic term mapping semantic projection relates f structure semantic structure distinguish multiple path entering f structure map set path f structure semantic structure path f structure available semantic space resource make possible semantic formula exploit information multiple path f structure order account multiple us f structure semantic contribution resulting system restricted case approach overgenerate property resource sensitivity resource sharing appears problematic provides explanatory advantage system replicate resource derivation section review previous approach semantics coordination argument sharing note drawback describe revised semantic framework section work example non constituent coordination right node raising section discus example involving intensional verb section",
        "steedman steedman steedman steedman working framework combinatory categorial grammar ccg present adequate analysis non constituent coordination date noted steedman discussed oehrle addition rule function composition inventory syntactic rule categorial grammar enables formation constituent right peripheral gap providing basis clean treatment case right node raising exemplified sentence example handled coordination schema allows category conjoined shown schema give rise actual rule semantics depends number argument shared material take case rnr considered rule form shown contraction rule allows single argument utilized noted hudson example rnr involve coordinate structure case fall purview coordination schema analysis sentence available ccg framework addition xsubstitute combinator steedman defined steedman use combinator assimilates case noncoordinate rnr case involving parasitic gap approach drawback offer competing analysis syntax sentence seek analysis rnr resource sharing general uniform semantics treatment available ccg tight integration syntax semantics influential adopted semantic treatment coordination approach partee rooth propose generalized conjunction scheme conjuncts type combined case steedman operator contraction inherent schema allows single shared argument distributed argument conjunct lifting allowed produce type necessary combination coordination scheme type lifting effect copying argument higher type quantifier case coordinated intensional verb propose processing strategy requiring expression interpreted lowest possible type type raising taking place necessary illustrate partee rooth assume extensional verb find entered lexicon basic type intensional verb like want require quantifier argument type ignoring intensionality extensional verb find support coordinated basic type intensional verb want seek coordinated basic higher type argument expression quantified intensional extensional verb coordinated extensional verb type raised promote type intensional verb lead desired result unwelcome consequence approach appears gone unnoticed literature arises case verb conjoined intensional verb coordinated extensional verb copy quantifier distributed verb coordinate structure instance extensional verb intensional verb coordinated expression quantifier result quantifier scoped extensional verb wrong result sentence hillary wanted found supported candidate desired result quantifier scope extensional verb hillary found supported candidate case verb extensional obvious way modify partee rooth proposal produce",
        "steedman steedman steedman steedman working framework combinatory categorial grammar ccg present adequate analysis non constituent coordination date noted steedman discussed oehrle addition rule function composition inventory syntactic rule categorial grammar enables formation constituent right peripheral gap providing basis clean treatment case right node raising exemplified sentence example handled coordination schema allows category conjoined shown schema give rise actual rule semantics depends number argument shared material take case rnr considered rule form shown contraction rule allows single argument utilized noted hudson example rnr involve coordinate structure case fall purview coordination schema analysis sentence available ccg framework addition xsubstitute combinator steedman defined steedman use combinator assimilates case noncoordinate rnr case involving parasitic gap approach drawback offer competing analysis syntax sentence seek analysis rnr resource sharing general uniform semantics treatment available ccg tight integration syntax semantics",
        "influential adopted semantic treatment coordination approach partee rooth propose generalized conjunction scheme conjuncts type combined case steedman operator contraction inherent schema allows single shared argument distributed argument conjunct lifting allowed produce type necessary combination coordination scheme type lifting effect copying argument higher type quantifier case coordinated intensional verb propose processing strategy requiring expression interpreted lowest possible type type raising taking place necessary illustrate partee rooth assume extensional verb find entered lexicon basic type intensional verb like want require quantifier argument type ignoring intensionality extensional verb find support coordinated basic type intensional verb want seek coordinated basic higher type argument expression quantified intensional extensional verb coordinated extensional verb type raised promote type intensional verb lead desired result unwelcome consequence approach appears gone unnoticed literature arises case verb conjoined intensional verb coordinated extensional verb copy quantifier distributed verb coordinate structure instance extensional verb intensional verb coordinated expression quantifier result quantifier scoped extensional verb wrong result sentence hillary wanted found supported candidate desired result quantifier scope extensional verb hillary found supported candidate case verb extensional obvious way modify partee rooth proposal produce correct result problem ability copy quantifier inherent schema unrestricted second problem account steedman coordination schema partee rooth type raising strategy applies coordinate structure need type raise extends case involving coordination sentence present analysis preserve intuition underlying partee rooth processing strategy predicts generates correct reading case account applies example involving coordination case sentence",
        "lfg assumes syntactic level representation constituent structure c structure encodes phrasal dominance precedence relation functional structure f structure encodes syntactic predicate argument structure f structure sentence given lexical entry specify syntactic constraint f structure semantic information semantic information expressed ina meaning language anda language assembling meaning glue language meaning language appropriate logic present purpose higher order logic suffice meaning language bill appear right meaning relation glue language tensor fragment linear logic girard semantic contribution lexical entry refer meaning constructor linear logic formula consisting instruction glue language combining meaning lexical entry syntactic argument obtain meaning f structure headed entry instance meaning constructor verb supported glue language formula paraphrasable subj mean obj mean sentence mean system described dalrymple relation associate expression meaning language f structure result f structure contributed single meaning constructor resource derivation linear logic form logical contraction inherent approach discussed earlier case resource shared appear problematic framework need multiple use f structure meaning result appearance particular lexical item conjunction particular syntactic construction parasitic gap construction result multiple path f structure contains structure sharing motivated syntactic ground revise earlier framework model term occurrence f structure resource logic structure regarded finite function set attribute set atomic value semantic form f structure identify occurrence f structure path root occurrence set occurrence f structure identified path set f structure domain projection path set root f structure path set considered satisfy property extension path identical f structure reached path identical path set read f structure example discussed correspondence set path set set f structure picked path set method yield prediction case path set represented resource logic r relation relation represented place predicate form indicate path set appears end path length extending path set f structure appears end singleton path f structure example f structure given result r relation represent path set entering f structure label r relation indicates set path denotes set path concatenated subj subset set path denoted axiom interpretation provides link meaning path set related r relation axiom set path meaning r relation introduced resource produced linear logic operator allows conclusion time necessary r relation introduced f structure deduction performed derive",
        "meaning example meaning constructor r relation axiom lexical entry bill nafta supported according label f structure obtain following premise combining axiom contribution bill yield formula state path set r related path set corresponding f structure bill receives bill meaning r relation formula derive giving meaning subject fthe meaning constructor supported combine derive formula bill supported shown meaning nafta r relation axiom derive meaning shown combine derive step universal instantiation modus ponens second derivation possible supported nafta combined result combined bill use linear logic provides flexible mechanism deducing meaning sentence based f structure representation linguistic phenomenon developed framework extension based including quantifier anaphora dalrymple intensional verb complex predicate dalrymple logic fit resource sensitivity natural language semantics correspondence f structure relationship meaning multiple use resource arises multiple path f structure section system applies case right node raising",
        "consider derivation basic case right node raising rnr illustrated sentence repeated f structure example shown meaning constructor contributed lexical item follows treat binary relation suffices example general allow case constituent conjoined second meaning constructor contributed appearance prefixed linear logic operator time necessary case example r relation resulting feature value relationship manifest f structure equivalent derivation order step meaning bill supported hillary opposed r relation axiom derive meaning bill supported hillary opposed fashion described section combine antecedent consequents foregoing formula yield consuming meaning r relation cref axiom derive axiom r relation following implication derived formula transitivity obtain consuming contribution nafta universal instantiation modus ponens obtain meaning sentence stage accountable resource consumed deduction complete consider sentence quantified shared rooth observe agree quantifier case scope resulting reading bill supported hillary opposed bill analysis predicts fact way partee rooth analysis meaning contributed lexical item f structure dependency previous example object dalrymple meaning derived contribution f structure trade bill derivation final step derived formula labeled bill supported hillary opposed formula match antecedent quantified meaning universal instantiation modus ponens derive derivation quantifier meaning scope meaning coordinated material result quantifier meaning appears scoping conjunct available rule given return point section analysis extends case noncoordinate rnr example repeated example analysis f structure trade bill resource shared object verb coordinated case limitation preclude going derivation straightforward given semantic contribution lexical item r relation fact coordination involved bearing result semantics resource sharing distinct coordination analysis noted separation possible ccg tight integration syntax semantics lfg syntax semantics interface coupled affording flexibility handle coordinated non coordinated case rnr semantics allows semantics coordination require schema entity polymorphic type meaning type",
        "consider derivation basic case right node raising rnr illustrated sentence repeated f structure example shown meaning constructor contributed lexical item follows treat binary relation suffices example general allow case constituent conjoined second meaning constructor contributed appearance prefixed linear logic operator time necessary case example r relation resulting feature value relationship manifest f structure equivalent derivation order step meaning bill supported hillary opposed r relation axiom derive meaning bill supported hillary opposed fashion described section combine antecedent consequents foregoing formula yield consuming meaning r relation cref axiom derive axiom r relation following implication derived formula transitivity obtain consuming contribution nafta universal instantiation modus ponens obtain meaning sentence stage accountable resource consumed deduction complete",
        "consider sentence quantified shared rooth observe agree quantifier case scope resulting reading bill supported hillary opposed bill analysis predicts fact way partee rooth analysis meaning contributed lexical item f structure dependency previous example object dalrymple meaning derived contribution f structure trade bill derivation final step derived formula labeled bill supported hillary opposed formula match antecedent quantified meaning universal instantiation modus ponens derive derivation quantifier meaning scope meaning coordinated material result quantifier meaning appears scoping conjunct available rule given return point section analysis extends case noncoordinate rnr example repeated example analysis f structure trade bill resource shared object verb coordinated case limitation preclude going derivation straightforward given semantic contribution lexical item r relation fact coordination involved bearing result semantics resource sharing distinct coordination analysis noted separation possible ccg tight integration syntax semantics lfg syntax semantics interface coupled affording flexibility handle coordinated non coordinated case rnr semantics allows semantics coordination require schema entity polymorphic type meaning type",
        "return consider case involving intensional verb preferred reading sentence quantifier scope extensional predicate shown f structure example given meaning constructor lexical item given figure second meaning constructor introduced order handle case conjuncts contribution derivation meaning sentence following r relation result f structural relationship following analysis given dalrymple lexical entry want take quantified argument requires quantified meaning duplicated reading result provide special rule duplicating quantified np necessary interest space step derivation meaning hillary found supported axiom r relation derive duplicate meaning candidate qnp duplication combine copy foregoing formula yield combine meaning candidate meaning hillary wanted axiom r relation obtain foregoing formula deduce desired result specify partee rooth style processing strategy prefer reading require use qnp duplication strategy predicts reading generated example section predicts desired reading sentence reading requires quantifier reading generated partee rooth derivable requires quantifier us qnp duplication preferred reading requiring quantifier us qnp duplication allows flexibility case pragmatic suggests quantifier copied distributed multiple extensional verb partee rooth account apply case intensional verb case account applies case intensional verb coordination example applies case resource sharing",
        "given account resource sharing syntax semantics interface lfg multiple use semantic contribution result viewing dependency f structure resource way correspondence f structure relation meaning maintained resulting account suffer overgeneration inherent approach applies case resource sharing involve coordination lends extension intensional verb case advantage assumed account partee rooth separated issue arriving appropriate f structure syntax issue deriving correct semantics f structure argued correct distinction given treatment second issue treatment issue articulated future forum",
        "unification based grammar formalism viewed generalization context free grammar cfg nonterminal symbol replaced infinite domain feature structure popularity stem way syntactic generalization stated mean constraint feature value expressivity formalism undesirable consequence processing naive implementation unification grammar parser feature structure play role nonterminals standard context free grammar parser large feature structure stored intermediate step computation space requirement algorithm expensive need perform non destructive unification mean large proportion processing time spent copying feature structure approach problem refine parsing algorithm developing technique restriction structure sharing lazy unification reduce structure stored need copying feature structure shieber pereira karttunen kay wroblewski gerdemann godden kogure emele tomabechi harrison ellison technique yield significant improvement performance generality unification based grammar formalism mean case expensive processing unavoidable approach address fundamental issue tradeoff descriptive capacity formalism computational power paper identify set constraint placed unification based grammar formalism order guarantee existence polynomial time parsing algorithm choice constraint motivated showing generalize constraint inherent linear indexed grammar lig begin describing constraint inherent lig admit tractable processing algorithm consider constraint generalized formalism manipulates tree stack constraint identify tree based system regarded constraint unification based grammar formalism patr shieber",
        "indexed grammar viewed cfg nonterminal associated stack index specify nonterminals rewritten associated stack modified described gazdar constrained stack passed mother single daughter size domain nonterminals associated stack analogue nonterminals cfg bound grammar vijay shanker weir demonstrate polynomial time performance achieved use structure sharing possible constraint way lig use stack stack unbounded size arise derivation possible specify dependent unbounded stack appear distinct place derivation tree sharing checking applicability rule step derivation involves comparison structure size goal generalize constraint inherent lig formalism manipulates feature structure stack guiding heuristic avoid formalism generate tree set unbounded number unbounded dependent branch appears structure sharing technique lig generalized straightforward way formalism generalize lig allow stack passed mother daughter recursion produce unbounded number unbounded dependent branch alternative allow unbounded stack shared daughter mother rule mention unbounded stack stack associated mother associated daughter refer extension linear indexed grammar plig stack shared sibling passed mother consequence possibility recursion increase number dependent branch fact number dependent branch bounded length right hand production token plig generate structural description dependent branch begin node sibling tree shown figure unobtainable branch rooted dependent branch originating sibling limitation overcome moving formalism manipulates tree stack consider extension cfg nonterminal associated tree specify tree associated mother related tree associated daughter denote tree order term example following production requires subtrees mother tree shared daughter addition daughter common subtree need incorporate kind generalized notion linearity system linearity restriction lig require mother tree passed daughter partial linearity plig permit subtrees shared mother shared daughter condition tree set shown figure generated node share tree occurs node copy distributed daughter formalism described simulate arbitrary turing machine computation note instantaneous description turing machine encoded tree shown figure turing machine simulated unary production",
        "following production glossed state scanning symbol change state write symbol left solution problem prevent single daughter sharing subtrees mother impose restriction leaf open possibility generating tree branch length violating condition tree bounded number unbounded dependent branch show set tree generated illustrating effect following production assume induction daughter nonterminals associated binary tree height tree constrained equal production given requires identical left subtrees subtrees binary tree right subtrees mother shown allows construction binary tree height repeated unbounded number time binary tree produced overcome problem impose following additional constraint production grammar require subtrees mother passed daughter share subtrees appear sibling mother tree condition rule production responsible building binary tree subtrees sibling mother tree despite fact daughter share common subtree daughter share subtrees special case condition subtrees occurring daughter appear sibling mother condition rule turing machine simulation refer formalism linear tree grammar pltg illustration constraint place shared subtrees figure show local tree appear derivation tree local tree licensed following production respect constraint pltg production figure daughter node labelled share common subtree subtrees shared mother daughter appear sibling tree associated mother",
        "note acyclic feature structure entrancy viewed tree branch labelled feature name atomic value found leaf node interior node unlabelled observation consider constraint formulated tree system pltg constraint unification based grammar formalism patr system linear patr tree feature structure consider possibility entrancy feature structure root plpatr derivation tree involve entrancy following reason believe constitute great limitation appear unification based grammar feature structure associated root tree regarded structure derived input output parser consequence tendency use grammar rule accumulate single large feature structure giving complete encoding analysis unbounded feature information passed tree way violates constraint developed paper giving prominence root feature structure suggest entire derivation tree seen object derived input parser return feature structure associated node tree available feature information need passed tree required order establish dependency derivation tree approach taken need entrancy root feature structure entrancy form shared feature structure node found plpatr example figure",
        "lig powerful cfg known weakly equivalent tree adjoining grammar combinatory categorial grammar head grammar vijay shanker weir powerful lig generate k copy language fixed example plig generate languagefor regular language believe language involving copy string matching bracket described example generated shown example generated pltg pltg generate languagefor context free language appears class language generated pltg included language generated linear context free rewriting system vijay shanker construction involved proof underlies recognition algorithm discussed section case tree set lig tree adjoining grammar tree set generated pltg path set context free language word set string labelling root frontier path derivation tree context free language tree set lig tree adjoining grammar independent branch pltg tree set exhibit dependent branch number dependent branch tree bounded grammar number dependent branch tree set bounded grammar generate set binary tree",
        "section outline main idea underlying polynomial time recognition algorithm plpatr generalizes cky algorithm kasami younger key algorithm use structure sharing technique similar process lig vijay shanker weir understand technique applied case helpful consider simpler case lig cky algorithm recognition algorithm cfg given grammar input string algorithm construct array having element element store nonterminals derive substring naive adaptation algorithm lig recognition involve storing set nonterminals associated stack stack length proportional length input string resultant algorithm exhibit exponential space time complexity worst case shanker weir showed behaviour naive algorithm improved lig derivation application rule depend bounded portion stack storing unbounded stack particular array entry suffices store bounded portion pointer residue figure show lig derivation substring object derivation tree node labelled distinguished descendant root point symbol unbounded stack exposed node called terminator node labelled difficult portion derivation terminator node dependent stack follows stack derivation substring tree corresponding derivation tree capture sense lig derivation exhibit context freeness storage stack achieved storing bounded information nonterminal stack relevant rule application pointer entry representing subderivation object describing adapt technique case discus sense plpatr derivation exhibit context freeness property constraint identified paper ensure feature value manipulated behave stack like way consequence storage technique lig recognition generalized case derived tree shown figure node root subtrees called f terminator g terminator tree root speaking f terminator node node get value feature constraint form plpatr production derivation root terminator depend bounded feature structure root figure feature structure expanded extent dependency example case value feature feature fixed value feature feature fixed mean example applicability production path root root depends feature having value depend value feature tree value",
        "feature isand value feature issuppose addition tree shown figure grammar generates pair tree shown figure feature structure root compatible agree respect part expanded root node context freeness mean given tree shown figure tree shown figure generated grammar give mean storing unbounded feature structure associated node derivation tree derived feature structure analogy situation derived feature structure viewed consisting bounded relevant rule application unbounded information value feature feature store recognition array bounded information value pointer array element element recognition array compatible unifiable bounded local information correspond different possible value feature example use single entry recognition array store fact feature structure appear root tree figure derive substring entry underspecified example value feature specified feature stored array entry substring feature value end story contrast lig plpatr license structure sharing right hand production partial linearity permit feature value shared daughter shared mother case appears checking applicability production point derivation entail comparison structure unbounded size fact plpatr recognition algorithm employ second array called compatibility array encodes information compatibility derived feature structure compatible derived feature structure stored compatibility array approach store feature structure main recognition array presence tuple compatibility array index encode input substring spanned indicates existence derivation compatible feature structure context freeness plpatr new entry added compatibility array manner based existing entry need reconstruct complete feature structure",
        "speech tagging hidden markov model statistical model assign grammatical category word text early work field relied corpus tagged human annotator train model cutting suggest training achieved minimal lexicon priori information probability baum welch estimation refine model paper report experiment designed determine manual training information needed experiment suggests initial biasing lexical transition probability essential achieve good accuracy second experiment reveals distinct pattern baum welch estimation pattern estimation reduces accuracy tagging improving pattern applicable predicted quality initial model similarity tagged training corpus corpus tagged heuristic deciding use estimation effective manner given conclusion agreement merialdo greater detail contribution different part model",
        "speech tagging process assigning grammatical category individual word corpus approach make use statistical technique called hidden markov model hmm model defined collection parameter transition probability express probability tag follows preceding second order model lexical probability giving probability word given tag regard word tag text tag non probability hypothesised word probable sequence tag given sequence word determined probability algorithm known backward viterbi algorithm assigns probability tag word viterbi prune tag chosen probability lower one competing hypothesis corresponding gain computational efficiency introduction algorithm cutting lucid description sharman principal source parameter model tagged corpus prepared human annotator available transition lexical probability estimated frequency pair tag tag associated word procedure called baum welch estimation untagged corpus passed algorithm initial model resulting probability determine new value lexical transition probability iterating algorithm corpus parameter model converge value optimal given text degree convergence measured perplexity measure sum plog hypothesis probability give estimate degree disorder model algorithm described cutting sharman mathematical justification found huang major use hmms speech tagging claw garside availability large corpus fast computer recent resurgence interest number variation alternative viterbi algorithm tried church brill marcus brill derose kupiec effective tagger based pure hmm developed xerox cutting important aspect tagger good accuracy minimal tagged training data accuracy correct assignment tag word token compared human annotator quoted word corpus xerox tagger attempt avoid need hand tagged training corpus possible approximate model constructed hand improved estimation untagged training corpus example iteration sufficient initial model set transition tag lexicon favoured having higher initial probability model improved keeping number parameter model assist low frequency item lexicon grouped equivalence class word given equivalence class tag lexical probability word looked data common estimation word class count estimation result xerox experiment appear encouraging tagged corpus hand labour intensive error prone semi automatic approach marcus good",
        "thing reduce human involvement possible careful examination experiment needed place cutting compare success rate work achieved hand tagged training text estimation unclear initial biasing contributes success rate significant human intervention needed provide biasing advantage automatic training weaker intervention needed new text domain kind biasing cutting describe reflects linguistic insight combined understanding prediction tagger expected one aim paper examine role training play tagging process experimental evaluation accuracy tagger varies initial condition result suggest unconstrained initial model produce good quality result trained hand tagged corpus better approach based estimation training come different source second experiment show different pattern estimation pattern vary broad characterisation initial condition outcome experiment point heuristic making effective use training estimation direction research similar described carried merialdo similar conclusion discus work principal contribution work separate effect lexical transition parameter model result vary different degree similarity training test data",
        "experiment conducted tagger written cambridge university computer laboratory sharp laboratory tagger implement viterbi algorithm training hand tagged corpus model estimated counting number transition tag tag total occurrence tag total occurrence word tag transition probability tag tag estimated lexical probability estimation formula past example claw garside normalises lexical probability total frequency word tag baum welch estimation formula suggests approach described appropriate confirmed greater tagging accuracy transition seen training corpus given small non probability lexicon list word tag seen training corpus probability word found lexicon open class tag hypothesised equal probability word added lexicon end iteration estimation probability hypothesis diverge uniform measure accuracy tagger compare chosen tag provided human annotator method quoting accuracy literature common proportion word token receiving correct tag better measure proportion ambiguous word given correct tag ambiguous mean tag hypothesised figure look impressive give better measure tagger factor trivial assignment tag non ambiguous word corpus fraction word ambiguous accuracy ambiguous word overall accuracy recovered accuracy figure quoted ambiguous word training test corpus drawn lob corpus penn treebank hand tagging corpus different example lob tagset tag penn treebank tagset general pattern result presented vary corpus tagset",
        "experiment concerned effect initial condition accuracy baum welch estimation model trained hand tagged corpus manner described degraded way simulate effect poorer training follows un degraded lexical probability calculated lexical probability ordered frequent tag highest lexical probability absolute value unreliable lexical probability proportional overall tag frequency independent actual occurrence word training corpus lexical probability value lexicon contains information possible tag word un degraded transition probability calculated transition probability value expect achieve printed dictionary listing part speech order frequency training represented case xerox experiment cutting correspond initial biasing probability test corpus constructed lob corpus lob b lob l lob b g part inclusive lob b j part inclusive lob b j train model lob b lob l lob b g passed iteration algorithm untagged data case best accuracy ambiguous word usual algorithm noted additional test tried assigning probable tag lexicon ignoring tag tag transition result summarised table corpus denotes frequent tag test example figure relate overall accuracy lob b contains ambiguous token respect lexicon lob b j overall accuracy case general pattern result similar test corpus difference interest case better lob l case particular better case possible explanation case test data overlap training data good quality lexicon influence interesting reason unclear result corpus suggests significant follow experiment confirm result corpus penn treebank equivalence class ensure lexical entry total relative frequency larger corpus specific accuracy different test overall pattern remained suggesting artifact tagset detail text observation result follows test poor performance accuracy good achieved picking frequent tag course implies lexicon quality follows baum welch estimation effective technique initial data biasing transition case lexical probability case necessary training hand tagged corpus case",
        "experiment apparent baum welch estimation decrease accuracy iteration progress second experiment conducted decide appropriate use baum welch estimation pattern behaviour classicala general trend rising accuracy iteration fall accuracy local indicates model converging optimum better starting point maximumhighest accuracy iteration falling case initial model better quality achieve converge optimum notion optimality respect hmm linguistic judgement correct tagging maximumrising accuracy small number iteration falling initial maximum example behaviour shown figure value accuracy test condition unimportant want general pattern second experiment aim trying discover pattern applies circumstance order help decide train model expected pattern initial maximum use early maximum halt process iteration classical halt process standard way comparing perplexity successive model test conducted similar manner experiment building lexicon transition hand tagged training corpus applying test corpus varying degree degradation different degree degradation degradation degradation lexicon degradation transition selected test corpus varying degree similarity training corpus text text similar domain text different test conducted combination degradation similarity different corpus penn treebank ranging size word word estimation allowed run iteration result appear table showing best accuracy achieved ambiguous word iteration occurred pattern estimation initial maximum early maximum classical pattern summarised table entry table showing pattern test given condition test gave early peak graph accuracy number iteration pattern classical early maximum variation reading example similar case draw general conclusion pattern obtained different sort data lexicon degraded pattern classical good lexicon degraded transition test corpus differing training corpus pattern tends maximum test corpus similar model pattern initial maximum examining accuracy table case initial maximum early maximum accuracy tends higher classical behaviour likely going model converging similar quality case pattern classical convergence start lower quality model improves case start higher quality deteriorates case maximum iteration accuracy improving correspond creation entry unknown word fine tuning one",
        "observation previous section propose following guideline train hmm use tagging hand tagged training corpus available use test training corpus identical use estimation use small number iteration training corpus available lexicon relative frequency data available use estimation small number iteration training corpus lexicon available use estimation standard convergence test perplexity lexicon initial biasing transition needed good result obtained result presented merialdo describes experiment compare effect training hand tagged corpus baum welch algorithm initial condition experiment estimation gave decrease accuracy starting point derived significant hand tagged text addition merialdo highlight point estimation starting word hand tagged text show early maximum behaviour conclusion tagger trained hand tagged text possible begin applying estimation untagged text step taken work pattern estimation behaviour differing guideline use obtain good starting point hand tagged corpus available small lexicon transition biased useful heuristic practical point view step look automatic way predicting accuracy tagging process given corpus model preliminary experiment measure perplexity average probability hypothesis indication convergence estimation show strong correlation accuracy needed similarity measure model corpus tagged model model obtained training output corpus tagger hand tagged corpus preliminary experiment measure kullback liebler distance initial new model showed good prediction accuracy end turn way making prediction source information extrinsic model corpus",
        "online text available increasing volume increasing number language growing need robust processing technique analyze text expensive time consuming adaptation new domain genre need motivates research automatic text processing rely general principle linguistics computation depend knowledge individual word paper describe experiment automatic derivation knowledge necessary speech tagging speech tagging interest number application example access text data base kupiec robust parsing abney general parsing marcken charniak goal find unsupervised method tagging relies general distributional property text property invariant language sublanguages proposed algorithm successful grammatical category automatic tagging possible demand accuracy modest following section discus related work describe learning procedure evaluate brown corpus francis kucera",
        "simplest speech tagger bigram trigram model church charniak require large tagged training text based tagging introduced brill requires hand tagged text training pretagged text necessary hidden markov model cutting kupiec lexicon needed specifies possible part speech word marcus shown effort necessary construct speech lexicon reduced combining learning procedure partial speech categorization elicited informant present paper concerned tagging language sublanguages priori knowledge grammatical category available situation occurs practice brill marcus researcher worked learning grammatical property word train connectionist net predict word process generates internal representation reflect grammatical category try infer grammatical category bigram statistic chater finch use vector model word clustered according similarity close neighbor corpus ney present probabilistic model entropy maximization relies immediate neighbor word corpus applies factor analysis collocation target word certain right immediate neighbor approach common classify word individual occurrence widespread speech ambiguity word problematic word plant categorized us verb noun categorization considered meaningful infinitive marker distinguished homophonous preposition previous paper schuetze trained neural network disambiguate speech context information word categorized scheme fails case soldier come soldier come context identical information lexical item question needed combination context correct classification paper compare tagging algorithm based classifying word type based classifying word context",
        "start constructing representation syntactic behavior word respect left right context working hypothesis syntactic behavior reflected co occurrence pattern measure similarity word respect syntactic behavior left degree share neighbor left count neighbor assembled vector dimension neighbor cosine employed measure similarity assign value word share neighbor share refer vector left neighbor word left context vector vector right neighbor right context vector unreduced context vector experiment described entry corresponding frequent word brown corpus basic idea measuring distributional similarity term shared neighbor modified sparseness data infrequent adjective happen modify different noun corpus right similarity according cosine measure undesirable high frequency word simple vector model yield misleading similarity measurement case point article share right neighbor consonant vowel similar respect right syntactic context despite lack common right neighbor solution problem application singular value decomposition represent left vector word corpus matrix row word left neighbor represented column possible neighbor approximate row column vector low dimensional space detail svd decomposes matrix matrix left vector case matrix diagonal k k matrix contains singular value descending order ith singular value interpreted indicating strength ith principal component orthonormal matrix approximate row column restricting matrix column principal component obtains matrix product square approximation matrix rank chose reduction dimensional space svd described paper address problem generalization sparseness broad stable generalization represented dimension large value retained dimensionality reduction contrast dimension corresponding small singular value represent idiosyncrasy phonological constraint usage dropped gain efficiency manipulate smaller vector reduced dimension svdpack compute singular value decomposition described paper berry show nearest neighbor word ordered according closeness head word dimensionality reduction highest similarity according left right context listed clear difference nearest neighbor space right context neighbor contain verb preposition verb govern noun phrase right left context neighborhood reflects fact prepositional phrase position adverb making left context similar",
        "left context neighbor word similar type noun phrase subject position auxiliary right context neighbor infinitive complement adjective like similar respect left context different generalization preposition transitive verb similar identical way govern noun phrase lost left right property word lumped representation example demonstrate importance representing generalization left right context left right context vector basis different tag induction experiment described detail induction based word type onlyinduction based word type contextinduction based word type context restricted natural contextsinduction based word type context generalized left right context vector context vector word characterize distribution neighboring word left right concatenation left right context vector serve representation word distributional behavior finch chater schuetze formed concatenated vector word surface form brown corpus use raw dimensional context vector apply matrix word dimensional context vector obtained dimensional reduced vector svd clustered class fast clustering algorithm buckshot cutting group average agglomeration applied sample classification constitutes baseline performance distributional speech tagging occurrence word assigned class pointed procedure problematic ambiguous word order exploit contextual information classification token use context vector word occurring token occurrence word represented concatenation context vector right context vector preceding word left context vector right context vector left context vector following word motivation word syntactic role depends syntactic property neighbor potential entering syntactic relationship neighbor property context consider right context vector preceding word left context vector following word represent contextual information important categorization example disambiguation work work important fact expects noun phrase left important right context vector contribute disambiguation immediate neighbor crucial categorization simplification result presented work svd applied address problem sparseness generalization selected word triplet corpus formed concatenation context vector described singular value decomposition resulting matrix defines mapping dimensional space concatenated context vector dimensional reduced space tag set induced clustering reduced vector selected occurrence class tag defined centroid corresponding class sum member tagging occurrence word proceeds retrieving relevant context vector right context vector",
        "previous word left context vector following word context vector concatenating component vector mapping vector dimension computing correlation cluster centroid assigning occurrence closest cluster procedure applied token brown corpus method distributional tagging successful fails token neighbor punctuation mark context vector punctuation mark contribute little information syntactic categorization grammatical dependency word punctuation mark contrast strong dependency neighboring word reason second induction basis word type context performed token informative context punctuation mark token rare word neighbor included rare word occurrence excluded similar reason word occurs fewer time left right context vector capture little information syntactic categorization experiment natural context selected processed svd clustered class classification applied natural context brown corpus context vector capture information distributional interaction frequent word gain accuracy tag induction information word way let right context vector record class left context vector occur right word rationale word similar left context characterize word right similar way example similar left context characterize right context firefighter containing inflected verb form having separate entry right context vector like word like characterized generalized entry inflected verb form occurs right proposal implemented applying singular value decomposition matrix left context vector clustering resulting context vector class generalized right context vector word formed counting word class occurred right entry count number time word class occurs right corpus opposed number time word frequency rank occurs right left context vector derived analogous procedure word based right context vector information left kept separate computation differs previous approach finch chater schuetze left right context vector word concatenated vector fewer different type syntactic context type syntactic category example transitive verb preposition belong different syntactic category right context identical require noun phrase generalization exploited left right context treated argument step derivation word frequent word left right neighbor vector word based scheme class based scheme make likely meaningful representation formed word",
        "context vector word characterize distribution neighboring word left right concatenation left right context vector serve representation word distributional behavior finch chater schuetze formed concatenated vector word surface form brown corpus use raw dimensional context vector apply matrix word dimensional context vector obtained dimensional reduced vector svd clustered class fast clustering algorithm buckshot cutting group average agglomeration applied sample classification constitutes baseline performance distributional speech tagging occurrence word assigned class pointed procedure problematic ambiguous word",
        "order exploit contextual information classification token use context vector word occurring token occurrence word represented concatenation context vector right context vector preceding word left context vector right context vector left context vector following word motivation word syntactic role depends syntactic property neighbor potential entering syntactic relationship neighbor property context consider right context vector preceding word left context vector following word represent contextual information important categorization example disambiguation work work important fact expects noun phrase left important right context vector contribute disambiguation immediate neighbor crucial categorization simplification result presented work svd applied address problem sparseness generalization selected word triplet corpus formed concatenation context vector described singular value decomposition resulting matrix defines mapping dimensional space concatenated context vector dimensional reduced space tag set induced clustering reduced vector selected occurrence class tag defined centroid corresponding class sum member tagging occurrence word proceeds retrieving relevant context vector right context vector previous word left context vector following word context vector concatenating component vector mapping vector dimension computing correlation cluster centroid assigning occurrence closest cluster procedure applied token brown corpus method distributional tagging successful fails token neighbor punctuation mark context vector punctuation mark contribute little information syntactic categorization grammatical dependency word punctuation mark contrast strong dependency neighboring word reason second induction basis word type context performed token informative context punctuation mark token rare word neighbor included rare word occurrence excluded similar reason word occurs fewer time left right context vector capture little information syntactic categorization experiment natural context selected processed svd clustered class classification applied natural context brown corpus",
        "context vector capture information distributional interaction frequent word gain accuracy tag induction information word way let right context vector record class left context vector occur right word rationale word similar left context characterize word right similar way example similar left context characterize right context firefighter containing inflected verb form having separate entry right context vector like word like characterized generalized entry inflected verb form occurs right proposal implemented applying singular value decomposition matrix left context vector clustering resulting context vector class generalized right context vector word formed counting word class occurred right entry count number time word class occurs right corpus opposed number time word frequency rank occurs right left context vector derived analogous procedure word based right context vector information left kept separate computation differs previous approach finch chater schuetze left right context vector word concatenated vector fewer different type syntactic context type syntactic category example transitive verb preposition belong different syntactic category right context identical require noun phrase generalization exploited left right context treated argument step derivation word frequent word left right neighbor vector word based scheme class based scheme make likely meaningful representation formed word vocabulary generalized context vector input tag induction procedure described word based context vector word triplet selected corpus encoded dimensional vector consisting generalized context vector decomposed singular value decomposition clustered class resulting classification applied token brown corpus",
        "result experiment evaluated forming class tag penn treebank shown table experiment showed distributional method distinguish adnominal predicative us adjective black cat cat black tag adn introduced us adjective noun participle modifier tag prd stand predicative us adjective penn treebank par brown corpus determine token function modifier mark special symbol interjection foreign word tag fewer instance excluded evaluation present result word type based induction induction based word type context tag table list frequency corpus frequency number induced tag assigned class number time occurrence labeled belonging correct number time token different tag miscategorized instance incorrect precision recall categorization number correct token divided sum correct incorrect token number correct token divided total number token column column give van rijsbergen measure computes aggregate score precision recall van rijsbergen chose equal weight precision recall clear table incorporating context improves performance score increase tag average improvement tag thought describing word class wide range heterogeneous syntactic function cardinal particular context quantificational adnominal us bare date age jan gave age enumeration light surprising word type method better cardinal show performance generalized context vector better word based context vector number tag better worse performance conclude certainty generalized context vector induce tag higher quality frequent word capture relevant distributional information additional information frequent word available generalized vector small effect look result natural context containing punctuation mark rare word better evaluation context indicating low quality distributional information punctuation mark rare word difficulty successful tag induction natural context performance varies good preposition determiner pronoun conjunction infinitive marker modal possessive marker induction fails cardinal reason mentioned ing form participle gerund difficult exhibit verbal nominal property occur wide variety different context part speech typical frequent context worrying tag assigned high number cluster adn closer look reveals cluster embody finer distinction example noun cluster head larger noun phrase noun cluster fledged np member class function subject consists proper noun",
        "pair triple cluster collapsed linguistic ground separated distributional criterion linguistic correlate analysis divergence classification assigned tag revealed main source error rare word rare syntactic phenomenon indistinguishable distribution non local dependency word difficult lack distributional evidence example tie verb time occurrence corpus occurrence miscategorized context vector provide evidence verbal use syntactic construction pose related problem instance justify creation separate cluster example verb taking bare infinitive classified adverb rare phenomenon provide strong distributional evidence dare speak legislation help remove case tag vbn prd participle predicative adjective demonstrates difficulty word class indistinguishable distribution distributional clue distinguishing vbn prd complement common tag class created vbn prd distinguished part speech understanding necessary distinguish state described phrase form adjective process described phrase form participle method fails local dependency categorization non local dependency informative example adverb hester dean conjunction add united state policy similar immediate neighbor comma decision consider immediate neighbor responsible type error taking wider context account disambiguate part speech question",
        "avenue future research interested pursuing planning apply algorithm untagged language rich morphology difficult english fewer token type data base categorization decision error analysis suggests considering non local dependency improve result induced characterized local dependency input procedure learn phrase structure brill marcus finch phrase constraint incorporated distributional tagger characterize non local dependency procedure induces hard speech classification occurrence context occurrence assigned category mean accepted classification adequate synchronic ross diachronic tabor evidence suggesting word us inherit property prototypical syntactic category example fun fun thing property noun adjective superlative funnest possible planning explore soft classification algorithm account phenomenon",
        "collocation present specific problem translation human automatic context construction heavy smoker english attempt translate french german find literal translation heavy yield wrong result concept expressed adjective excessive translated grand large french stark strong german observe sense adjective stark grand heavy equivalent collocational context course case context grande boite starke schachtel heavy box adjective viewed equivalent adjective literal translation share meaning property collocational context specify special equivalence machine translation dictionary answer lie addressing concept underlies union adjective noun case intensification establish single meaning representation adjective viewed interlingual pivot translation studied computational linguist different context instance substantial body paper extraction co occurring word corpus statistical method choueka church hank smadja list author focus technique providing material processing task word sense disambiguation information retrieval natural language generation use collocation different application discussed author mcroy pustejovsky smadja mckeown collocation considered useful problem certain application generation nirenburg machine translation heid raab theoretical point view abeille schabes krenn erbach appear concerned investigating lexical function lf zolkovsky candidate interlingual device translation adjectival verbal collocates work related research heid raab respect extension suggestion work differs scope exploration direction",
        "use lexical function interlingual representation respect original interpretation transferred context meaning text theory different theoretical setting embedded concept hpsg like grammar theory section review operation consider feature treatment wanted preserve imported hpsg framework explanatory combinatory dictionary ecd expression une ferme intention rsistance acharne argument poids bruit infernal donner une leon faire pa commetre crime described lexical combinatorics zone expression moins figes called collocation considered consist part base collocate example noun base adjective verb collocates idea adjective collocates verb collocates share important meaning component paraphrasable intense fact adjective verb interchangeable restricted meaning accompanying noun coded dictionary lexical function case magn oper article ecd describes called lexeme word specific reading lexical combinatorics zone find list lexical function relevant particular lexeme lexical function followed lexeme result value function applied head word idea combination argument value function form collocation terminology argument corresponds base value collocate following feature representation important function represent important syntactico semantic relation base collocate restricted combinatorial potential collocate lexeme accounted listing base occur second characteristic point collocational restriction seen lexical idiosyncratic collocation listed aspect collocation deal relation collocate lexeme occurring counterpart lexeme differ respect literal variant sharing property deal including ecd entry free variant putting collocate specific information entry base result lexical function entry collocate result taking entry free variant overwriting information provided base aspect analysis wanted encode hpsg following base collocate relation lexicon level lexical function situated collocate information free variant entry provided straightforward solution problem problem taken ecd architecture creating dedicated collocates field entry base contains relevant collocates second problem concerned obvious place lexical function semantic representation provided hpsg reason lf deep syntax level model level oriented meaning reason level appropriate transfer translation",
        "want use lexical function transfer contrast ecd meaning collocate represented lexical function following example entry criticism encoding strong collocate use abbreviation feature path ecd base contains specific zone collocates listed case feature colls set lexical entry value collocate subentry bear value lexical function semantics field representation lexical function chosen real semantic value collocate read feature structure specifying semantics strong collocate predicate collocate subentry provides partial information fact provides information specific occurrence strong combination criticism case semantics given assume lexicon contains super entry provides information shared different occurrence strong entry variable strong point course architecture try avoid redundant specification information possible instance assumes mechanism default unification strong refer entry describing strong ordinary use value particular collocational strong overwrite value provided ordinary entry proposal specified way get lexical entry base collocate representation collocational expression hpsg description complex expression constrained principle assume collocation subject constraint ordinary rule combination combining adjective noun instance account property collocational combination left typical collocational restriction need accounted added principle say construction analysed collocation indicated type collocation head adjunct structure head complement structure specific restriction holding head adjunct head complement consider case illustrated heavy smoker example adjunct daughter contain adjective collocate collocational construction collocate adjunct licensed noun head daughter implemented requiring collocates field colls head daughter contains reference lexical entry compatible adjunct daughter literal reading expression heavy smoker phrase analysed collocation principle apply",
        "explanatory combinatory dictionary ecd expression une ferme intention rsistance acharne argument poids bruit infernal donner une leon faire pa commetre crime described lexical combinatorics zone expression moins figes called collocation considered consist part base collocate example noun base adjective verb collocates idea adjective collocates verb collocates share important meaning component paraphrasable intense fact adjective verb interchangeable restricted meaning accompanying noun coded dictionary lexical function case magn oper article ecd describes called lexeme word specific reading lexical combinatorics zone find list lexical function relevant particular lexeme lexical function followed lexeme result value function applied head word idea combination argument value function form collocation terminology argument corresponds base value collocate following feature representation important function represent important syntactico semantic relation base collocate restricted combinatorial potential collocate lexeme accounted listing base occur second characteristic point collocational restriction seen lexical idiosyncratic collocation listed aspect collocation deal relation collocate lexeme occurring counterpart lexeme differ respect literal variant sharing property deal including ecd entry free variant putting collocate specific information entry base result lexical function entry collocate result taking entry free variant overwriting information provided base",
        "aspect analysis wanted encode hpsg following base collocate relation lexicon level lexical function situated collocate information free variant entry provided straightforward solution problem problem taken ecd architecture creating dedicated collocates field entry base contains relevant collocates second problem concerned obvious place lexical function semantic representation provided hpsg reason lf deep syntax level model level oriented meaning reason level appropriate transfer translation want use lexical function transfer contrast ecd meaning collocate represented lexical function following example entry criticism encoding strong collocate use abbreviation feature path ecd base contains specific zone collocates listed case feature colls set lexical entry value collocate subentry bear value lexical function semantics field representation lexical function chosen real semantic value collocate read feature structure specifying semantics strong collocate predicate collocate subentry provides partial information fact provides information specific occurrence strong combination criticism case semantics given assume lexicon contains super entry provides information shared different occurrence strong entry variable strong point course architecture try avoid redundant specification information possible instance assumes mechanism default unification strong refer entry describing strong ordinary use value particular collocational strong overwrite value provided ordinary entry proposal specified way get lexical entry base collocate representation collocational expression hpsg description complex expression constrained principle assume collocation subject constraint ordinary rule combination combining adjective noun instance account property collocational combination left typical collocational restriction need accounted added principle say construction analysed collocation indicated type collocation head adjunct structure head complement structure specific restriction holding head adjunct head complement consider case illustrated heavy smoker example adjunct daughter contain adjective collocate collocational construction collocate adjunct licensed noun head daughter implemented requiring collocates field colls head daughter contains reference lexical entry compatible adjunct daughter literal reading expression heavy smoker",
        "specified way get lexical entry base collocate representation collocational expression hpsg description complex expression constrained principle assume collocation subject constraint ordinary rule combination combining adjective noun instance account property collocational combination left typical collocational restriction need accounted added principle say construction analysed collocation indicated type collocation head adjunct structure head complement structure specific restriction holding head adjunct head complement consider case illustrated heavy smoker example adjunct daughter contain adjective collocate collocational construction collocate adjunct licensed noun head daughter implemented requiring collocates field colls head daughter contains reference lexical entry compatible adjunct daughter literal reading expression heavy smoker phrase analysed collocation principle apply",
        "project tried investigate use lexical function interlingual device shared semantic representation collocation language pair typing collocation function open way treatment collocation given language module substantial reduction number collocation handled multilingual transfer dictionary existence collocation function established analysis information generate correct translation target language illustrate english analysis module analyse transfer module map synthesised french module example point translation strategy mixture transfer interlingua base transferred representation collocate shared source target representation treatment collocation rest assumption number lexical function lexical function assigned significant number collocation realise lexical function lexical function restricted particular language etc following paragraph present outline translation process discus problem follow approach propose way solve assumed starting point transfer semantic representation phrase semantic representation input transfer implies relate semantic value word phrase purpose satisfying semantics collocates orthography word use lexical function particular realisation collocate particular language state relation semantic representation source language target language semantic relation phrase heavy smoker french counterpart explicit following bilingual sign lexicon contain bilingual sign possible value reln translating heavy smoker grand fumeur need obvious entry smoker fumeur entry interlingual status lexical function self evident occurrence magn left intact transfer generation component assigns monolingual lexical entry function certain nuance meaning different syntactic realization discus problem raised abstraction section important problem stem interpretation lf implied use interlingua meaning collocate way reduces meaning implied lexical function interpretation trouble free assume lf deliver unique value case contrary observed example attested corpus range adverbial construction possible verbal head oppose function magn appropriate descriptor case adverb function typical context adverb denotes meaning aspect imprecision lf mean mean distinguishing intensifier possible context given keyword sufficient information choose appropriate translation multiple possibility exist target language important question dramatic loss translation quality addressing issue overgenerality introduces sub superscript lexical function enhancing precision making sensitive meaning aspect lexical item",
        "operate intended meaning precise likely imply unary mapping argument value subscript reference particular semantic component keyword introduction device account lf demonstrates need precision fact necessary address semantic aspect lexeme standing co occurrence relation fact asserted anick pustejovsky heid raab collocational system predictable lexical semantics noun attempt explore notion investigated approach nominal semantics known qualia structure pustejovsky considered complement notion improve descriptive power promising avenue occur postulation subscript based qualia role assuming relevant aspect noun semantics application lf semantic qualia structure monolithic lexeme bon delivering evaluative qualifier standard expression praise approval imagine application function constitutive agentive role noun lecture deliver case idea precision lexical function enhanced appealing semantic facet argument issue raised concern translation collocation non collocational construction maintain consistent interlingual approach translation case extend based approach consider case briefly linguistic analysis reveals case nominal based collocational construct realised compound germanic language bunch key sleutelbos possible account phenomenon developed concept merged lf zolkovsky lf intended case value lexeme exists appears reduce merge meaning specified argument single lexicalised form projecting syntagmatic unit argue case compound formation process accounted compound embodies concept mediated argument lexeme allow compound delivered value merged sleutelbos observation useful context assume effect mapping merged unmerged lf capture correspondence distinct structural realisation concept way emulate mapping use lexical paraphrasing rule instance conceive lexical paraphrasing rule follows assume monolingual english lexicon assign collocate bunch mult value keyword key dutch lexical entry sleutel instantiate sleutelbos value merged mult use paraphrasing rule effect mapping arrive interlingual approach translation example despite structural mismatch example exist productive morphological process affixation lead lexicalisation language concept exist syntagmatic construct suggest use merged lf corresponding mapping lexical paraphrasing rule possible translation strategy case",
        "assumed starting point transfer semantic representation phrase semantic representation input transfer implies relate semantic value word phrase purpose satisfying semantics collocates orthography word use lexical function particular realisation collocate particular language state relation semantic representation source language target language semantic relation phrase heavy smoker french counterpart explicit following bilingual sign lexicon contain bilingual sign possible value reln translating heavy smoker grand fumeur need obvious entry smoker fumeur entry interlingual status lexical function self evident occurrence magn left intact transfer generation component assigns monolingual lexical entry",
        "lexical function certain nuance meaning different syntactic realization discus problem raised abstraction section important problem stem interpretation lf implied use interlingua meaning collocate way reduces meaning implied lexical function interpretation trouble free assume lf deliver unique value case contrary observed example attested corpus range adverbial construction possible verbal head oppose function magn appropriate descriptor case adverb function typical context adverb denotes meaning aspect imprecision lf mean mean distinguishing intensifier possible context given keyword sufficient information choose appropriate translation multiple possibility exist target language important question dramatic loss translation quality addressing issue overgenerality introduces sub superscript lexical function enhancing precision making sensitive meaning aspect lexical item operate intended meaning precise likely imply unary mapping argument value subscript reference particular semantic component keyword introduction device account lf demonstrates need precision fact necessary address semantic aspect lexeme standing co occurrence relation fact asserted anick pustejovsky heid raab collocational system predictable lexical semantics noun attempt explore notion investigated approach nominal semantics known qualia structure pustejovsky considered complement notion improve descriptive power promising avenue occur postulation subscript based qualia role assuming relevant aspect noun semantics application lf semantic qualia structure monolithic lexeme bon delivering evaluative qualifier standard expression praise approval imagine application function constitutive agentive role noun lecture deliver case idea precision lexical function enhanced appealing semantic facet argument issue raised concern translation collocation non collocational construction maintain consistent interlingual approach translation case extend based approach consider case briefly linguistic analysis reveals case nominal based collocational construct realised compound germanic language bunch key sleutelbos possible account phenomenon developed concept merged lf zolkovsky lf intended case value lexeme exists appears reduce merge meaning specified argument single lexicalised form projecting syntagmatic unit argue case compound formation process accounted compound embodies concept mediated argument lexeme allow compound delivered value merged",
        "sleutelbos observation useful context assume effect mapping merged unmerged lf capture correspondence distinct structural realisation concept way emulate mapping use lexical paraphrasing rule instance conceive lexical paraphrasing rule follows assume monolingual english lexicon assign collocate bunch mult value keyword key dutch lexical entry sleutel instantiate sleutelbos value merged mult use paraphrasing rule effect mapping arrive interlingual approach translation example despite structural mismatch example exist productive morphological process affixation lead lexicalisation language concept exist syntagmatic construct suggest use merged lf corresponding mapping lexical paraphrasing rule possible translation strategy case",
        "important problem stem interpretation lf implied use interlingua meaning collocate way reduces meaning implied lexical function interpretation trouble free assume lf deliver unique value case contrary observed example attested corpus range adverbial construction possible verbal head oppose function magn appropriate descriptor case adverb function typical context adverb denotes meaning aspect imprecision lf mean mean distinguishing intensifier possible context given keyword sufficient information choose appropriate translation multiple possibility exist target language important question dramatic loss translation quality addressing issue overgenerality introduces sub superscript lexical function enhancing precision making sensitive meaning aspect lexical item operate intended meaning precise likely imply unary mapping argument value subscript reference particular semantic component keyword introduction device account lf demonstrates need precision fact necessary address semantic aspect lexeme standing co occurrence relation fact asserted anick pustejovsky heid raab collocational system predictable lexical semantics noun attempt explore notion investigated approach nominal semantics known qualia structure pustejovsky considered complement notion improve descriptive power promising avenue occur postulation subscript based qualia role assuming relevant aspect noun semantics application lf semantic qualia structure monolithic lexeme bon delivering evaluative qualifier standard expression praise approval imagine application function constitutive agentive role noun lecture deliver case idea precision lexical function enhanced appealing semantic facet argument",
        "issue raised concern translation collocation non collocational construction maintain consistent interlingual approach translation case extend based approach consider case briefly linguistic analysis reveals case nominal based collocational construct realised compound germanic language bunch key sleutelbos possible account phenomenon developed concept merged lf zolkovsky lf intended case value lexeme exists appears reduce merge meaning specified argument single lexicalised form projecting syntagmatic unit argue case compound formation process accounted compound embodies concept mediated argument lexeme allow compound delivered value merged sleutelbos observation useful context assume effect mapping merged unmerged lf capture correspondence distinct structural realisation concept way emulate mapping use lexical paraphrasing rule instance conceive lexical paraphrasing rule follows assume monolingual english lexicon assign collocate bunch mult value keyword key dutch lexical entry sleutel instantiate sleutelbos value merged mult use paraphrasing rule effect mapping arrive interlingual approach translation example despite structural mismatch example exist productive morphological process affixation lead lexicalisation language concept exist syntagmatic construct suggest use merged lf corresponding mapping lexical paraphrasing rule possible translation strategy case",
        "paper discussed lexicographical concept lexical function introduced describe collocation interlingual device machine translation structure shown essential ecd analysis embedded lexicon grammar unification based theory language use lexical function interlingua assumes relevant aspect meaning collocate captured determines accuracy translation impoverished generalised nature basic lf suggested way lf enriched lexical semantic information improve translation quality interlingua level reflects common expression form translational equivalent abstract specific syntactic realisation collocation translate non collocation provide way represent expression lexical function provided illustration proceed case",
        "conversation person refer object known participant particular situation arises giving direction example hearer understand reference reach building recognize intended referent reference sort achieved collaboration conversants case speaker goal having hearer know identify object speaker attempt achieve goal building description object belief hearer ability identify possible hearer need confident description adequate mean identifying referent inevitable difference belief world hearer confident speaker hearer collaborate new referring expression hearer belief adequate seen following portion telephone conversation recorded psathas dialogue speaker confident able identify intersection lowell street suggests intersection marked reply elaboration initial expression find confident accepts reference type reference different type studied researcher assumed agent mutual knowledge referent appelt appelt kronfeld clark wilkes gibbs heeman hirst searle copresent referent heeman hirst cohen referent focus attention reiter dale theory speaker intention hearer know referent identify type reference wish model rely assumption draw theory base model work clark wilkes gibbs heeman hirst modeled second people collaborate reference object mutual knowledge discus model describe",
        "fundamental experiment clark wilkes gibbs demonstrated conversants use set collaborative procedure establish mutual belief hearer understood reference experiment subject given set hard describe tangram figure kept hidden subject required subject rearrange set match ordering set conversation subject obliged collaborate constructing description figure allow identified example look angel stick wilkes gibbs developed following process model explain finding initiate process speaker present initial version referring expression speaker pass judgment accept reject postpone decision reject postpones expression refashioned refashionings accomplished main way repairing expression correcting speech error expanding expression adding qualification replacing expression new qualification judgment refashioning pair operates current referring expression replacing new process continues expression kept participant common ground accepted excerpt clark wilkes gibbs data illustrates rejection line replacement line acceptance line heeman hirst rendered clark wilkes gibbs model casting planning paradigm model cover initiator referring action recipient try understand reference model initiator goal having recipient identify referent construct referring plan given set belief recipient belief result initiator plan set surface speech action hearing action recipient try infer plan order understand reference referring expression represented plan derivation unsuccessful referring expression invalid plan repair agent collaborate agent infer plan invalid agent view pollack evaluation process attempt find instantiation variable constraint satisfied mental action executable respect hearer belief speaker belief recipient find initial referring expression plan invalid agent collaborate repair hirst plan repair technique refashion expression discourse plan meta plan communicate change collaborative dialogue modeled term evolution referring plan agent communicate understood plan referring plan constrains choice referent construct instance reject plan postpone plan resulting surface speech action s reject s postpone agent refashion referring expression plan context judgment replacing action replace plan adding new action expand plan result plan surface speech action s action model play role initiator recipient perform plan construction inference copy model converse acting speaker hearer hearer copy system performs plan inference set surface speech action observes update",
        "crucial assumption clark wilkes gibbs work heeman hirst model recipient initial referring expression knowledge referent question clark wilkes gibbs experiment example tangram figure word hearer understand referring expression content describes object know agent use criterion understand reference building example heard building criterion base understanding basis model hearer accept referring expression plan ifthe plan contains description useful making identification plan hearer execute identify referent andthe hearer confident identification plan adequate condition described appelt important success referring action depends hearer formulating useful identification plan referring expression plan identification plan mental action plan encode useful description second condition hold hearer believe identification plan good identify referent visible involves giving information salient attribute referent model agent associate numeric confidence value attribute referring expression composing computes level confidence adequacy complete referring expression plan interpreted ranging low confidence high confidence present composition function simple addition envision complex system compute confidence algebra confidence non numeric system overall confidence value exceeds set value agent confidence threshold agent belief plan adequate agent initiator belief able understand reference agent belief understood reference confidence value attribute equivalent salience context referring expression purpose direction giving visual prominence involve identifiability familiarity functional importance devlin lynch approach encode salient property static hierarchy davis reiter dale salience depend context surrounding referent example height tall building salient surrounded tall building computation complex adopted middle ground simple context independent approach blown contextual analysis middle ground involves taking type object account choosing attribute landmark relate example height architectural style salient feature describing building describing intersection having sign traffic light important approach allows encode salience hierarchy dependent referent show example simple salience hierarchy agent hierarchy set partial ordering attribute represented lambda expression indexed object type table confidence value architectural style describe building confidence value tall building attribute salient architectural style row describing intersection follow agent belief salience difference",
        "agent us salience hierarchy related purpose determine salient particular situation second determine adequacy description hierarchy accessed plan construction plan inference plan construction hierarchy constructing initial referring expression plan elaborating inadequate plan allowing agent choose salient property referent agent construct initial referring expression plan way heeman hirst system action intermediate plan referring expression plan allow speaker choose salient attribute chosen constraint surface speech action sure speaker belief attribute true mental action intermediate plan add confidence value attribute final constraint make sure sum exceeds agent confidence threshold referring plan valid describe unique object adequate respect speaker belief mean attribute required unique description necessary example construct reference building example speaker consulted salience hierarchy table determined architectural style salient described building looking single attribute exceed confidence threshold plan inference salience hierarchy evaluating recognized plan mental action intermediate plan determine confidence value attribute hearer salience hierarchy add final constraint plan make sure hearer confidence threshold exceeded judging adequacy referring expression plan fall regular plan evaluation process final constraint hold invalidity noted plan operated discourse plan example recognizing reference example hearer evaluates plan belief salience information table computes confidence value value exceeds confidence threshold accept plan believe error constraint check confidence threshold",
        "recipient confident adequacy plan us instance postpone plan inform initiator confident adequacy causing initiator raise confidence threshold refashion expression ability help initiator suggesting good way expand suggestion conversational agent suggests new attribute deems increase confidence expression adequacy expression expanded include attribute example hearer confident adequacy looking building suggest initiator use height architectural style asking tall suggestion initiator expand expression tall funny looking building sense suggestion illocutionary act questioning suggesting way expand plan agent asking referent suggested attribute decide suggestion agent us instance suggest expand plan mental action decomposition chooses attribute belief salient result plan surface speech action s suggest communicates suggestion initiator referring expression elaborate referring expression knowledge hearer expression make suggestion initiator option elaborating plan suggestion expand plan according belief referent attribute salience hand suggestion attempt expand plan affirming denying attribute suggested possible use suggestion elaborate plan avoiding unwanted conversational implicature use plan adequate decomposition expand plan call plan constructor goal constructing modifier schema suggested attribute input sense continuing construction initial referring plan plan constructor attempt find plan surface speech action suggested attribute yield possible case speaker construct expansion plan adequate according belief response suggestion depends suggestion expand plan speaker canaffirm plan expanded suggestion s affirm speech act affirm suggestion additional attribute suggested s affirm s action ordeny suggestion s deny inform s action plan expanded postponement elaboration suggestion move agent collaborate discourse refashioning referring expression believe recipient confident adequate",
        "agent refers particular object known agent intention agent able identify object possible mean referring expression inevitable difference belief world salient agent collaborate expression adequate implemented computational plan based model account collaborative nature reference domain interactive direction giving agent construct referring expression plan referent salient feature agent understands reference confident adequacy inferred plan mean identifying referent collaborate agent use judgment suggestion elaboration move refashion referring expression believe recipient understood",
        "natural language generation process building adequate utterance given content nature decision making problem appelt decision basis specified input input information insufficient respect input structure generation provided ai system global problem producing sufficient input information generator occur translation mismatch machine translation kameyama case generator use default formulate request clarification order able continue processing produce utterance simultaneous interpretation request unusual default allow standalone handling problem example problem speech recognition automatic interpretation lead result like man men come hotel tomorrow system able preference alternative evaluating context information generator choose probable number value complete verbalization incremental generation input information produced handed step step incomplete sufficient behaviour generator motivated psycholinguistic observation people start speaking necessary linguistic material chosen articulating noun phrase dominating verb selected consequence underspecification incremental generation based working default uttered processing input consumption finished give example default context simultaneous interpretation japanese negation specified end sentence english specified finite verb japanese english translation analysis transfer generation performed parallel incremental way system commit positive value knowing actual polarity speaking default handling specifies processing decision making continue sufficient input information compare default handling advice system reason uncertainty assumption incremental system facility able repair default decision assumption turn wrong information given example negation specifier given end japanese input sentence integrated output sentence finite verb uttered case output repaired repeating part utterance able meet oops wo able meet hotel evening following section argue appropriateness processing conforming default handling processing conforming mode make overall system homogeneous combination default caused processing input licensed processing requires specific description homogeneity helpful case input verifies default assumption rendering unnecessary recomputation opposite case default withdrawn mark default homogeneity introduced incremental system default description given term input specification representation allows easy checking coincidence chosen default input given content paper summarized follows provides general description default generation emphasizing specific requirement incremental system identifying condition default triggered section application default section definition description section outlined crucial",
        "literature non incremental generation need default taken account common point view restricts input sufficient generation text structure meteer syntactic generator incremental generation author agree necessity default smedt kitano ward sufficient depth answer question guide process default handling repair generator problem starting point following consideration assume generation decision making process aim producing plausible utterance based given information mentioned section case process stop caused underspecification input finishing output define module named default handler try resume process giving advice making assumption missing input specification respect task discussedin situation default applied section default handling integrated system section knowledge default handling described section andhow assumption cancelled turn inconsistent arriving input section incremental generation mentioned section interleaved input consumption output production cause specific default situation incremental processing scheme allows increase efficiency flexibility making analysis generation process system simultaneous interpretation overlap time competing goal incremental generation spoken output taken account estimating usefulness default fluency long hesitation avoided production utterance order acceptable hearer error utterance cause misunderstanding case error recovered appropriate self correction use self correction erroneous expression beavoided decrease intelligibility utterance trade fluency reliability maximal reliability requires secure decision lead output delay hand maximal fluency necessitates use assumption repair define default situation situation generation system finished utterance time consumed given input able continue processing non incremental generation corresponds fact input lack necessary information entire input assumed given time undecidable number value example described section default handling triggered incremental generation system new piece information enables continue processing specification negation value outlined example section possible alternative wait input trigger default handling violates fluency goal violate reliability goal propose explicit use time limit delay interval certainty default described value default situation identified certainty default checked exceeds predefined threshold determines degree fluency reliability application default decrease global certainty system state limit maximal number default applicable sentence strategy integrate default handling ongoing processing handled way differs normal processing system short cut advantage efficient handling default designer",
        "default component free deciding realization default system disadvantage difficulty providing consistency default caused input licensed processing ongoing processing deal default value ordinary manner processing conforming default handling efficient guarantee consistency processing case replacement input licensed value incremental generation system provide repair facility case non monotonic modification default caused result option order overall system homogeneous knowledge source default handling provide plausible action default situation represent knowledge set heuristic rule called default description default description defines set operation carried certain situation generation process continued default description following form set default precondition defines test applied given situation order find corresponding default body activated include test existence particular information test structure creation test state processing default body describes continue processing default adequate way incremental system propose express body specification input increment important prerequisite size increment defined cope varying amount information important advantage approach homogeneity overall system homogeneous representation default caused input licensed structure easiest direct way test coincidence contradiction default specified input caused value section approach outlined different example non incremental system operational approach preferable way consume additional input increment presupposing input considered default situation occurs default precondition applicable certainty value default description examined find provides system plausible action individual default description account global constraint processing stated knowledge source system example assumption nominative case german complement verb reason homogeneity default description compatible specification knowledge basic processing order guarantee consistency default description contain orthogonal basic knowledge source repair false assumption crucial point default handling context incremental processing default information remain cause decision system non incremental input value given contradict default value step repair inconsistency input provided default caused value identified matching value effect respective default withdrawn introducing input provided value system decision generation influence decision system effect default body propagated entire system choosing construction main clause causal subordinate clause influence choice syntactic realization speaking withdrawing default assumption realized backtracking earlier state system default introduced non monotonic change current state system disadvantage backtracking partial result thrown",
        "define default situation situation generation system finished utterance time consumed given input able continue processing non incremental generation corresponds fact input lack necessary information entire input assumed given time undecidable number value example described section default handling triggered incremental generation system new piece information enables continue processing specification negation value outlined example section possible alternative wait input trigger default handling violates fluency goal violate reliability goal propose explicit use time limit delay interval certainty default described value default situation identified certainty default checked exceeds predefined threshold determines degree fluency reliability application default decrease global certainty system state limit maximal number default applicable sentence",
        "strategy integrate default handling ongoing processing handled way differs normal processing system short cut advantage efficient handling default designer default component free deciding realization default system disadvantage difficulty providing consistency default caused input licensed processing ongoing processing deal default value ordinary manner processing conforming default handling efficient guarantee consistency processing case replacement input licensed value incremental generation system provide repair facility case non monotonic modification default caused result option order overall system homogeneous",
        "knowledge source default handling provide plausible action default situation represent knowledge set heuristic rule called default description default description defines set operation carried certain situation generation process continued default description following form set default precondition defines test applied given situation order find corresponding default body activated include test existence particular information test structure creation test state processing default body describes continue processing default adequate way incremental system propose express body specification input increment important prerequisite size increment defined cope varying amount information important advantage approach homogeneity overall system homogeneous representation default caused input licensed structure easiest direct way test coincidence contradiction default specified input caused value section approach outlined different example non incremental system operational approach preferable way consume additional input increment presupposing input considered default situation occurs default precondition applicable certainty value default description examined find provides system plausible action individual default description account global constraint processing stated knowledge source system example assumption nominative case german complement verb reason homogeneity default description compatible specification knowledge basic processing order guarantee consistency default description contain orthogonal basic knowledge source",
        "repair false assumption crucial point default handling context incremental processing default information remain cause decision system non incremental input value given contradict default value step repair inconsistency input provided default caused value identified matching value effect respective default withdrawn introducing input provided value system decision generation influence decision system effect default body propagated entire system choosing construction main clause causal subordinate clause influence choice syntactic realization speaking withdrawing default assumption realized backtracking earlier state system default introduced non monotonic change current state system disadvantage backtracking partial result thrown reused processing monotonic change preserve result framework cancelling default requires system identify result caused default handling link immediate result default body result influenced decision allow identification disadvantage non monotonic change complexity computation supported truth maintenance system designing incremental system simple backtracking ruled sentence uttered withdrawn perceived addressee message end processing conforming default handler generation realizing repair non monotonic change",
        "adaptation general discussion default handling system vm gen provides concrete example reader show homogeneous combination default handling regular processing utterance repair possible syntactic generator vm gen development tag gen kilger framework verbmobil aspeech speech translation system usefulness simultaneous interpretation result incremental parallel style processing gen able consume input increment varying size increment describe lexical item semantic relation input increment handed object distributed parallel system try verbalize structure result corresponding input increment gen us extension tree adjoining grammar tag joshi syntactic representation formalism adequate description natural language support incremental generation kilger finkler following introduce example default processing triggered german inflection process vm gen substantiate global statement section us syntactic property element compute morphological form information specified input number noun inherited element number verb case noun reason missing information necessitate different method treatment integrated regular processing information type missing problem analysis section assumption simulating respective input default missing number information vm gen look follows set default precondition applied object obj vm gen order test kind underspecification number example default body introduces new value creating input increment default test coincidence input licensed value realized comparison object vm gen unique association input increment object vm gen obj identifier allows translating input modification modification state respective object case contradiction default default caused decision revised assumption influenced global constraint example studied psycholinguistics utterance noun verb chosen noun besucher english visitor known agent action uttered subject position sentence default treatment presupposes choice dummy verb subcategorizes subject active voice use dummy verb underspecified verbal structure integrated allows simple global test rule case value assignment different complement required german verb rule represented grammar description subcategorization frame verb reason homogeneity use information stored syntactic knowledge source vm gen expressing syntactic constraint default handling advantage approach processing continued consistent way eas introduction input licensed value default choosing missing case value specified follows default precondition rule characterize situation object contains information case identifies input category noun semantic function object specified agent verb defined head object n object inherit case value",
        "know allowed occupy position utterance default body system creates v object obj basis input information entity obj chooses minimal syntactic structure inheritance net grammar desribes verb category concrete filler dummy verb subject complement active voice verbal phrase n structure combined v structure introduced v object normal processing case value inherited position assigned subject uttered basic vm gen module provides repair strategy order allow specification addition modification deletion input increment model flexible input interface feature system repair input increment associated object vm gen input modification translated modification object state modification object state make compare new old information case difference modified part sent concerned object dependency relation determine communication link object allow hierarchical organization object basis synchronizing repair repair triggered example described verb voice passive specified case mapping semantic role agent syntactic function subject revised agent realized von phrase object check uttered sentence includes revised material object participated uttering case sends error message uppermost object hierarchy engaged uttering object able synchronize global repair realized simple repair strategy consists repeating concerned part utterance der besucher dieser termin wird von dem besucher gewnscht",
        "paper proposes processing conforming default handler generation realizing repair non monotonic change provide system default description set default precondition express possible reason dead end situation default triggered precondition match current situation certainty value default exceeds predefined threshold default body expressed term missing input specification order system work verified advantage processing conforming default handling implementing default handler vm gen future work extend default precondition handling complex contextual information apply default handling microplanning lexical choice verbmobil respect sophisticated output aim combine vm gen flexible repair component system vm gen verbmobil scenario multilingual generation english german japanese mean multilinguality processing applied different language underlying knowledge source language specific constraint defined handling adapted requirement multilingual generation language specific default description knowledge source question arises knowledge shared intend use core knowledge source representing common phenomenon core set default description english german contains description reaction missing number value noun aim develop efficient storing mechanism hierarchy intersecting core description",
        "pattern matching capability neural network detect syntactic constituent natural language approach bear comparison probabilistic system advantage negative positive information modelled computation advance net trained run time computational load low work neural network automated system find partial parse declarative sentence connectionist processor operate grammatic framework supported pre processor filter data reduce problem tractable size prototype accessed internet user try text detail author sentence locate subject find head subject sentence second sentence second process workstation prototype technical manual subject head detected case section known complexity parsing addressed decomposing problem locating syntactic constituent time sentence decomposed broad syntactic categoriespre subject subject locating subjectthen constituent processed underlying principle employed step sentence sentence generate string boundary marker syntactic constituent question placed possible position neural net selects string correct placement paper give overview natural language converted representation neural net handle problem reduced manageable size outline neural net selection process comprehensive account given lyon description neural net process lyon lyon frank hybrid system core process data driven parameter neural network derived training text neural net trained supervised mode example marked correct incorrect able classify unseen example initial processing stage problem size constrained operate skeletal grammatic framework tractability addressed reducing data application prohibitive rule local constraint pruning process effective",
        "work developed text technical manual perkins engine ltd translated semi automatic process pym partial parse support process instance occurring modal verb distinguished number english language necessary locate subject identify head determine number order translate main verb sentence parser trained find syntactic subject head agrees number main verb manual written pace perkins approved clear english guideline aim producing clear unambiguous text declarative sentence extracted processing half imperative level classification future figure characteristic corpus mark counted word formula word",
        "order reconcile computational feasibility empirical realism appropriate form language representation critical step constraining problem size partition unlimited vocabulary restricted number speech tag stage processing place different requirement classification system customised tagsets developed processing stage need place subject marker task disambiguate tag found necessary use number information stage example consider sentence word water person singular present verb plural noun order disambiguate tag place subject marker necessary know noun verb sentence parsed level return tagset stage mode class distinguished number head subject found number agreement verb assessed stage tagset mode includes number information class optimal tagsets given task field work planned need larger tagsets capture linguistic information smaller one constrain computational load theoretic tool find entropy different tag sequence language support decision representation functional approach taken tagging word allocated class depending syntactic role instance superlative adjective act noun given tag noun adjective approach extended taking adjacent word act single lexical item unit pair adjective taken single superlative adjective tagged module claw program version word allocated tag class tag mapped small customised tagsets disambiguation parsing task handled neural net pre processor version claw dictionary word word tagged suffix information default invoked correct tag included set allocated tag necessary proposed larger dictionary version address problem way tag allocated word punctuation mark represent boundary syntactic constituent noun phrase verb phrase marker considered invisible tag hypertags probabilistic relationship adjacent tag way word church approach embedded syntactic constituent sought single pas lead computation overload pocock atwell approach us similar concept differs embedded syntactic constituent detected time separate step hypertags opening closing bracket marking possible location syntactic constituent question representation hierarchical language structure converted string tag represented linear vector",
        "way tag allocated word punctuation mark represent boundary syntactic constituent noun phrase verb phrase marker considered invisible tag hypertags probabilistic relationship adjacent tag way word church approach embedded syntactic constituent sought single pas lead computation overload pocock atwell approach us similar concept differs embedded syntactic constituent detected time separate step hypertags opening closing bracket marking possible location syntactic constituent question representation hierarchical language structure converted string tag represented linear vector",
        "system generates set tag string sentence hypertags placed possible position subject detection task generate string tag including hypertags inserted pair closure enforced arbitrary limit maximum word pre subject word subject initial work described extended word pre subject subject section word end subject end sentence mark initial restriction sentence word counting punctuation mark word alternative placement word possible tag instance sentence word alternative tag generate possible string hypertags inserted word including punctuation total number string beit feasible detect string number classifier marked string incorrect percentage classified useless order find correct string outside candidate dropped minimal grammar set lyon ebnf form composed rule instance subject contain noun type word particular rule sentence eliminate candidate string arbitrary limit length pre subject subject small set extension grammar semi local constraint instance relative pronoun occurs verb follow constituent technical manual constraint grammatic framework declarative sentence system pre subject long small number excluded system handle co ordinated head length pre subject extended word subject word average excluded grammatic framework reduce number candidate string subject detection stage problem addressed method suggested barton local constraint rein generation intractable number possibility system local constraint prohibited tag pair triple adjacent tag allowed determiner verb start subject verb generation candidate string prohibited tuple encountered process aborted prohibited pair triple method number candidate string reduced technical manual average string string left sentence left single string filter rule differ generative rule produce allowable string language case production admitted allowed contrast prohibited allowed stage data ready present neural net give overview process",
        "minimal grammar set lyon ebnf form composed rule instance subject contain noun type word particular rule sentence eliminate candidate string arbitrary limit length pre subject subject small set extension grammar semi local constraint instance relative pronoun occurs verb follow constituent technical manual constraint grammatic framework declarative sentence system pre subject long small number excluded system handle co ordinated head length pre subject extended word subject word average excluded",
        "grammatic framework reduce number candidate string subject detection stage problem addressed method suggested barton local constraint rein generation intractable number possibility system local constraint prohibited tag pair triple adjacent tag allowed determiner verb start subject verb generation candidate string prohibited tuple encountered process aborted prohibited pair triple method number candidate string reduced technical manual average string string left sentence left single string filter rule differ generative rule produce allowable string language case production admitted allowed contrast prohibited allowed stage data ready present neural net give overview process",
        "different network architecture investigated share input output representation output net vector element node represent correct incorrect yes figure input net derived candidate string sequence tag hypertags converted binary vector element vector represent feature flagged absent present form vector written illusion representing order sequential order maintained method representing sequence chosen sequential order input captured taking adjacent tag pair triple feature element individual tag converted bipos tripos representation method tag tripos bipos element redundant code aid processing sparse data typical natural language work described sentence truncated word hypertag marking close subject process improved going sentence",
        "net gave best result simple single layer net figure derived hodyne net wyard nightingale conventionally single layer net layer processing node layer network process inseparable data investigated necessary particular processing task linear separability data related order system us higher order pair triple input question appropriate network architecture examined pao widrow lehr lyon net presented training string desired classification marked weight connection input output node adjusted required level performance reached weight fixed trained net ready classify unseen sentence prototype accessible internet trained sentence technical manual augmented weighted link disabled string presented network training mode activates set input node input node linked output node representing desired response connected weight connection initialised input node connected output tuples occur grammatical ungrammatical string connected output figure input layer node possible tuple tag hypertags start symbol upper bound number input node practice maximum activated testing mode unseen tuple appears make contribution result activation input layer fed weighted connection output node summed highest output mark winning node desired node win action taken desired node win weight connection desired node incremented weight connection unwanted node decremented algorithm differs method feed network trained supervised mode perform classification task different penalty measure trigger weight update propagation single layer training method minimise metric based squared error lse desired actual activation output node reason differentiable error measure sort necessary multi layer net documented rumelhart mcclelland single layer net choose update weight error output node trigger weight update connection feed lse minimising number misclassifications certain type data second method direct training appropriate natural language domain desirable information infrequent common event event noise useful contribution classification task need method capture information infrequent event adopt direct measure misclassification suited data zipfian distribution shannon update factor chosen meet requirement positive asymptotic maximum minimum bound factor greatest central region move direction original hodyne",
        "function work practice update factor given following formula strengthening weight weakening thenrecall weight initialised training find weight range bounded bytotal time training measured second number iterative cycle necessary depends threshold chosen trained net cross detail vector representation demonstration prototype take second recent improved representation string trained second result net given table found triple gave good result pair triple net train correct lower threshold give better generalisation give better result test data trained net run unseen data weight link fixed link disabled activated initialised tuples occurred training corpus contribution classification task pre processer time candidate string generated presented network output interpreted difference yes activation level recorded string score considered measure grammaticality string highest score taken correct result given network trained corpus tested corpus prototype user process text net trained corpus augmented",
        "net presented training string desired classification marked weight connection input output node adjusted required level performance reached weight fixed trained net ready classify unseen sentence prototype accessible internet trained sentence technical manual augmented weighted link disabled string presented network training mode activates set input node input node linked output node representing desired response connected weight connection initialised input node connected output tuples occur grammatical ungrammatical string connected output figure input layer node possible tuple tag hypertags start symbol upper bound number input node practice maximum activated testing mode unseen tuple appears make contribution result activation input layer fed weighted connection output node summed highest output mark winning node desired node win action taken desired node win weight connection desired node incremented weight connection unwanted node decremented algorithm differs method feed network trained supervised mode perform classification task different penalty measure trigger weight update propagation single layer training method minimise metric based squared error lse desired actual activation output node reason differentiable error measure sort necessary multi layer net documented rumelhart mcclelland single layer net choose update weight error output node trigger weight update connection feed lse minimising number misclassifications certain type data second method direct training appropriate natural language domain desirable information infrequent common event event noise useful contribution classification task need method capture information infrequent event adopt direct measure misclassification suited data zipfian distribution shannon update factor chosen meet requirement positive asymptotic maximum minimum bound factor greatest central region move direction original hodyne function work practice update factor given following formula strengthening weight weakening thenrecall weight initialised training find weight range bounded bytotal time training measured second number iterative cycle necessary depends threshold chosen trained net cross detail vector representation demonstration prototype take second recent improved representation string trained second result",
        "trained net run unseen data weight link fixed link disabled activated initialised tuples occurred training corpus contribution classification task pre processer time candidate string generated presented network output interpreted difference yes activation level recorded string score considered measure grammaticality string highest score taken correct result given network trained corpus tested corpus prototype user process text net trained corpus augmented",
        "par postulated sentence negative positive example occur natural language negative correlation important source information occurrence word group word inhibit following wish exploit constraint recognised introduced idea distituents element sentence separated opposed element constituent cling address problem finding valid metric distituency generalized mutual information statistic marked mutual information minimum method supported small rule grammar approach capture sense inhibitory factor play negative neutral role want distinguish item unlikely occur happened turn training data example sentence cref string cref correct distinguished correct par training data order improbability modelled inhibitory connection nile silverman hidden markov model implemented neural network theoretical ground incorporating negative example language learning process originates work gold developed angluin examined process learning grammar formal language example showed language high chomsky hierarchy cfgs inference positive data powerful inference positive negative data illustrate consider case inference number example presented inference machine possible grammar postulated positive data problem generalization arises postulated grammar superset real grammar sentence real grammar accepted positive negative data counter example reduce postulated grammar nearer real grammar developed theory formal language argued similar consideration apply grammar inferred positive example certain subset regular language garcia vidal inference process degenerate look procedure possible positive example stored case negative information required plausible model unbounded natural language method required parse found inferring grammar positive negative information modelled neural net work investigate effect training network positive example current size corpus data relationship neural net rule prohibition table seen following way single rule prohibiting tuple adjacent tag omitted neural network handle linking node representing tuple processing step need reduce number candidate tag string presented neural network manageable proportion section data pre processed filtering prohibition rule constraint number candidate string desirable bound head detection task rule system data driven possible rule invoked needed problem tractable",
        "relationship neural net rule prohibition table seen following way single rule prohibiting tuple adjacent tag omitted neural network handle linking node representing tuple processing step need reduce number candidate tag string presented neural network manageable proportion section data pre processed filtering prohibition rule constraint number candidate string desirable bound head detection task rule system data driven possible rule invoked needed problem tractable",
        "working prototype indicates method described worth developing connectionist method generalise training corpus unseen text data represented higher order tuples single layer network traditional problem training time arise multi layer net data advantage perform lyon supporting role grammatic framework prohibition filter underestimated scope system extended found necessary enhance element laborious work preparing training data time representation modified new set string generated need marking autodidactic check included speed task run marked training data early version network trained data result correct incorrect parse occurs check sentence marked feature system described stochastic process connectionist method low computational load runtime utilise implicit information training data modelling negative relationship powerful concept exploited effort squeeze available piece useful information natural language processing work planned extend partial parser decompose sentence hierarchical constituent part order number subsidiary task addressed system improved identifying group word act single lexical item decomposition problem investigated instance tag disambiguation task precede placement subject boundary marker separate step detailed investigation language representation issue undertaken critical issue investigating appropriate network architecture carried",
        "parsing taken step taming natural language understanding task broad coverage nlp remains jungle inhabited wild beast instance parsing noun compound appears require detailed world knowledge unavailable outside domain sparck jones obscure endangered specie noun compound flourishing modern language appearance paragraph diachronic study show veritable population explosion leonard substantial work noun compound exists linguistics levi ryder computational linguistics finin mcdonald isabelle technique suitable broad coverage parsing remain unavailable paper explores application corpus statistic charniak noun compound parsing computational problem addressed arens vanderwende sproat task illustrated example par assigned compound differ sequence part speech identical problem analogous prepositional phrase attachment task explored hindle rooth approach propose involves computing lexical association corpus select correct parse similar architecture applied noun compound experiment accuracy system measured dimension analysis model applied adjacency dependency range training scheme employed computed tuning factor suggested literature parameterisations association word association concept collected machine tagging corpus hindle rooth use partial parser acquire training data machinery appears unnecessary noun compound proposed use simple word pattern acquisition verb subcategorisation information analogous approach compound lauer constitutes scheme evaluated pattern produce false training example resulting noise introduces minor distortion liberal alternative use co occurrence window us fixed word window collect information sense disambiguation smadja us content word window extract significant collocation range windowed training scheme employed use window provides natural mean trading data quality data sparseness undermines system accuracy wider window admit sufficient volume extra accurate data outweigh additional noise existing corpus based algorithm proposed analysing noun compound subjected evaluation case comparison performed fact author appear unaware proposal describe algorithm algorithm use adjacency model analysis procedure go marcus procedure stated term call oracle determine noun compound acceptable reproduced reference given noun acceptable build alternative structure preferable build build suggested corpus statistic provide oracle idea basis algorithm use adjacency model simplest reported pustejovsky",
        "word compound search conducted corpus possible subcomponents found chosen bracketed pair example backup compiler disk encountered analysis proposed rough heuristic stated outcome subcomponents appear evaluation algorithm proposal liberman sproat sophisticated allows frequency word compound proposal involves comparing mutual information pair adjacent word bracketing pair exhibit highest evaluation method demonstration example work proposal based adjacency model appears resnik complex selectional association predicate word defined based contribution word conditional entropy predicate association pair word compound computed taking maximum selectional association possible way pair predicate argument association metric complicated decision procedure follows outline devised marcus unambiguous noun compound parsed wall street journal wsj corpus estimate association value analysed test set compound tuning accuracy compared baseline achieved bracketing noun fourth algorithm described lauer differs striking manner us dependency model model utilises following procedure given noun determine acceptable structure acceptable build build show graphical comparison analysis model lauer degree acceptability provided statistical measure corpus metric mutual information like measure based probability modification relationship derived idea parse tree capture structure semantic relationship noun compound dependency model attempt choose parse make resulting relationship acceptable possible example backup compiler disk encountered analysis claim dependency model make intuitive sense following reason compound calcium ion exchange left branching word bracketed reason calcium ion frequent ion exchange plausible compound bracketing ion object exchange correct parse depends calcium characterises ion mediates exchange significant difference model prediction proportion left right branching compound dras dependency model left branching compound occur right branching compound third time test set resnik proportion left branching compound contrast adjacency model appears predict proportion dependency model proposed kobayasi analysing japanese noun compound corpus acquire association bracket sequence kanji length equivalent word simple calculation",
        "parsing taken step taming natural language understanding task broad coverage nlp remains jungle inhabited wild beast instance parsing noun compound appears require detailed world knowledge unavailable outside domain sparck jones obscure endangered specie noun compound flourishing modern language appearance paragraph diachronic study show veritable population explosion leonard substantial work noun compound exists linguistics levi ryder computational linguistics finin mcdonald isabelle technique suitable broad coverage parsing remain unavailable paper explores application corpus statistic charniak noun compound parsing computational problem addressed arens vanderwende sproat task illustrated example par assigned compound differ sequence part speech identical problem analogous prepositional phrase attachment task explored hindle rooth approach propose involves computing lexical association corpus select correct parse similar architecture applied noun compound experiment accuracy system measured dimension analysis model applied adjacency dependency range training scheme employed computed tuning factor suggested literature parameterisations association word association concept collected machine tagging corpus",
        "hindle rooth use partial parser acquire training data machinery appears unnecessary noun compound proposed use simple word pattern acquisition verb subcategorisation information analogous approach compound lauer constitutes scheme evaluated pattern produce false training example resulting noise introduces minor distortion liberal alternative use co occurrence window us fixed word window collect information sense disambiguation smadja us content word window extract significant collocation range windowed training scheme employed use window provides natural mean trading data quality data sparseness undermines system accuracy wider window admit sufficient volume extra accurate data outweigh additional noise",
        "existing corpus based algorithm proposed analysing noun compound subjected evaluation case comparison performed fact author appear unaware proposal describe algorithm algorithm use adjacency model analysis procedure go marcus procedure stated term call oracle determine noun compound acceptable reproduced reference given noun acceptable build alternative structure preferable build build suggested corpus statistic provide oracle idea basis algorithm use adjacency model simplest reported pustejovsky word compound search conducted corpus possible subcomponents found chosen bracketed pair example backup compiler disk encountered analysis proposed rough heuristic stated outcome subcomponents appear evaluation algorithm proposal liberman sproat sophisticated allows frequency word compound proposal involves comparing mutual information pair adjacent word bracketing pair exhibit highest evaluation method demonstration example work proposal based adjacency model appears resnik complex selectional association predicate word defined based contribution word conditional entropy predicate association pair word compound computed taking maximum selectional association possible way pair predicate argument association metric complicated decision procedure follows outline devised marcus unambiguous noun compound parsed wall street journal wsj corpus estimate association value analysed test set compound tuning accuracy compared baseline achieved bracketing noun fourth algorithm described lauer differs striking manner us dependency model model utilises following procedure given noun determine acceptable structure acceptable build build show graphical comparison analysis model lauer degree acceptability provided statistical measure corpus metric mutual information like measure based probability modification relationship derived idea parse tree capture structure semantic relationship noun compound dependency model attempt choose parse make resulting relationship acceptable possible example backup compiler disk encountered analysis claim dependency model make intuitive sense following reason compound calcium ion exchange left branching word bracketed",
        "reason calcium ion frequent ion exchange plausible compound bracketing ion object exchange correct parse depends calcium characterises ion mediates exchange significant difference model prediction proportion left right branching compound dras dependency model left branching compound occur right branching compound third time test set resnik proportion left branching compound contrast adjacency model appears predict proportion dependency model proposed kobayasi analysing japanese noun compound corpus acquire association bracket sequence kanji length equivalent word simple calculation show preprocessing hueristics guess bracketing provides higher accuracy test set statistical model render experiment inconclusive",
        "test set ambiguous noun compound extracted word grolier encyclopedia corpus following way corpus tagged parsed conservative strategy looking unambiguous sequence noun distinguish noun word university pennsylvania morphological analyser described karp generate set word noun shall set consecutive sequence word extracted word sequence form test set reason clear sequence consisting word roget thesaurus retained giving total test triple triple analysed context entire article appeared case sequence noun compound noun appear adjacent constituent boundary marked error compound exhibited hindle rooth termed semantic indeterminacy possible bracketings distinguished context remaining compound assigned left branching right branching analysis show number kind example figure result reported computed compound received parse problem applying lexical association noun compound enormous number parameter required possible pair noun require vast memory space creates severe data sparseness problem require data parameter hearst coined term conceptual association refer association value computed group word assuming word group behave parameter space built term group term word study conceptual association group consisting category version roget thesaurus thesaurus category parameter represents degree acceptability structure noun appearing appears assumption word group behave constant given category lauer dras write parameter event denotes modification noun noun ensure test set disjoint training data occurrence test noun compound removed training corpus type training scheme explored study unsupervised employ pattern follows pustejovsky counting occurrence subcomponents training instance sequence word number time sequence occurs training corpus second type us window collect training instance observing pair noun co occur fixed number word study variety window size let number time sequence occurs training corpus windowed count asymmetric case window word wide yield mutual information metric proposed liberman sproat different training scheme arrive appropriate count possible estimate parameter expressed term category word necessary combine count word arrive estimate case estimate wherehere number category appears",
        "effect dividing evidence training instance possible category word normaliser ensures parameter head noun sum unity high level description section remains decision process analyse noun compound test compound present set possible analysis goal choose analysis likely word compound suffices compute ratio probability left branching analysis right branching ratio greater unity left branching analysis chosen unity right branching analysis chosen ratio unity analyser guess left branching rare conceptual association shown experimental result adjacency model given compound estimate ratio dependency model ratio case sum possible category word compound dependency model equation factor affected data sparseness probability estimate possible category numerator denominator conceal preference given parameter involving case observe test instance provides information event occur recalculate ratio possible category non constant correction probability estimate unseen case putting dependency model equal footing adjacency model equation presented dependency model differ developed lauer dras way additional weighting factor favour left branching analysis arises construction based dependency model predicts left branching analysis occur work reported lauer dras us simplistic estimate probability word given thesaurus category equation assume probability constant show result making addition method",
        "test set ambiguous noun compound extracted word grolier encyclopedia corpus following way corpus tagged parsed conservative strategy looking unambiguous sequence noun distinguish noun word university pennsylvania morphological analyser described karp generate set word noun shall set consecutive sequence word extracted word sequence form test set reason clear sequence consisting word roget thesaurus retained giving total test triple triple analysed context entire article appeared case sequence noun compound noun appear adjacent constituent boundary marked error compound exhibited hindle rooth termed semantic indeterminacy possible bracketings distinguished context remaining compound assigned left branching right branching analysis show number kind example figure result reported computed compound received parse",
        "problem applying lexical association noun compound enormous number parameter required possible pair noun require vast memory space creates severe data sparseness problem require data parameter hearst coined term conceptual association refer association value computed group word assuming word group behave parameter space built term group term word study conceptual association group consisting category version roget thesaurus thesaurus category parameter represents degree acceptability structure noun appearing appears assumption word group behave constant given category lauer dras write parameter event denotes modification noun noun",
        "ensure test set disjoint training data occurrence test noun compound removed training corpus type training scheme explored study unsupervised employ pattern follows pustejovsky counting occurrence subcomponents training instance sequence word number time sequence occurs training corpus second type us window collect training instance observing pair noun co occur fixed number word study variety window size let number time sequence occurs training corpus windowed count asymmetric case window word wide yield mutual information metric proposed liberman sproat different training scheme arrive appropriate count possible estimate parameter expressed term category word necessary combine count word arrive estimate case estimate wherehere number category appears effect dividing evidence training instance possible category word normaliser ensures parameter head noun sum unity",
        "given high level description section remains decision process analyse noun compound test compound present set possible analysis goal choose analysis likely word compound suffices compute ratio probability left branching analysis right branching ratio greater unity left branching analysis chosen unity right branching analysis chosen ratio unity analyser guess left branching rare conceptual association shown experimental result adjacency model given compound estimate ratio dependency model ratio case sum possible category word compound dependency model equation factor affected data sparseness probability estimate possible category numerator denominator conceal preference given parameter involving case observe test instance provides information event occur recalculate ratio possible category non constant correction probability estimate unseen case putting dependency model equal footing adjacency model equation presented dependency model differ developed lauer dras way additional weighting factor favour left branching analysis arises construction based dependency model predicts left branching analysis occur work reported lauer dras us simplistic estimate probability word given thesaurus category equation assume probability constant show result making addition method",
        "different training scheme estimate parameter set estimate analyse test set adjacency dependency model scheme pattern given section windowed training scheme window width word accuracy test set experiment shown figure seen dependency model accurate adjacency model true spectrum training scheme proportion case procedure forced guess data supported analysis supported low pattern word window training scheme guess rate model word window training scheme guess rate larger window model forced guess case pattern training scheme difference adjacency dependency significant level demonstrating superiority dependency model compound grolier encyclopedia case windowed training scheme outperform pattern scheme additional instance admitted windowed scheme noisy improvement result applying method ema corpus obtained wilco ter stal support conclusion dependency model superior adjacency model dras suggest improvement method factor favouring left branching arises formal dependency construction andfactors allowing naive estimate variation probability category change motivated dependency model applied adjacency model comparison implement equation modified incorporate factor term sum entire ratio multiplied training scheme applied extension accuracy result shown figure comparison untuned accuracy figure shown dotted line marked improvement observed adjacency model dependency model improved determine difference conceptual association pattern training scheme retrained lexical count dependency adjacency model word test set system applied total noun parameter required branching favoured factor described previous section estimate category probability meaningless lexical association method guess rate shown figure association outperforms lexical association ability generalise problem training method given section restriction training data noun noun common one verbal adjectival usage preclude occur noun provide useful training information current system ignores test tagged data difference available brill tagger brill applied corpus tagged training data available corpus tagger default rule rule produced brill training brown",
        "different training scheme estimate parameter set estimate analyse test set adjacency dependency model scheme pattern given section windowed training scheme window width word accuracy test set experiment shown figure seen dependency model accurate adjacency model true spectrum training scheme proportion case procedure forced guess data supported analysis supported low pattern word window training scheme guess rate model word window training scheme guess rate larger window model forced guess case pattern training scheme difference adjacency dependency significant level demonstrating superiority dependency model compound grolier encyclopedia case windowed training scheme outperform pattern scheme additional instance admitted windowed scheme noisy improvement result applying method ema corpus obtained wilco ter stal support conclusion dependency model superior adjacency model",
        "lauer dras suggest improvement method factor favouring left branching arises formal dependency construction andfactors allowing naive estimate variation probability category change motivated dependency model applied adjacency model comparison implement equation modified incorporate factor term sum entire ratio multiplied training scheme applied extension accuracy result shown figure comparison untuned accuracy figure shown dotted line marked improvement observed adjacency model dependency model improved",
        "problem training method given section restriction training data noun noun common one verbal adjectival usage preclude occur noun provide useful training information current system ignores test tagged data difference available brill tagger brill applied corpus tagged training data available corpus tagger default rule rule produced brill training brown corpus result poor tagging accuracy possible tagged corpus produce better result training scheme tuned analysis procedure applied test set show resulting accuracy accuracy value figure displayed dotted line admitting additional training data based tagger introduces noise reducing accuracy pattern training scheme improvement dependency model producing highest overall accuracy",
        "experiment demonstrate number important point general crude corpus statistic provide information syntax compound noun information applied broad coverage parsing assist control search shown corpus moderate size possible reasonable result tagger parser employing customised training pattern windowed co occurrence help possible data sparse condition better performance achieved method significance use conceptual association deserves mention argued broad coverage system impossible contrast previous work conceptual association resulted little improvement task performed study technique proved worth supporting generality generalisation training information outperforms equivalent lexical association approach given information comparison performed experiment stand exhibiting greatest contrast experiment dependency model provides substantial advantage adjacency model prevalent proposal literature result accordance informal reasoning given section model commendation predicts observed proportion left branching compound found extracted test set accurate technique achieved accuracy compared achieved guessing left branching high frequency occurrence noun compound text suggests use technique probabilistic parser result higher performance broad coverage natural language processing",
        "natural language processing filtering weed search path redundant going proof tree corresponding natural language expression generated parsed optimization comprises extension specific processing strategy exploit specific knowledge grammar computational task time remains unclear optimization relate mean paper starting definite clause characterization filtering derived logic grammar magic compilation filter optimization performed processor independent clean fashion template general compilation technique efficient evaluation logic program developed deductive database community ramakrishnan logic program magic produce new program filtering resulting evaluation characterized called magic predicate produce variable binding filtering evaluated original rule program extended binding effective result definite clause characterization filtering magic brings filtering logic underlying grammar discus filter optimization optimization direction independent sense useful generation parsing expository reason presented basis example generation compilation limit information filtering lead nontermination tree fragment enumerated evaluation magic compiled grammar connected johnson forthcoming magic generation fall prey non termination face head recursion generation analog left recursion parsing necessitates dynamic processing strategy memoization extended abstraction function restriction shieber weaken filtering subsumption check discard redundant result shown large class grammar subsumption check influence processing efficiency eliminated tuning magic predicate derived particular grammar applying abstraction function line fashion eliminate superfluous filtering step line optimization order right hand category rule logic grammar processed minnen resulting processing behavior considered generalization head corner generation approach shieber need rely notion semantic head chain rule head corner behavior mimicked strict fashion",
        "approach focus exploiting specific knowledge grammar computational task making filtering explicit extending processing strategy information effective generation example extended processing strategy head corner generation semantic linking shieber earley generation semantic filter shieber approach accomplish considerable improvement respect efficiency termination behavior remains unclear optimization relate comprises logic specialized form filtering bringing filtering logic underlying grammar possible perspicuous clean way filtering optimized particular fashion approach relate make filtering explicit characterizing definite clause understood filtering reversed binding information available result evaluation derived evaluation definite clause characterization filtering following basic magic algorithm taken ramakrishnan program query program construct new program new predicate predicate arity rule add modified version rule rule head modified version obtained adding literal body rule head literal body add magic rule head body contains literal literal precede rule seed fact query illustrate algorithm zoom application algorithm particular grammar rule original grammar rule look follows step algorithm result following modified version original grammar rule magic literal added right hand rule guard application rule change semantics original grammar serf way incorporate relevant binding derived magic predicate avoid redundant application rule right hand literal original rule step derives following magic rule derive guard original rule guard rule defining right hand literal second right hand literal original rule lead following magic rule step algorithm ensures seed created original rule defining start category query corresponding generation john buy mary book lead following seed seed constitutes representation initial binding provided query magic predicate derive guard creation seed delayed run time grammar need recompiled possible query compilation illustrated basis simple logic grammar extract figure grammar optimized generation minnen right hand side rule reordered simple left right evaluation order constitutes optimal evaluation order grammar simple generation strategy terminate result head recursion rule necessary use memoization extended abstraction function subsumption check generation attractive inefficient forced generate possible natural language",
        "expression licensed grammar check start category possible process efficient excluding specific lexical entry semantic filter use semantic filter evaluation requires grammar obey semantic monotonicity constraint order ensure completeness shieber magic compiled grammar figure result applying algorithm previous section head recursive example grammar performing optimization beeri ramakrishnan call magic predicate corresponding lexical entry removed data flow analysis fine tune magic predicate specific processing task hand generation user specified query specification intended input beeri ramakrishnan argument bound serve filtering purpose removed modified version original rule grammar adapted effect taking data flow account observed comparing rule previous section rule figure show result generation sentence john buy mary book case example seed look follows fact passive edge item figure resulted semi naive evaluation ramakrishnan constitutes dynamic evaluation repeated derivation fact derived fact naive evaluation bancilhon blocked active edge memoized figure consist tree structure connected dotted line left corresponds filtering derivation filtering tree reversed derives magic fact starting seed fashion tree right proof tree example sentence built result unifying derived magic fact applying particular rule order derive fact magic fact unified magic literal modified version rule addition fact represented order figure clear line represent normal fact combined magic fact derive new magic fact reconstructed numbering fact figure resulting processing behavior identical behavior result earley generation gerdemann different filtering step performed fashion order obtain generator similar generator described shieber compilation process modified lexical entry extended magic literal case shieber generator evaluation magic compiled grammar produced magic variant guaranteed complete case original grammar obeys semantic monotonicity constraint",
        "magic make filtering explicit characterizing definite clause understood filtering reversed binding information available result evaluation derived evaluation definite clause characterization filtering following basic magic algorithm taken ramakrishnan program query program construct new program new predicate predicate arity rule add modified version rule rule head modified version obtained adding literal body rule head literal body add magic rule head body contains literal literal precede rule seed fact query illustrate algorithm zoom application algorithm particular grammar rule original grammar rule look follows step algorithm result following modified version original grammar rule magic literal added right hand rule guard application rule change semantics original grammar serf way incorporate relevant binding derived magic predicate avoid redundant application rule right hand literal original rule step derives following magic rule derive guard original rule guard rule defining right hand literal second right hand literal original rule lead following magic rule step algorithm ensures seed created original rule defining start category query corresponding generation john buy mary book lead following seed seed constitutes representation initial binding provided query magic predicate derive guard creation seed delayed run time grammar need recompiled possible query",
        "cycle removal incorporating relevant indexing collapsing redundant magic predicate magic compiled grammar figure look displayed figure show chart resulting generation sentence john buy mary book seed identical example previous section fact chart resulted naive evaluation semi naive evaluation subsumption checking ramakrishnan resulting processing behavior similar behavior result head corner generation different filtering step performed fashion head corner approach jump pivot pivot order satisfy assumption concerning flow semantic information semantic chaining generates starting semantic head fashion example seed delay apply base case vp procedure jumping intermediate chain non chain rule respect initial reordering rule led rule final grammar figure crucial section",
        "result characterizing filtering definite clause representation magic brings filtering logic underlying grammar allows optimized processor independent clean fashion discus possible filter optimization based program transformation technique called unfolding tamaki sato referred partial execution pereira shieber evaluation original grammar evaluation magic compiled version fall prey non termination face head recursion possible eliminate subsumption check tuning magic predicate derived particular grammar line fashion order illustrate magic predicate adapted subsumption check eliminated necessary closer look relation magic predicate fact derive figure relation magic predicate example grammar represented unfolding tree pettorossi proietti ordinary unfolding tree constructed basis seed seed adorned specification argument considered bound seed derived user specified query magic unfolding tree represented unfolding tree figure show exists need subsumption checking rule figure produce fact cyclic magic rule derived head recursive rule example grammar reason rule magic compiled grammar influence efficiency processing grammar completeness evaluation process type cycle magic compiled grammar general undecidable possible trim magic predicate applying abstraction function result explicit representation filtering need postpone abstraction run time trim magic predicate line consider bringing abstraction logic definite clause representation filtering weakened mild form connectedness result affect completeness shieber following magic rule rule derived head recursive rule specified subcategorization list considered filtering information fn rule build large subcategorization list matched subcategorization list lexical entry buy rule cyclic cyclic line abstraction trimming magic rule given bounded term depth sato tamaki restrictor shieber constructing unfolding tree reveals fact cycle result magic rule information discard culprit direct indirect cycle magic compiled grammar eliminate necessity subsumption checking case consider magic rule figure general rule subsumption checking lead spurious ambiguity rule produce magic fact subject built possible solution problem couple magic rule modified version original grammar rule instigated accomplish propose technique considered line variant indexing technique described gerdemann indexing technique illustrated basis running example rule figure coupled modified version original rule instigated rule rule receive index modified version rule defining np",
        "adapted percolate index guarding magic fact licensed application illustrated basis adapted version rule illustrated section allows avoidance spurious ambiguity absence subsumption check case example grammar collapse filtering step apparent closer investigation unfolding tree figure magic predicate provide identical variable binding guard application modified version original grammar rule reduce number magic fact produced processing figure rule eliminated unfolding literal modified rule result following new rule us seed filtering need intermediate filtering step note unfolding literal lead instantiation argument vform finite result fact literal remainder magic compiled grammar rule discarded filter optimization reminiscent computing deterministic closure magic compiled grammar doerre compile time optimization magic grammar figure lead succinct grammar brings different processing behavior resulting grammar compared head corner generation shieber section cycle removal incorporating relevant indexing collapsing redundant magic predicate magic compiled grammar figure look displayed figure show chart resulting generation sentence john buy mary book seed identical example previous section fact chart resulted naive evaluation semi naive evaluation subsumption checking ramakrishnan resulting processing behavior similar behavior result head corner generation different filtering step performed fashion head corner approach jump pivot pivot order satisfy assumption concerning flow semantic information semantic chaining generates starting semantic head fashion example seed delay apply base case vp procedure jumping intermediate chain non chain rule respect initial reordering rule led rule final grammar figure crucial section",
        "like evaluation original grammar evaluation magic compiled version fall prey non termination face head recursion possible eliminate subsumption check tuning magic predicate derived particular grammar line fashion order illustrate magic predicate adapted subsumption check eliminated necessary closer look relation magic predicate fact derive figure relation magic predicate example grammar represented unfolding tree pettorossi proietti ordinary unfolding tree constructed basis seed seed adorned specification argument considered bound seed derived user specified query magic unfolding tree represented unfolding tree figure show exists need subsumption checking rule figure produce fact cyclic magic rule derived head recursive rule example grammar reason rule magic compiled grammar influence efficiency processing grammar completeness evaluation process type cycle magic compiled grammar general undecidable possible trim magic predicate applying abstraction function result explicit representation filtering need postpone abstraction run time trim magic predicate line consider bringing abstraction logic definite clause representation filtering weakened mild form connectedness result affect completeness shieber following magic rule rule derived head recursive rule specified subcategorization list considered filtering information fn rule build large subcategorization list matched subcategorization list lexical entry buy rule cyclic cyclic line abstraction trimming magic rule given bounded term depth sato tamaki restrictor shieber constructing unfolding tree reveals fact cycle result magic rule information discard culprit direct indirect cycle magic compiled grammar eliminate necessity subsumption checking case consider magic rule figure general rule subsumption checking lead spurious ambiguity rule produce magic fact subject built possible solution problem couple magic rule modified version original grammar rule instigated accomplish propose technique considered line variant indexing technique described gerdemann indexing technique illustrated basis running example rule figure coupled modified version original rule instigated rule rule receive index modified version rule defining np adapted percolate index guarding magic fact licensed application illustrated basis adapted version rule illustrated section allows avoidance spurious ambiguity absence subsumption check case example grammar",
        "finding type cycle magic compiled grammar general undecidable possible trim magic predicate applying abstraction function result explicit representation filtering need postpone abstraction run time trim magic predicate line consider bringing abstraction logic definite clause representation filtering weakened mild form connectedness result affect completeness shieber following magic rule rule derived head recursive rule specified subcategorization list considered filtering information fn rule build large subcategorization list matched subcategorization list lexical entry buy rule cyclic cyclic line abstraction trimming magic rule given bounded term depth sato tamaki restrictor shieber constructing unfolding tree reveals fact cycle result magic rule information discard culprit",
        "removing direct indirect cycle magic compiled grammar eliminate necessity subsumption checking case consider magic rule figure general rule subsumption checking lead spurious ambiguity rule produce magic fact subject built possible solution problem couple magic rule modified version original grammar rule instigated accomplish propose technique considered line variant indexing technique described gerdemann indexing technique illustrated basis running example rule figure coupled modified version original rule instigated rule rule receive index modified version rule defining np adapted percolate index guarding magic fact licensed application illustrated basis adapted version rule illustrated section allows avoidance spurious ambiguity absence subsumption check case example grammar",
        "unfolding collapse filtering step apparent closer investigation unfolding tree figure magic predicate provide identical variable binding guard application modified version original grammar rule reduce number magic fact produced processing figure rule eliminated unfolding literal modified rule result following new rule us seed filtering need intermediate filtering step note unfolding literal lead instantiation argument vform finite result fact literal remainder magic compiled grammar rule discarded filter optimization reminiscent computing deterministic closure magic compiled grammar doerre compile time optimization magic grammar figure lead succinct grammar brings different processing behavior resulting grammar compared head corner generation shieber section",
        "extent useful collapse magic predicate unfolding depends grammar optimized reordering right hand side rule grammar discussed section rule running example optimized resulting processing behavior fallen case lead intermediate filtering step non chaining sentence rule addition literal corresponding subject chain non chain rule path semantic head cycle removed magic compiled grammar indexing avoid spurious ambiguity discussed previous section subsumption checking eliminated grammar ambiguous fulfill line parsability constraint shieber grammar required obey refer dependency constraint particular right hand literal evaluated result evaluation determine remainder right hand rule appears give schematic example grammar obey dependency constraint derived fact seed evaluation grammar ifigure lead spurious ambiguity possible solution result fact filtering resulting magic literal rule unspecific problematic nondeterminism disappear combining solution solution problem arises result fact solution lead identical filter evaluation literal solution determine respect dependency constraint optimization rule grammar important reordering right hand side rule grammar nondeterminism reduced shown minnen way following intended semantic dependency dependency constraint satisfied large class grammar",
        "number researcher shown organisation discourse level individual utterance grosz sidner levin moore polanyi scha reichman current exploratory study us control parameter identifying higher level structure address conversational participant co ordinate move higher level unit particular looking way use signal beginning end high level unit research identified mean speaker signal information discourse structure listener cue word phrase grosz sidner reichman intonation pierrehumbert pronominalisation guidon brennan cue word approach reichman claimed phrase like offer explicit information listener speaker current contribution discourse relates gone example speaker use expression signal conclude said sidner relate use phrase change attentional state example signal listener new topic set referent introduced case indicate return previous topic referent set indirect way signalling discourse structure intonation pierrehumbert showed intonational contour related discourse segmentation new topic signalled change intonational contour final indirect cue discourse structure speaker choice referring expression grammatical structure number researcher grosz sidner brennan guindon reichman given account relate continuing retaining shifting focus approach concentrated particular surface linguistic phenomenon investigated putative cue serf signal number dialogue problem approach cue infrequent indicator particular type shift want construct general theory discourse want know range cue serving function study take different approach begin identifying shift control dialogue look shift signalled speaker second problem previous research criterion identifying discourse structure explicit study explicit criterion given analyse relation cue structure",
        "data recording telephone conversation client expert concerning problem software tape recording dialogue transcribed analysis conducted typewritten transcript raw recording total turn dialogue utterance dialogue classified category assertion declarative utterance state fact answer question classified assertion ground supplying listener factual information command utterance intended instigate action audience included utterance imperative form relink intended induce action question utterance intended elicit information audience included utterance interrogative form question included paraphrase speaker reformulated repeated said classified question ground effect induce listener confirm deny stated prompt utterance express propositional content prompt thing like yes uhu devised rule determine location control dialogue rule related control utterance type question speaker defined control question followed question command conversant reason question uttered following question command attempt clarify preceding utterance elicited previous speaker utterance directing conversation right assertion speaker defined control assertion response question reason given question assertion response question said controlling discourse command speaker defined controlling conversation command utterance imperative form served elicit action classified way prompt listener defined controlling conversation speaker abdicating turn case turn consisted utterance control rule applied final utterance applied control rule found control alternate speaker speaker turn turn basis long sequence turn control remained speaker suggest dialogue organised level individual turn phase control located speaker mean number turn phase",
        "utterance dialogue classified category assertion declarative utterance state fact answer question classified assertion ground supplying listener factual information command utterance intended instigate action audience included utterance imperative form relink intended induce action question utterance intended elicit information audience included utterance interrogative form question included paraphrase speaker reformulated repeated said classified question ground effect induce listener confirm deny stated prompt utterance express propositional content prompt thing like yes uhu",
        "devised rule determine location control dialogue rule related control utterance type question speaker defined control question followed question command conversant reason question uttered following question command attempt clarify preceding utterance elicited previous speaker utterance directing conversation right assertion speaker defined control assertion response question reason given question assertion response question said controlling discourse command speaker defined controlling conversation command utterance imperative form served elicit action classified way prompt listener defined controlling conversation speaker abdicating turn case turn consisted utterance control rule applied final utterance applied control rule found control alternate speaker speaker turn turn basis long sequence turn control remained speaker suggest dialogue organised level individual turn phase control located speaker mean number turn phase",
        "went analyse control exchanged participant boundary phase examined utterance phase ground mechanism indicating end phase speaker controlling phase cue participant dialogue male wished control discourse total shift control dialogue identified main class cue signal control shiftsthese prompt repetition summary looked signal given interruption shift utterance control shift prompt explain shift resulting person control indicating following example line indicates control shift occasion found person control dialogue signalled new information offer repeating said occasion giving summary said preceding utterance phase occasion defined repetition assertion express propositional content previous assertion contains new information summary consisted concise reference entire set information given client problem solution plan repetition accompanied cue word prefixed assertion linguistic characteristic summary reichman suggests summary cue speaker found example instance dialogue summary characterised concise reference object entity described detail wondering related refers error message taken utterance describe characteristic summary contrast concrete description dialogue err system program standard file complete mean file tail record followed ve clue situation illustrates change specific general repetition summary operate cue summarising speaker indicating natural breakpoint dialogue indicate add stage work similar way fact speaker reiterates indicates topic previous case person controlling dialogue gave signal control exchanged occasion shift indication given went analyse condition interruption occurred fall category vital fact response vital fact clarification total occasion shift client interrupted contradict speaker supply relevant information believed expert know interjection supply extra information marked cue contradict said explicit marker remaining direct denial class interruption occur client interjection supply missing fact client",
        "blocked plan rejected explanation expert produced occasion shift interruption previous example illustrates reversion control expert client supplied information client belief relevant expert following example client control interrupt clarify said happened occasion shift occasion clarification prefixed final occasion marker direct question described circumstance interruption occur explain occur suggest following principle account interruption principle concern information participant basing plan andthe plan quality expert client believe information expert problem true information sufficient solve problem expressed following rule concern truth information ambiguity information listener belief fact belief fact relevant belief speaker belief speaker know interrupt listener belief speaker assertion relevant ambiguous interrupt quality expert client believe plan expert generated adequate solve problem comprehensible client rule express principle concern effectiveness plan ambiguity plan listener belief belief present obstacle proposed plan belief proposed plan satisfied interrupt listener belief assertion proposed plan ambiguous interrupt framework interruption seen strategy produced conversational participant perceive principle adhered investigated occasion prompt repetition summary failed elicit control shift predicted considered possible type failure speaker cue continue speaker cue listener fail respond found instance case speaker produce phrase like continue intonational contour information break suggesting phrase prefix cue find instance second case following prompt following summary long pause indicating speaker ready respond conducted similar analysis cue word identified literature repetition summary interruption cue word associated instance cue word occurring control shift",
        "occasion found person control dialogue signalled new information offer repeating said occasion giving summary said preceding utterance phase occasion defined repetition assertion express propositional content previous assertion contains new information summary consisted concise reference entire set information given client problem solution plan repetition accompanied cue word prefixed assertion linguistic characteristic summary reichman suggests summary cue speaker found example instance dialogue summary characterised concise reference object entity described detail wondering related refers error message taken utterance describe characteristic summary contrast concrete description dialogue err system program standard file complete mean file tail record followed ve clue situation illustrates change specific general repetition summary operate cue summarising speaker indicating natural breakpoint dialogue indicate add stage work similar way fact speaker reiterates indicates topic",
        "total occasion shift client interrupted contradict speaker supply relevant information believed expert know interjection supply extra information marked cue contradict said explicit marker remaining direct denial class interruption occur client interjection supply missing fact client blocked plan rejected explanation expert produced occasion shift interruption previous example illustrates reversion control expert client supplied information client belief relevant expert following example client control interrupt clarify said happened occasion shift occasion clarification prefixed final occasion marker direct question described circumstance interruption occur explain occur suggest following principle account interruption principle concern information participant basing plan andthe plan quality expert client believe information expert problem true information sufficient solve problem expressed following rule concern truth information ambiguity information listener belief fact belief fact relevant belief speaker belief speaker know interrupt listener belief speaker assertion relevant ambiguous interrupt quality expert client believe plan expert generated adequate solve problem comprehensible client rule express principle concern effectiveness plan ambiguity plan listener belief belief present obstacle proposed plan belief proposed plan satisfied interrupt listener belief assertion proposed plan ambiguous interrupt framework interruption seen strategy produced conversational participant perceive principle adhered",
        "described circumstance interruption occur explain occur suggest following principle account interruption principle concern information participant basing plan andthe plan quality expert client believe information expert problem true information sufficient solve problem expressed following rule concern truth information ambiguity information listener belief fact belief fact relevant belief speaker belief speaker know interrupt listener belief speaker assertion relevant ambiguous interrupt quality expert client believe plan expert generated adequate solve problem comprehensible client rule express principle concern effectiveness plan ambiguity plan listener belief belief present obstacle proposed plan belief proposed plan satisfied interrupt listener belief assertion proposed plan ambiguous interrupt framework interruption seen strategy produced conversational participant perceive principle adhered",
        "investigated occasion prompt repetition summary failed elicit control shift predicted considered possible type failure speaker cue continue speaker cue listener fail respond found instance case speaker produce phrase like continue intonational contour information break suggesting phrase prefix cue find instance second case following prompt following summary long pause indicating speaker ready respond conducted similar analysis cue word identified literature repetition summary interruption cue word associated instance cue word occurring control shift",
        "analysis concerned control shift shift identified series rule related utterance type control dialogue indicated different type control shift shift change topic topic remained went examine relationship topic shift different type cue interruption described necessary classify control shift according resulted shift topic identified topic shift following way judge presented dialogue dialogue marked control shift occurred judge asked state control shift accompanied topic shift judge agreed shift agreed shift disagreement majority judgment taken type control shift clear difference cue topic shift shift case interruption occur topic result topic shift pattern obvious prompt repetition summary prompt occurring topic repetition summary occurring topic suggests change topic negotiated process controlling participant signal ready close topic producing prompt repetition summary accepted participant apparent unusual participant seize control change topic interruption majority occasion participant wait strongest possible cue prompt changing topic looked general aspect control topic investigated number utterance participant control found organisation dialogue level topic found dialogue divided part separated topic shift labelled central shift part dialogue different term controlled initiated topic central shift client control turn topic expert control turn topic respective number turn client expert control central shift dialogue exception topic dialogue client control turn topic central shift expert control turn topic addition looked initiated topic produced utterance topic found dialogue client initiate topic central shift expert initiate later one discovered close relationship topic initiation topic dominance topic person initiated topic control turn expect point expert begin control turn topic point expert begin initiate new topic",
        "analysing type control shift clear difference cue topic shift shift case interruption occur topic result topic shift pattern obvious prompt repetition summary prompt occurring topic repetition summary occurring topic suggests change topic negotiated process controlling participant signal ready close topic producing prompt repetition summary accepted participant apparent unusual participant seize control change topic interruption majority occasion participant wait strongest possible cue prompt changing topic",
        "looked general aspect control topic investigated number utterance participant control found organisation dialogue level topic found dialogue divided part separated topic shift labelled central shift part dialogue different term controlled initiated topic central shift client control turn topic expert control turn topic respective number turn client expert control central shift dialogue exception topic dialogue client control turn topic central shift expert control turn topic addition looked initiated topic produced utterance topic found dialogue client initiate topic central shift expert initiate later one discovered close relationship topic initiation topic dominance topic person initiated topic control turn expect point expert begin control turn topic point expert begin initiate new topic",
        "main result exploratory study finding control useful parameter identifying discourse structure parameter identified level structure dialogue control phase topic andglobal organisation control phase found type utterance prompt repetition summary signal control shift low level structure identified control phase cue word phrase reliable predicting shift result challenge claim recent discourse theory grosz sidner reichman argue close relation cue word discourse structure examined utterance type related topic shift found interruption introduced new topic evidence high level structure dialogue evidenced topic initiation control early topic initiated dominated client opposite true later part focus current research modelling speaker listener goal allen perrault cohen levesque little research real dialogue investigating goal communicated inferred study identifies surface linguistic phenomenon reflect fact participant monitoring goal plan perceived succeeding participant use explicit cue prompt repetition summary signal readiness stage plan case participant perceive obstacle goal achieved resort interruption tried explicit rule addition methodology different study attempted provide explanation dialogue fragment dialogue explicit criterion manner identify discourse structure number dialogue small taken single problem domain likely finding central shift specific diagnostic dialogue studied research applying technique set data establish generality control rule suggested",
        "application speech recognition handwriting recognition spelling correction performance quality language model utilized bahl baker kernighan srihari baltus static language modeling performance remained unchanged advent n gram language model year shannon n gram language model capture dependency n word window largest practical natural language dependency natural language occur word window addition n gram model large making difficult implement memory constrained application appealing alternative grammar based language model model expressed probabilistic grammar tend compact n gram language model ability model long distance dependency lari young resnik schabes date little success constructing grammar based language model competitive n gram model problem magnitude paper describe corpus based induction algorithm probabilistic context free grammar outperforms n gram model inside outside algorithm baker medium sized domain result mark time grammar based language model surpassed n gram modeling task moderate size algorithm employ greedy heuristic search bayes framework post pas inside outside algorithm",
        "grammar induction framed search problem framed exception research angluin smith search space taken class grammar example work search space probabilistic context free grammar objective function taken measure dependent training data want find grammar sense model training data work language modeling including n gram model inside outside algorithm fall maximum likelihood paradigm take objective function likelihood training data given grammar optimal grammar objective function generates string training data string grammar poor language model overfit training data model language large n gram model inside outside algorithm issue evaded bounding size form grammar considered optimal grammar expressed work wish limit size grammar considered basic shortcoming maximum likelihood objective function encompass compelling intuition occam razor simpler smaller grammar preferable complex larger grammar factor objective function favor smaller grammar large prevent objective function preferring grammar overfit training data present bayes grammar induction framework includes factor motivated manner goal grammar induction taken finding grammar largest posteriori probability given training data finding grammar whereand denote training data observation unclear estimate apply bayes rule gethence frame search search objective function likelihood training data multiplied prior probability grammar satisfy goal favoring smaller grammar choosing assigns higher probability grammar particular solomonoff proposes use universal priori probability solomonoff related minimum description length principle proposed rissanen case grammatical language modeling corresponds length description grammar bit universal priori probability elegant property salient dominates enumerable probability distribution",
        "described grammar induction search grammar optimizes objective function framework restrict particular grammar formalism work consider probabilistic context free grammar assume simple greedy search strategy maintain single hypothesis grammar initialized small trivial grammar try find modification hypothesis grammar addition grammar rule result grammar higher score objective function find superior grammar new hypothesis grammar repeat process find modification improves current hypothesis grammar initial grammar choose grammar generate string assure grammar cover training data initial grammar listed table sentential symbol expands sequence expands nonterminal symbol grammar set nonterminal symbol consists different nonterminal symbol expanding terminal symbol grammar model sentence sequence generated nonterminal symbol maintain property search process symbol add grammar add rule assures sentential symbol expand symbol adding symbol affect probability grammar assigns string use term set describe set modification consider current hypothesis grammar produce superior grammar set includes following move create rule formmove create rule formfor context free grammar possible express weakly equivalent grammar rule form mentioned new symbol create rule task calculating objective function grammar inexpensive calculating requires parsing entire training data afford parse training data grammar considered practical data set million word likely afford parse data achieve goal employ approximation notice need calculate actual value objective function need able distinguish applied current hypothesis grammar produce grammar higher score objective function need able calculate difference objective function resulting approximate probability training data change applied possible approximate probability training data probability single probable parse viterbi parse training data recalculating viterbi parse training data scratch applied use heuristic predict change viterbi parse example consider case training data consists sentencesin figure display viterbi parse data initial hypothesis grammar algorithm let consider adding ruleto initial grammar concomitant rule reasonable heuristic predicting viterbi parse change replace adjacent expand single expands displayed figure actual heuristic use move form analogous heuristic set predicting difference",
        "viterbi parse resulting estimate change probability training data predicted viterbi parse stray great deal actual viterbi parse error accumulate applied minimize effect process training data initial hypothesis grammar parse sentence training data search optimal grammar sentence described search framework use resulting grammar parse second sentence search optimal grammar sentence grammar starting point repeat process parsing sentence best grammar found previous sentence searching best grammar taking account new sentence entire training corpus covered parsing sentence previous sentence processed yield accurate viterbi par search process parse corpus initial hypothesis grammar addition achieve goal parsing sentence section describe parameter grammar probability associated grammar rule set evaluating objective function particular grammar use optimal parameter setting given training data score given grammar achieve searching optimal parameter value expensive approximate optimal value setting parameter based viterbi parse training data parsed rely post pas described refine parameter value rule table parameter set arbitrary small constant value parameter set smoothed frequency reduction viterbi parse data seen remaining symbol set expand possible expansion creating rule form corresponds different specific rule created current number symbol grammar expensive consider rule point search use heuristic constrain move appraised left hand rule create new symbol heuristic selects optimal choice vast majority time constraint move described section yield arbitrary context free language address add movemove create rule formwith iteration construct grammar generate arbitrary regular language implemented move enable construction arbitrary context free grammar belongs future work constrain symbol consider right hand new rule use trigger trigger phenomenon viterbi parse sentence indicative particular lead better grammar example figure fact symbol occur indicative create rule developed set trigger set consider specific triggered sentence parsed incremental processing conspicuous shortcoming search framework grammar search space unexpressive recall grammar model sentence sequence",
        "generated symbol language large dependence adjacent constituent free parameter search parameter symbol fixed expand choice necessary search tractable address issue use outside algorithm post pas methodology derived described lari create new nonterminal symbol create rule form denotes set nonterminal symbol acquired initial grammar induction phase taken new sentential symbol new rule replace rule listed table parameter rule initialized grammar starting point run outside algorithm training data convergence word naive rule attach symbol parsing data use rule depend outside algorithm train initialized rule post pas allows express dependency adjacent symbol addition allows train parameter fixed initial grammar induction phase",
        "consider task calculating objective function grammar inexpensive calculating requires parsing entire training data afford parse training data grammar considered practical data set million word likely afford parse data achieve goal employ approximation notice need calculate actual value objective function need able distinguish applied current hypothesis grammar produce grammar higher score objective function need able calculate difference objective function resulting approximate probability training data change applied possible approximate probability training data probability single probable parse viterbi parse training data recalculating viterbi parse training data scratch applied use heuristic predict change viterbi parse example consider case training data consists sentencesin figure display viterbi parse data initial hypothesis grammar algorithm let consider adding ruleto initial grammar concomitant rule reasonable heuristic predicting viterbi parse change replace adjacent expand single expands displayed figure actual heuristic use move form analogous heuristic set predicting difference viterbi parse resulting estimate change probability training data predicted viterbi parse stray great deal actual viterbi parse error accumulate applied minimize effect process training data initial hypothesis grammar parse sentence training data search optimal grammar sentence described search framework use resulting grammar parse second sentence search optimal grammar sentence grammar starting point repeat process parsing sentence best grammar found previous sentence searching best grammar taking account new sentence entire training corpus covered parsing sentence previous sentence processed yield accurate viterbi par search process parse corpus initial hypothesis grammar addition achieve goal parsing sentence",
        "section describe parameter grammar probability associated grammar rule set evaluating objective function particular grammar use optimal parameter setting given training data score given grammar achieve searching optimal parameter value expensive approximate optimal value setting parameter based viterbi parse training data parsed rely post pas described refine parameter value rule table parameter set arbitrary small constant value parameter set smoothed frequency reduction viterbi parse data seen remaining symbol set expand possible expansion",
        "consider creating rule form corresponds different specific rule created current number symbol grammar expensive consider rule point search use heuristic constrain move appraised left hand rule create new symbol heuristic selects optimal choice vast majority time constraint move described section yield arbitrary context free language address add movemove create rule formwith iteration construct grammar generate arbitrary regular language implemented move enable construction arbitrary context free grammar belongs future work constrain symbol consider right hand new rule use trigger trigger phenomenon viterbi parse sentence indicative particular lead better grammar example figure fact symbol occur indicative create rule developed set trigger set consider specific triggered sentence parsed incremental processing",
        "conspicuous shortcoming search framework grammar search space unexpressive recall grammar model sentence sequence generated symbol language large dependence adjacent constituent free parameter search parameter symbol fixed expand choice necessary search tractable address issue use outside algorithm post pas methodology derived described lari create new nonterminal symbol create rule form denotes set nonterminal symbol acquired initial grammar induction phase taken new sentential symbol new rule replace rule listed table parameter rule initialized grammar starting point run outside algorithm training data convergence word naive rule attach symbol parsing data use rule depend outside algorithm train initialized rule post pas allows express dependency adjacent symbol addition allows train parameter fixed initial grammar induction phase",
        "mentioned work employ bayes grammar induction framework described solomonoff solomonoff solomonoff specify concrete search algorithm make suggestion nature research includes work cook stolcke omohundro work employ heuristic search bayes framework different prior probability grammar algorithm efficient applied small data set grammar induction algorithm successful language modeling include inside outside algorithm lari young lari young pereira schabes special case expectation maximization algorithm work mccandless work mccandless us heuristic search procedure similar different search criterion knowledge algorithm surpassed performance n gram model language modeling task substantial scale",
        "evaluate algorithm compare performance algorithm n gram model inside outside algorithm n gram model tried domain smoothing particular n gram model took linear combination lower order n gram model particular follow standard practice mercer bahl brown smoothed gram probability linear combination gram frequency training data smoothed gram probability denotes count word sequence training data smoothing parameter trained backward algorithm baum eagon held data tied similar prevent data sparsity inside outside algorithm follow methodology described lari young given create probabilistic context free grammar consisting chomsky normal form rule nonterminal symbol given terminal symbol ruleswhere denotes set terminal symbol domain parameter initialized starting point inside outside algorithm run convergence smoothing combine expansion distribution symbol uniform distribution smoothed parameter denotes unsmoothed parameter value number different way symbol expands lari young methodology parameter trained inside outside algorithm held data smoothing performed outside post pas algorithm domain tried computational demand algorithm impractical apply large vocabulary large training set problem present result algorithm medium sized domain case use sentence training sentence held smoothing test sentence measure performance entropy test data domain created training test data ideal grammar hand benchmark result particular probabilistic grammar generate data domain created grammar hand grammar small english like probabilistic context free grammar consisting nonterminal symbol terminal symbol rule second domain derived grammar parsed text word parsed wall street journal data penn treebank extracted occurring symbol occurring rule expanding symbol symbol occurs right hand rule frequent symbol create rule expands symbol unique terminal symbol removing unreachable rule yield grammar nonterminals terminal rule set reflect frequency corresponding rule parsed corpus domain took english text reduced size vocabulary mapping word speech tag tagged wall street journal text penn treebank tag set size table summarize result ideal grammar denotes grammar generate training test data algorithm list best performance achieved tried best column state value realized performance achieve moderate significant improvement performance n gram model inside outside algorithm domain speech domain outperformed",
        "n gram model outperform outside algorithm table display sample number parameter execution time decstation associated algorithm choose yield equivalent performance algorithm pas row refers main grammar induction phase algorithm post pas row refers outside post pas algorithm produce compact model n gram model running inside outside algorithm use outside post pas discrepancy fact require smaller number new nonterminal symbol achieve equivalent performance found post pas converges given number nonterminal symbol",
        "algorithm outperformed outside algorithm experiment attribute difference bayes maximum likelihood objective function believe difference result effective search strategy particular algorithm employ greedy hill climbing strategy algorithm gain advantage able add new rule grammar outside algorithm gradient descent search discovers nearest local minimum search landscape initial grammar rule grammar parameter search take place fixed k dimensional space algorithm possible expand hypothesis grammar increasing dimensionality parameter space searched apparent local minimum space local minimum space extra dimension provide pathway improvement hypothesis grammar algorithm prone suboptimal local minimum inside outside algorithm n gram model domain demonstrates algorithm able advantage grammatical structure present data superiority n gram model speech domain indicates competitive modeling occurring data necessary model collocational information need modify algorithm model n gram information",
        "research represents step quest developing grammar based language model natural language induce model compact outperform n gram language model medium sized domain algorithm run time space linear size training data larger domain reach feel largest contribution work lie actual algorithm specified indication potential induction framework described solomonoff implemented subset move developed inspection result give reason believe additional move improve performance algorithm induction framework restricted probabilistic context free grammar completing implementation set plan explore modeling context sensitive phenomenon work demonstrates solomonoff elegant framework deserves consideration",
        "acknowledged account natural language utterance given term syntactic semantic phenomenon example hirschberg shown order understand scalar implicature analyze conversants belief intention recognize normal state implicatures consider mutual belief plan green understand conversational implicatures associated indirect reply consider discourse expectation discourse plan discourse relation green green carberry presupposition inferrable certain lexical construct factives aspectuals etc syntactic construct cleft pseudo cleft sentence complexity individualize recognition stage inference defeated context knowledge belief plan agent constitute context pragmatic rule notion tricky deal scholar logic pragmatic learned circumvent observer phenomenon preferred defeasibility mathematical world frege russell quine exists logical system impossible formalize cancellation presupposition definite referent exist hirst marcu hirst taxonomize previous approach defeasible pragmatic inference category omit work defeasibility related linguistic phenomenon discourse anaphora speech act linguistic approach account defeasibility pragmatic inference analyzing context consists previous utterance including current karttunen kay procedural rule gazdar karttunen peter lexical syntactic structure weischedel intention anaphoric constraint sandt zeevat decide presupposition implicatures projected pragmatic inference utterance analyzed problem approach assign dual life pragmatic inference initial stage member simple complex utterance defeasible utterance analyzed possibility left cancelling inference natural implicatures presupposition inferred cancelled sequence utterance proceeds research conversation repair hirst abounds example address issue detail section way accounting cancellation occur analyzed text extend boundary pragmatic inference evaluated look utterance assumes implicatures connected discourse entity utterance approach allow cancellation discourse unit way allowing pragmatic inference cancelled assign status defeasible information formalizes presupposition logical framework handle default reiter approach tractable treat natural disjunction exclusive implication logical equivalence approach fail account cancellation pragmatic inference presupposition weischedel implicatures green generated cancelled aware formalism computational approach offer unified explanation cancellability pragmatic inference general approach handle cancellation occur sequence utterance aim provide approach assume existence type pragmatic inference set necessary condition true order inference triggered set condition met corresponding inference drawn assigned defeasible status",
        "role context knowledge conversants decide inference survive pragmatic inference structure boundary time cancellation occur offer unified explanation pragmatic inference inferable simple utterance complex utterance sequence utterance considered propose new formalism called stratified logic handle pragmatic inference start giving brief introduction main idea underlie main step algorithm defined backbone stratified logic different class pragmatic inference captured formalism algorithm computes expected result representative class pragmatic inference result report obtained implementation written common lisp us screamer siskind mcallester macro package provides nondeterministic construct",
        "offer brief overview stratified logic reader referred marcu comprehensive study logic support type indefeasible information type defeasible information defeasible defeasible notion defeasible information meant capture inference anomalous cancel notion defeasible information meant capture inference cancelled abnormality lattice figure underlies semantics stratified logic lattice depicts level strength account inference pertain natural language semantics pragmatic indefeasible information belongs layer defeasible information belongs layer defeasible information belongs layer layer partitioned according polarity truth falsity lattice show partial order defined different level truth example stronger sense defined infelicitously true false level stronger level stronger level syntactic level allow atomic formula labelled according underlying lattice formula obtained usual way formula satisfaction relation split according level truth u satisfaction satisfaction d satisfaction definition extends natural way negated compound formula satisfaction definition associated level strength provides high degree flexibility theory interpreted perspective allows freedom u satisfaction perspective tighter signal defeasible information cancelled d satisfaction interpretation given set utterance respect knowledge base computed extension semantic tableau method extension proved sound complete marcu partial ordering determines set optimistic interpretation theory interpretation preferred optimistic interpretation contains information information updated future mean interpretation make utterance true assigning relation defeasible status interpretation make utterance true assigning relation stronger status preferred optimistic informative allows option future defeated inference triggered utterance differentiate semantic inference introduce new quantifier semantics defined pragmatic inference form instantiated object universe discourse pertain utterance having form antecedent pragmatic rule uttered rule applied meta logical construct uttered applies logical translation utterance theory yield following definition algorithm described detail marcu take input set order stratified formula represents adequate knowledge base express semantic knowledge necessary condition triggering pragmatic inference translation utterance set utterance uttered algorithm build set possible interpretation given utterance generalization semantic tableau technique model ordering relation filter optimistic interpretation defeasible inference triggered pragmatic ground checked cancelled optimistic interpretation cancelled labelled pragmatic inference given utterance set utterance",
        "offer brief overview stratified logic reader referred marcu comprehensive study logic support type indefeasible information type defeasible information defeasible defeasible notion defeasible information meant capture inference anomalous cancel notion defeasible information meant capture inference cancelled abnormality lattice figure underlies semantics stratified logic lattice depicts level strength account inference pertain natural language semantics pragmatic indefeasible information belongs layer defeasible information belongs layer defeasible information belongs layer layer partitioned according polarity truth falsity lattice show partial order defined different level truth example stronger sense defined infelicitously true false level stronger level stronger level syntactic level allow atomic formula labelled according underlying lattice formula obtained usual way formula satisfaction relation split according level truth u satisfaction satisfaction d satisfaction definition extends natural way negated compound formula satisfaction definition associated level strength provides high degree flexibility theory interpreted perspective allows freedom u satisfaction perspective tighter signal defeasible information cancelled d satisfaction interpretation given set utterance respect knowledge base computed extension semantic tableau method extension proved sound complete marcu partial ordering determines set optimistic interpretation theory interpretation preferred optimistic interpretation contains information information updated future mean interpretation make utterance true assigning relation defeasible status interpretation make utterance true assigning relation stronger status preferred optimistic informative allows option future defeated inference triggered utterance differentiate semantic inference introduce new quantifier semantics defined pragmatic inference form instantiated object universe discourse pertain utterance having form antecedent pragmatic rule uttered rule applied meta logical construct uttered applies logical translation utterance theory yield following definition",
        "present set example cover representative group pragmatic inference contrast approach provide consistent methodology computing inference determining cancelled possible configuration simple complex utterance sequence utterance factive verb regret presupposes complement seen positive environment presupposition stronger acceptable defeat presupposition triggered negative environment infelicitous defeat belongs positive environment appropriate formalization utterance requisite pragmatic knowledge shown stratified semantic tableau corresponds theory given figure tableau yield model schema figure inferred mary came party model ordering relation establishes optimistic model theory contains information easier defeat explains mary came party presupposition utterance utterance implicatures appropriate formalization given second formula capture defeasible scalar implicatures formula reflects relevant semantic information theory provides optimistic model schema figure reflects expected pragmatic inference boy went theatre moment thought person utters adding extra utterance initial theory uttered went boy theatre obtain optimistic model schema conventional implicatures cancelled figure achilles heel theory presupposition vulnerability projection problem solution projection problem differ solution individual utterance following utterance associated presupposition symbol precedes inference drawn pragmatic ground chris bachelor presupposes chris male adult chris regret mary came party presupposes mary came party contradiction presupposition expect conversant infer hears utterance examines utterance observes contradiction presupposition carried individual component bachelor presupposes chris male spinster presupposes chris female expect conversant notice contradiction drop elementary presupposition interprets study stratified logic model ordering relation capture intuition appropriate formalization utterance necessary semantic pragmatic knowledge given translation utterance initial theory contains formalization defeasible implicature natural disjunction exclusive knowledge mary male lexical semantics word bachelor lexical pragmatic bachelor regret stratified semantic tableau generates model schema kept optimistic model utterance model yield mary came party chris male chris adult pragmatic inference utterance utterance stratified semantic tableau corresponds logical theory yield model chris adult satisfies definition projected presupposition utterance mentioned speech repair constitute good benchmark studying generation cancellation pragmatic inference sequence utterance mcroy hirst",
        "example jane friend john smith john pevler roommate mary met john smith married fellow jane conversation mary jane mention john aware mary know john year old boy context natural mary confused come wrong conclusion example mary reply john bachelor true john appropriate married fellow year old boy know john smith married male utterance make sense point jane realizes mary misunderstands time jane talking john pevler year old boy utterance constitute possible answer jane mary order clarify problem utterance sequence presupposes warns mary misunderstood previous utterance warning conveyed implicature point hearer mary start believe previous utterance elaborated false assumption know utterance come clarify issue express john adult cancel early presupposition note gap statement generation cancellation presupposition behavior described mirrored theory program methodology applied modeling conversational implicatures indirect reply green algorithm make use discourse expectation discourse plan discourse relation following dialog considered green answer conveys yes reply consisting implicate green notice previous model implicatures gazdar processing block implicature generated solves problem extending boundary analysis discourse unit approach exhibit constraint previous example dealing sequence utterance obtain different interpretation step question asked conversational implicature make necessary condition implicating true implication computed reinforces previous condition make precondition implicating false precondition implicating yes true implicature end dialogue conversant answered went shopping",
        "factive verb regret presupposes complement seen positive environment presupposition stronger acceptable defeat presupposition triggered negative environment infelicitous defeat belongs positive environment appropriate formalization utterance requisite pragmatic knowledge shown stratified semantic tableau corresponds theory given figure tableau yield model schema figure inferred mary came party model ordering relation establishes optimistic model theory contains information easier defeat explains mary came party presupposition utterance utterance implicatures appropriate formalization given second formula capture defeasible scalar implicatures formula reflects relevant semantic information theory provides optimistic model schema figure reflects expected pragmatic inference boy went theatre moment thought person utters adding extra utterance initial theory uttered went boy theatre obtain optimistic model schema conventional implicatures cancelled figure",
        "achilles heel theory presupposition vulnerability projection problem solution projection problem differ solution individual utterance following utterance associated presupposition symbol precedes inference drawn pragmatic ground chris bachelor presupposes chris male adult chris regret mary came party presupposes mary came party contradiction presupposition expect conversant infer hears utterance examines utterance observes contradiction presupposition carried individual component bachelor presupposes chris male spinster presupposes chris female expect conversant notice contradiction drop elementary presupposition interprets study stratified logic model ordering relation capture intuition appropriate formalization utterance necessary semantic pragmatic knowledge given translation utterance initial theory contains formalization defeasible implicature natural disjunction exclusive knowledge mary male lexical semantics word bachelor lexical pragmatic bachelor regret stratified semantic tableau generates model schema kept optimistic model utterance model yield mary came party chris male chris adult pragmatic inference utterance utterance stratified semantic tableau corresponds logical theory yield model chris adult satisfies definition projected presupposition utterance",
        "mentioned speech repair constitute good benchmark studying generation cancellation pragmatic inference sequence utterance mcroy hirst example jane friend john smith john pevler roommate mary met john smith married fellow jane conversation mary jane mention john aware mary know john year old boy context natural mary confused come wrong conclusion example mary reply john bachelor true john appropriate married fellow year old boy know john smith married male utterance make sense point jane realizes mary misunderstands time jane talking john pevler year old boy utterance constitute possible answer jane mary order clarify problem utterance sequence presupposes warns mary misunderstood previous utterance warning conveyed implicature point hearer mary start believe previous utterance elaborated false assumption know utterance come clarify issue express john adult cancel early presupposition note gap statement generation cancellation presupposition behavior described mirrored theory program",
        "methodology applied modeling conversational implicatures indirect reply green algorithm make use discourse expectation discourse plan discourse relation following dialog considered green answer conveys yes reply consisting implicate green notice previous model implicatures gazdar processing block implicature generated solves problem extending boundary analysis discourse unit approach exhibit constraint previous example dealing sequence utterance obtain different interpretation step question asked conversational implicature make necessary condition implicating true implication computed reinforces previous condition make precondition implicating false precondition implicating yes true implicature end dialogue conversant answered went shopping",
        "research pragmatic focus certain type presupposition implicatures provide global framework express type pragmatic inference pragmatic inference associated set necessary condition trigger inference set condition met inference drawn assigned defeasible status extended definition satisfaction notion optimism respect different interpretation yield preferred interpretation utterance sequence utterance interpretation contain pragmatic inference cancelled context conversant knowledge plan intention formalism yield algorithm implemented common lisp screamer algorithm computes pragmatic inference associated simple complex utterance sequence utterance allows cancellation pragmatic inference occur time discourse",
        "large body psycholinguistic evidence suggests meaning extracted end sentence end phrasal constituent marslen wilson tanenhaus recent evidence suggesting speech processing partial interpretation built word completed spivey knowlton potential computational application incremental interpretation including early parse filtering statistic based logical form plausibility interpretation fragment dialogue survey provided milward cooper referred current computational psycholinguistic literature main approach incremental construction logical form approach use grammar non standard constituency initial fragment sentence john like treated constituent assigned type semantics approach exemplified combinatory categorial grammar ccg steedman take basic application add new way combining element interpretation achieved standard shift reduce parser working left sentence alternative approach exemplified work stabler parsing stabler pulman left corner parsing pulman associate semantics partial structure formed left corner parse example syntax tree missing noun phrase followingcan given semantics function entity truth value like john having john like constituent approach problem grammar augmented operation powerful initial fragment constituent unwanted interaction rest grammar example case ccg lambek calculus given section addition extra operation mean given reading sentence different possible derivation called spurious ambiguity making simple parsing strategy shift reduce inefficient limitation parsing approach evident consider grammar left recursion case simple parser incomplete left corner parser resort buffering input wo word word illustrate problem considering fragment mary think john small number possible semantic representation exact number depending grammar second representation appropriate sentence finish sentential modifier allows verb phrase modifier semantic representation read syntactic structure parser provide single syntax tree node number syntax tree corresponding example semantic representation following tree suitable sentence mary think john shave mary think john coming mistake suggest possibility packing partial syntax tree including tree adjoining grammar joshi description theory marcus possibility choose single syntax tree use destructive tree operation parse approach adopt based milward milward syntax tree regarded performing main role provide syntactic information guide rest sentence integrated tree second provide basis semantic representation role",
        "captured syntactic type type corresponds infinite number partial syntax tree second role captured parser constructing semantic representation general processing model consists transition form provides state transition dynamic model processing state pair syntactic type semantic value main difference approach milward milward based expressive grammar formalism applicative categorial grammar opposed lexicalised dependency grammar categorial grammar allow category argument function treated function function given type n n adjectival modifier ability deal function function advantage enabling elegant linguistic description providing kind parsing parser fails word final word function constituent formed corresponding problem greater non determinism unambiguous word allowing possible transition crucial perform kind ambiguity packing language tuning discussed final section paper",
        "applicative categorial grammar basic form categorial grammar single combination rule corresponding function application applied linguistic description adjukiewicz bar hillel linguistic description bouma van noord overshadowed recent year hpsg pollard sag lambek categorial grammar lambek worth giving brief indication fit development directed applicative proposed bar hillel type included list argument left list argument right bar hillel notation feature based notation similar hpsg pollard sag obtain following category ditransitive verb list argument left gathered feature right order feature hillel employed single application rule corresponds following result system come formalised dependency grammar gaifman hay real difference bar hillel allowed argument function example adverb given typean unfortunate aspect bar hillel system application rule resulted primitive type argument functional type correspond single lexical item way form type non lexical verb phrase like mary adapting application rule allow function applied argument time bar hillel second system called categorial grammar adjukiewicz bar hillel bar hillel adopted curried notation adopted cgs represent function requires left right choice following type curried notation cgs choose structure include rule associativity mean type interchangeable lambek calculus associativity consequence calculus specified main impetus change applicative came work ade steedman steedman noted use function composition allows cgs deal unbounded dependency construction composition enables function applied argument argument incomplete allows peripheral extraction gap start end relative clause composition rule proposed order deal non peripheral extraction led unwanted effect grammar bouma treatment non peripheral extraction based lambek calculus standard composition built rule proven calculus introduced alternative forward backward slash normal args wh args moortgat introduced called modal operator wh argument morrill technique thought marking wh argument requiring special treatment lead unwanted effect grammar problem having composition basic non applicative operation cgs contain function function addition composition add new analysis sentence new string language fact composition form function argument function function example type composed type modified adjectival modifier type",
        "noun old dilapidated car unacceptable bracketing old dilapidated car cgs composition lambek calculus allow string boy given type predicting boy car acceptable noun individual example possible rule appropriate feature difficult general whilst retaining calculus suitable incremental interpretation wh argument need treated deal non peripheral extraction composition general rule problematic suggests return grammar use application general operation special treatment wh argument non curried notation bar hillel natural use separate wh list mark wh argument example category appropriate relative clause noun phrase gap possible specify operation act applicative operation respect left right argument list composition respect wh list similar way wh movement dealt gpsg gazdar hpsg wh argument treated slash mechanism feature inheritance principle correspond function composition argument produced categorial grammar look similar hpsg use hpsg applicative main reason applicative simpler formalism given simple syntax semantics interface function application syntax mapping function application semantics turn make easy provide proof soundness completeness incremental parsing algorithm technique developed able extended complex formalism hpsg",
        "section define grammar similar bar hillel grammar bar hillel allow argument absorbed time resulting grammar equivalent categorial grammar associativity category grammar defined follows syntactic type category syntactic type list category category right defined rule application left defined rule basic grammar provides spurious derivation sentence john like mary bracketed john like mary john like mary spurious derivation translate spurious ambiguity parser map string word semantic representation",
        "parser work left input string described term state transition rule current parsing state stack category chart transformed word new state explicit parser described term rule state new word create new state unusual feature equivalent stack mechanism time state characterised single syntactic type single semantic value stack semantic value syntax tree waiting connected transition state occur input new word transition reduce step shift reduce parser rule given figure difficult understand general form work upto rule considering kind rule need particular instance following pairing sentence fragment simplest possible type consider taking type description state parser absorbing fragment obtain sequence transition follows embedded sentence john like sue mapping suggests treat sentence mapping category expecting category noun phrase treated mapping consider individual transition simplest type argument expected state matched word generalised following rule similar function application standard cga similar transition occurs like expected like provides requires right form like absorbed state category need expect rule required similar function composition term tree structure happening replacement node partial tree second partial tree rule specified need generalised allow case lexical item argument replace like di transitive give tri transitive bet trivial non curried notation similar aacg obtain single rule state application corresponds application list argument function composition length n ary composition length change needed aacg notation inclusion extra feature list list store information argument waiting head reason explained lexicon identical standard aacg having h list set consider transition sentence expected encountered noun phrase john appropriate rule notation rule state looking look missing tree structure term rule state prediction obtained generalising allow lexical item missing argument expected argument missing argument application state prediction provide basis sound complete parser sentence achieved starting state expecting sentence applying rule non word input successful parse achieved final state expects argument example reconsider string john like sue sequence transition corresponding john like sue",
        "sentence given figure transition encountering john deterministic state application apply state prediction instantiated way result new state expecting argument given transition input like non deterministic application apply figure state prediction apply instantiated way correspond different way cutting left right subcategorisation list lexical entry like possibility corresponds prediction modifier prediction modifier verb phrase modifier function take subject verb separate argument fourth corresponds function requires argument interesting given figure choice particular transition point allows verb phrase modification assuming word sue implicit bracketing string fragment john like sue state application chosen state prediction possibility fragment john like sue retains flat structure modification verb phrase verb phrase structure introduced relates spurious ambiguity choice transition semantic consequence choice affect particular semantics modified worth noting necessary use h list needed distinguish case real functional argument function function function formed state prediction following tree node tree syntactic type case want allow modifier lower second headed list distinguishes case having headed list allowing prediction modifier",
        "consider sentence processing opposed incremental processing use lexicalised grammar major advantage use standard rule based grammar processing sentence lexicalised formalism look grammar grammatical information indexed word increase size grammar effect efficiency processing provided increase size addition new word increased lexical ambiguity set possible lexical entry sentence collected required converted set phrase structure rule correspond small subset rule based formalism equivalent lexicalised grammar parsing standard algorithm earley incremental parsing predict word appear sentence use technique base parser rule given gain grammatical information localised sentence localised particular word particular context need consider start sentence occurs end verb entry allows subject major problem noted paragraph nature parsing know word come parser use information word come lexicon particular language example given input np parser create state expecting np left state head final language unlikely state language english incremental interpretation use semantic representation plausible different language practical term naive interactive parallel prolog implementation current workstation fails interactive real sense word needed kind language tuning nature fixed restriction rule english rule us prediction noun phrase encountered exist left list appealing alternative base tuning statistical method achieved running parser corpus provide probability particular transition given particular word transition capture likelihood word having particular speech probability particular transition performed speech early work providing based parsing transition structured syntactic category tugwell simple markov process infinite number state problem sparse data necessary generalisation state example ignoring list processing model serial exploring ranked transition allowing backtracking semantic plausibility current interpretation drop ranked parallel exploring path ranked highest according transition probability semantic plausibility",
        "collaborative planning activity agent autonomous heterogeneous inevitable conflict arise belief planning process case conflict relevant task hand agent engage collaborative negotiation attempt square discrepancy belief paper present computational strategy detecting conflict proposed belief engaging collaborative negotiation resolve conflict warrant resolution model capable selecting effective aspect address pursuit conflict resolution case multiple conflict arise selecting appropriate evidence justify need modification capturing negotiation process recursive propose evaluate modify cycle action model handle embedded negotiation subdialogues",
        "collaborative consultation dialogue consultant executing agent collaborate developing plan achieve executing agent domain goal agent autonomous heterogeneous inevitable conflict belief arise planning process case collaborative agent attempt square joshi conflict engaging collaborative negotiation determine constitute shared plan action shared belief negotiation differs non collaborative negotiation argumentation attitude participant collaborative agent self centered act way benefit agent group facing conflict collaborative agent reject belief agree evaluate belief evidence provided adopt belief evidence convincing hand evaluation indicates agent maintain original belief attempt provide sufficient justification convince agent adopt belief belief relevant task hand paper present model engaging collaborative negotiation resolve conflict agent belief domain knowledge modeldetects conflict belief initiate negotiation subdialogue conflict relevant current task selects effective aspect address pursuit conflict resolution multiple conflict exist selects appropriate evidence justify system proposed modification user belief andcaptures negotiation process recursive propose evaluate modify cycle action enabling system handle embedded negotiation subdialogues",
        "researcher studied analysis generation argument reichman cohen sycara quilici maybury agent engaging argumentative dialogue interested winning argument exhibit different behavior collaborative agent sidner formulated artificial language modeling collaborative discourse proposal acceptance proposal rejection sequence work descriptive specify response generation strategy agent involved collaborative interaction joshi noted importance cooperative system providing support response identified strategy system adopt justifying belief specify criterion strategy selected described method determining include optional warrant justify claim based factor communication cost inference cost cost memory retrieval model focus determining include redundant utterance model determines justification needed claim convincing selects appropriate evidence system private belief support claim logan introduced idea utilizing belief revision mechanism galliers predict set evidence sufficient change user existing belief generate response information retrieval dialogue library domain argued library dialogue analyzed case negotiation extend initial belief conflict immediate resolution logan analysis occurring consultation dialogue columbia university transcript sri transcript show domain conflict resolution extend single exchange conflicting belief employ recursive model collaboration capture extended negotiation represents structure discourse system deal single conflict model selects focus pursuit conflict resolution multiple conflict arise addition provide process selecting multiple possible piece evidence",
        "collaborative negotiation occurs conflict arise agent developing shared plan collaborative planning collaborative agent driven goal developing plan satisfies interest agent group maximizes interest result distinctive feature collaborative negotiation collaborative agent insist winning argument change belief agent present convincing justification opposing belief differentiates collaborative negotiation argumentation reichman cohen quilici involved collaborative negotiation open honest present false information agent present information way mislead agent hold information agent use distinguishes collaborative negotiation non collaborative negotiation labor negotiation sycara agent interested belief order decide revise belief come agreement chu carroll carberry agent involved argumentation non collaborative negotiation agent belief consideration find weak point opponent belief attack win argument earlier work built sidner proposal acceptance proposal rejection sequence sidner developed model capture collaborative planning process propose evaluate modify cycle action chu carroll carberry model view collaborative planning agent proposing set action belief incorporated shared plan developed agent evaluating proposal determine accepts proposal agent proposing set modification original proposal proposed modification evaluated conflict arise propose modification proposed modification resulting recursive process research specify case multiple conflict arise agent identify unaccepted proposal address select evidence support proposed modification paper extends work incorporating modification process strategy determine aspect proposal agent address pursuit conflict resolution mean selecting appropriate evidence justify need modification",
        "order capture agent intention conveyed utterance model collaborative negotiation utilizes enhanced version dialogue model described lambert carberry represent current status interaction enhanced dialogue model level domain level consists domain plan constructed user execution problem solving level contains action performed construct domain plan belief level consists mutual belief pursued planning process order problem solving intention discourse level contains communicative action initiated achieve mutual belief chu carroll carberry paper focus evaluation modification proposed belief detail strategy engaging collaborative negotiation system maintains set belief domain user belief belief strength represents agent confidence holding belief model strength belief endorsement explicit record factor affect certainty hypothesis cohen following galliers logan endorsement based semantics utterance convey belief level expertise agent conveying belief stereotypical knowledge etc belief level dialogue model consists mutual belief proposed agent discourse action agent proposes new belief give optional supporting evidence set proposed belief represented belief tree belief represented child node intended support represented parent root node belief tree level belief contribute problem solving action affect domain plan developed set proposed belief system decide accept proposal initiate negotiation dialogue resolve conflict evaluation proposed belief start leaf node proposed belief tree acceptance piece proposed evidence affect acceptance parent belief intended support process continues level proposed belief evaluated resolution strategy invoked level proposed belief accepted collaborative agent agree belief relevant domain plan constructed irrelevant agree evidence belief young determining accept proposed belief evidential relationship evaluator construct evidence set containing system evidence support attack bel evidence accepted system proposed user support bel piece evidence contains belief bel evidential relationship walker weakest link assumption strength evidence weaker strength belief strength evidential relationship evaluator employ simplified version galliers belief revision mechanism logan compare strength evidence support attack bel strength set evidence outweighs decision accept reject bel difference strength exceed pre determined threshold evaluator insufficient information determine adopt bel initiate information sharing subdialogue chu carroll carberry share information user evaluate user original proposal information sharing user provides convincing support belief negation held system system adopt",
        "belief evaluation process resolving conflict negotiation illustrate evaluation proposed belief consider following utterance figure show belief discourse level dialogue model capture utterance belief evaluation process start belief leaf node proposed belief tree year system gather evidence pertaining belief includesa warranted belief dr smith postponed sabbatical warranted belief dr smith postponing sabbatical support belief going sabbatical year year strong belief dr smith visitor ibm year ibm year anda warranted belief dr smith visitor ibm year support belief going sabbatical year support ibm year year dr smith expressed desire spend sabbatical ibm belief revision mechanism invoked determine system belief year based system evidence user statement belief constitute warranted piece evidence proposed belief belief constitute strong piece evidence system accept year system belief sabbatical implies faculty member teaching course proposed evidential relationship accepted system accept level proposed belief ai system prior belief contrary expressed utterance evidence provided user implication antecedent accepted collaborative planning principle whittaker stenton walker suggests conversants provide evidence detected discrepancy belief possible agent detects relevant conflict notify agent conflict initiate negotiation subdialogue resolve fail responsibility collaborative agent capture attempt resolve conflict problem solving action modify proposal goal modify proposal form accepted agent applied belief modification modify proposal specialization correct node proposed belief accepted correct relation proposed evidential relationship accepted show problem solving recipe correct node subaction modify node responsible actual modification proposal applicability condition correct node specify action invoked belief node acceptable belief disagree proposed belief represented node collaborative interaction actual modification performed believe node acceptable conflict resolved captured applicability condition precondition modify node attempt satisfy precondition cause system post mutual belief achieved belief node acceptable leading system adopt discourse action change belief initiating collaborative negotiation subdialogue multiple conflict arise system user user proposal system identify aspect proposal focus pursuit conflict resolution example case correct node selected specialization",
        "modify proposal system determine parameter node correct node instantiated goal modification process resolve agent conflict unaccepted level proposed belief belief system provide evidence belief address unaccepted evidence proposed user eliminate user justification belief collaborative agent expected engage effective efficient dialogue system address unaccepted belief predicts resolve level conflict unaccepted level belief process selecting focus modification involves step identifying candidate focus tree proposed belief tree selecting focus candidate focus tree heuristic attack belief resolve conflict level belief candidate focus tree contains piece evidence proposed belief tree disbelieved user change user view unaccepted level proposed belief root node belief tree identified performing depth search proposed belief tree node visited belief evidential relationship parent examined belief relationship accepted evaluator search current branch terminate system accepts belief irrelevant accepts user support belief young piece evidence included candidate focus tree system continue search evidence belief tree proposed support unaccepted belief evidential relationship candidate focus tree identified system select focus modification based likelihood choice changing user belief level belief show algorithm selection process unaccepted belief bel belief proposed support select focus modification annotate bel withits focus modification bel focus contains set belief bel descendent disbelieved user predicted cause disbelieve bel andthe system evidence bel bel s attack focus modification determines attack supporting evidence eliminating user reason holding bel attack bel evaluating effectiveness attacking proposed evidence bel system determine possible refute piece evidence system belief sufficient evidence available convince user piece proposed evidence invalid effective attack evidence support algorithm applies evidence proposed support bel accepted system step recursive process algorithm annotates unaccepted belief evidential relationship proposed support bel focus modification bel system evidence bel attack contains belief selected addressed order change user belief bel value nil system predicts insufficient evidence available change user belief bel information obtained step select focus modification decides attack evidence proposed support bel bel step preference address unaccepted evidence mckeown focusing rule suggest continuing introduced topic",
        "said preferable returning previous topic mckeown algorithm considers attacking user support sufficient convince bel step gathering cand set evidence proposed user direct support bel accepted system system predicts refute bel nil algorithm hypothesizes user changed mind belief cand set predicts affect user belief bel step user predicted accept bel hypothesis algorithm invokes select min set select minimum subset cand set unaccepted belief pursue focus modification union focus belief minimum subset attacking evidence bel appear sufficient convince user bel algorithm check attacking bel accomplish goal providing evidence bel predicted successful focus modification bel step attacking bel predicted fail algorithm considers effect attacking bel unaccepted proposed evidence combining previous prediction process step combined evidence predicted fail system sufficient evidence change user view bel focus modification bel nil step step algorithm invoke function predict make use belief revision mechanism galliers discussed section predict user acceptance unacceptance based system knowledge user belief evidence presented logan result select focus modification set user belief need modified order change user belief unaccepted level belief negation belief posted system mutual belief achieved order perform modify action communication social psychology shown evidence improves persuasiveness message luchok mccroskey reynolds burgoon petty cacioppo hample quantity evidence indicates optimal evidence use high quality evidence consistent persuasive effect reinard hand grice maxim quantity specifies contribute information required important collaborative agent selects sufficient effective excessive evidence justify intended mutual belief convince user belief bel system selects appropriate justification identifying belief support bel applying filtering heuristic system determine justification bel needed predicting informing user bel sufficient convince bel justification presented justification predicted necessary system construct justification chain support bel piece evidence support bel system predicts user accept evidence justification user predicted accept piece evidence evid system augment evidence presented user posting evid mutual belief achieved selecting proposition serve justification result recursive",
        "process return chain belief justification support bel set belief forming justification chain identified system select set belief chain presented user predicted convince user bel system construct singleton set justification chain select set containing justification presented predicted convince user bel single justification chain predicted sufficient change user belief new set constructed combining single justification chain selection process repeated produce set possible candidate justification chain heuristic applied select heuristic prefers evidence system confident high quality evidence produce attitude change evidence form luchok mccroskey system justify belief high confidence user accept second heuristic prefers evidence novel user study shown evidence persuasive unknown hearer wyer morley heuristic based grice maxim quantity prefers justification chain contain fewest belief evaluation dialogue model figure modify proposal invoked level proposed belief accepted selecting focus modification system identify candidate focus tree invoke select focus modification algorithm belief root node candidate focus tree candidate focus tree identical proposed belief tree figure level proposed belief proposed evidence rejected evaluation process indicates focus modification ai year evidential relationship accepted select focus modification applied ai algorithm invoked year determine focus modifying child belief step figure system piece evidence year warranted piece evidence containing postponed year anda strong piece evidence containing ibm year support ibm year year evidence predicted sufficient change user belief year ai focus modification year correct node specialization modify proposal invoked focus modification belief order satisfy precondition modify u year posted mutual belief achieved user warranted belief year indicated semantic form utterance system predict informing user intended mutual belief sufficient change belief select justification available piece evidence supporting year presented system predict piece evidence combined proposed mutual belief sufficient change user belief filtering heuristic applied heuristic cause system select postponed year support evidence system confident system try establish mutual belief attempt satisfy precondition modify node cause system invoke inform discourse action generate following utterance user accepts system utterance satisfying precondition",
        "system maintains set belief domain user belief belief strength represents agent confidence holding belief model strength belief endorsement explicit record factor affect certainty hypothesis cohen following galliers logan endorsement based semantics utterance convey belief level expertise agent conveying belief stereotypical knowledge etc belief level dialogue model consists mutual belief proposed agent discourse action agent proposes new belief give optional supporting evidence set proposed belief represented belief tree belief represented child node intended support represented parent root node belief tree level belief contribute problem solving action affect domain plan developed set proposed belief system decide accept proposal initiate negotiation dialogue resolve conflict evaluation proposed belief start leaf node proposed belief tree acceptance piece proposed evidence affect acceptance parent belief intended support process continues level proposed belief evaluated resolution strategy invoked level proposed belief accepted collaborative agent agree belief relevant domain plan constructed irrelevant agree evidence belief young determining accept proposed belief evidential relationship evaluator construct evidence set containing system evidence support attack bel evidence accepted system proposed user support bel piece evidence contains belief bel evidential relationship walker weakest link assumption strength evidence weaker strength belief strength evidential relationship evaluator employ simplified version galliers belief revision mechanism logan compare strength evidence support attack bel strength set evidence outweighs decision accept reject bel difference strength exceed pre determined threshold evaluator insufficient information determine adopt bel initiate information sharing subdialogue chu carroll carberry share information user evaluate user original proposal information sharing user provides convincing support belief negation held system system adopt belief evaluation process resolving conflict negotiation illustrate evaluation proposed belief consider following utterance figure show belief discourse level dialogue model capture utterance belief evaluation process start belief leaf node proposed belief tree year system gather evidence pertaining belief includesa warranted belief dr smith postponed sabbatical warranted belief dr smith postponing sabbatical support belief going sabbatical year year strong belief dr smith visitor ibm year ibm year anda warranted belief dr",
        "smith visitor ibm year support belief going sabbatical year support ibm year year dr smith expressed desire spend sabbatical ibm belief revision mechanism invoked determine system belief year based system evidence user statement belief constitute warranted piece evidence proposed belief belief constitute strong piece evidence system accept year system belief sabbatical implies faculty member teaching course proposed evidential relationship accepted system accept level proposed belief ai system prior belief contrary expressed utterance evidence provided user implication antecedent accepted",
        "evaluation dialogue model figure modify proposal invoked level proposed belief accepted selecting focus modification system identify candidate focus tree invoke select focus modification algorithm belief root node candidate focus tree candidate focus tree identical proposed belief tree figure level proposed belief proposed evidence rejected evaluation process indicates focus modification ai year evidential relationship accepted select focus modification applied ai algorithm invoked year determine focus modifying child belief step figure system piece evidence year warranted piece evidence containing postponed year anda strong piece evidence containing ibm year support ibm year year evidence predicted sufficient change user belief year ai focus modification year correct node specialization modify proposal invoked focus modification belief order satisfy precondition modify u year posted mutual belief achieved user warranted belief year indicated semantic form utterance system predict informing user intended mutual belief sufficient change belief select justification available piece evidence supporting year presented system predict piece evidence combined proposed mutual belief sufficient change user belief filtering heuristic applied heuristic cause system select postponed year support evidence system confident system try establish mutual belief attempt satisfy precondition modify node cause system invoke inform discourse action generate following utterance user accepts system utterance satisfying precondition conflict resolved modify node performed change original proposed belief user propose modification system proposed modification resulting embedded negotiation subdialogue",
        "collaborative planning principle whittaker stenton walker suggests conversants provide evidence detected discrepancy belief possible agent detects relevant conflict notify agent conflict initiate negotiation subdialogue resolve fail responsibility collaborative agent capture attempt resolve conflict problem solving action modify proposal goal modify proposal form accepted agent applied belief modification modify proposal specialization correct node proposed belief accepted correct relation proposed evidential relationship accepted show problem solving recipe correct node subaction modify node responsible actual modification proposal applicability condition correct node specify action invoked belief node acceptable belief disagree proposed belief represented node collaborative interaction actual modification performed believe node acceptable conflict resolved captured applicability condition precondition modify node attempt satisfy precondition cause system post mutual belief achieved belief node acceptable leading system adopt discourse action change belief initiating collaborative negotiation subdialogue multiple conflict arise system user user proposal system identify aspect proposal focus pursuit conflict resolution example case correct node selected specialization modify proposal system determine parameter node correct node instantiated goal modification process resolve agent conflict unaccepted level proposed belief belief system provide evidence belief address unaccepted evidence proposed user eliminate user justification belief collaborative agent expected engage effective efficient dialogue system address unaccepted belief predicts resolve level conflict unaccepted level belief process selecting focus modification involves step identifying candidate focus tree proposed belief tree selecting focus candidate focus tree heuristic attack belief resolve conflict level belief candidate focus tree contains piece evidence proposed belief tree disbelieved user change user view unaccepted level proposed belief root node belief tree identified performing depth search proposed belief tree node visited belief evidential relationship parent examined belief relationship accepted evaluator search current branch terminate system accepts belief irrelevant accepts user support belief young piece evidence included candidate focus tree system continue search evidence belief tree proposed support unaccepted belief evidential relationship candidate focus",
        "tree identified system select focus modification based likelihood choice changing user belief level belief show algorithm selection process unaccepted belief bel belief proposed support select focus modification annotate bel withits focus modification bel focus contains set belief bel descendent disbelieved user predicted cause disbelieve bel andthe system evidence bel bel s attack focus modification determines attack supporting evidence eliminating user reason holding bel attack bel evaluating effectiveness attacking proposed evidence bel system determine possible refute piece evidence system belief sufficient evidence available convince user piece proposed evidence invalid effective attack evidence support algorithm applies evidence proposed support bel accepted system step recursive process algorithm annotates unaccepted belief evidential relationship proposed support bel focus modification bel system evidence bel attack contains belief selected addressed order change user belief bel value nil system predicts insufficient evidence available change user belief bel information obtained step select focus modification decides attack evidence proposed support bel bel step preference address unaccepted evidence mckeown focusing rule suggest continuing introduced topic said preferable returning previous topic mckeown algorithm considers attacking user support sufficient convince bel step gathering cand set evidence proposed user direct support bel accepted system system predicts refute bel nil algorithm hypothesizes user changed mind belief cand set predicts affect user belief bel step user predicted accept bel hypothesis algorithm invokes select min set select minimum subset cand set unaccepted belief pursue focus modification union focus belief minimum subset attacking evidence bel appear sufficient convince user bel algorithm check attacking bel accomplish goal providing evidence bel predicted successful focus modification bel step attacking bel predicted fail algorithm considers effect attacking bel unaccepted proposed evidence combining previous prediction process step combined evidence predicted fail system sufficient evidence change user view bel focus modification bel nil step step algorithm invoke function predict make use belief revision mechanism galliers discussed section predict user acceptance unacceptance based system knowledge user",
        "belief evidence presented logan result select focus modification set user belief need modified order change user belief unaccepted level belief negation belief posted system mutual belief achieved order perform modify action communication social psychology shown evidence improves persuasiveness message luchok mccroskey reynolds burgoon petty cacioppo hample quantity evidence indicates optimal evidence use high quality evidence consistent persuasive effect reinard hand grice maxim quantity specifies contribute information required important collaborative agent selects sufficient effective excessive evidence justify intended mutual belief convince user belief bel system selects appropriate justification identifying belief support bel applying filtering heuristic system determine justification bel needed predicting informing user bel sufficient convince bel justification presented justification predicted necessary system construct justification chain support bel piece evidence support bel system predicts user accept evidence justification user predicted accept piece evidence evid system augment evidence presented user posting evid mutual belief achieved selecting proposition serve justification result recursive process return chain belief justification support bel set belief forming justification chain identified system select set belief chain presented user predicted convince user bel system construct singleton set justification chain select set containing justification presented predicted convince user bel single justification chain predicted sufficient change user belief new set constructed combining single justification chain selection process repeated produce set possible candidate justification chain heuristic applied select heuristic prefers evidence system confident high quality evidence produce attitude change evidence form luchok mccroskey system justify belief high confidence user accept second heuristic prefers evidence novel user study shown evidence persuasive unknown hearer wyer morley heuristic based grice maxim quantity prefers justification chain contain fewest belief evaluation dialogue model figure modify proposal invoked level proposed belief accepted selecting focus modification system identify candidate focus tree invoke select focus modification algorithm belief root node candidate focus tree candidate focus tree identical proposed belief tree figure level proposed belief",
        "proposed evidence rejected evaluation process indicates focus modification ai year evidential relationship accepted select focus modification applied ai algorithm invoked year determine focus modifying child belief step figure system piece evidence year warranted piece evidence containing postponed year anda strong piece evidence containing ibm year support ibm year year evidence predicted sufficient change user belief year ai focus modification year correct node specialization modify proposal invoked focus modification belief order satisfy precondition modify u year posted mutual belief achieved user warranted belief year indicated semantic form utterance system predict informing user intended mutual belief sufficient change belief select justification available piece evidence supporting year presented system predict piece evidence combined proposed mutual belief sufficient change user belief filtering heuristic applied heuristic cause system select postponed year support evidence system confident system try establish mutual belief attempt satisfy precondition modify node cause system invoke inform discourse action generate following utterance user accepts system utterance satisfying precondition conflict resolved modify node performed change original proposed belief user propose modification system proposed modification resulting embedded negotiation subdialogue",
        "multiple conflict arise system user user proposal system identify aspect proposal focus pursuit conflict resolution example case correct node selected specialization modify proposal system determine parameter node correct node instantiated goal modification process resolve agent conflict unaccepted level proposed belief belief system provide evidence belief address unaccepted evidence proposed user eliminate user justification belief collaborative agent expected engage effective efficient dialogue system address unaccepted belief predicts resolve level conflict unaccepted level belief process selecting focus modification involves step identifying candidate focus tree proposed belief tree selecting focus candidate focus tree heuristic attack belief resolve conflict level belief candidate focus tree contains piece evidence proposed belief tree disbelieved user change user view unaccepted level proposed belief root node belief tree identified performing depth search proposed belief tree node visited belief evidential relationship parent examined belief relationship accepted evaluator search current branch terminate system accepts belief irrelevant accepts user support belief young piece evidence included candidate focus tree system continue search evidence belief tree proposed support unaccepted belief evidential relationship candidate focus tree identified system select focus modification based likelihood choice changing user belief level belief show algorithm selection process unaccepted belief bel belief proposed support select focus modification annotate bel withits focus modification bel focus contains set belief bel descendent disbelieved user predicted cause disbelieve bel andthe system evidence bel bel s attack focus modification determines attack supporting evidence eliminating user reason holding bel attack bel evaluating effectiveness attacking proposed evidence bel system determine possible refute piece evidence system belief sufficient evidence available convince user piece proposed evidence invalid effective attack evidence support algorithm applies evidence proposed support bel accepted system step recursive process algorithm annotates unaccepted belief evidential relationship proposed support bel focus modification bel system evidence bel attack contains belief selected addressed order change user belief bel value nil system predicts insufficient evidence available change",
        "user belief bel information obtained step select focus modification decides attack evidence proposed support bel bel step preference address unaccepted evidence mckeown focusing rule suggest continuing introduced topic said preferable returning previous topic mckeown algorithm considers attacking user support sufficient convince bel step gathering cand set evidence proposed user direct support bel accepted system system predicts refute bel nil algorithm hypothesizes user changed mind belief cand set predicts affect user belief bel step user predicted accept bel hypothesis algorithm invokes select min set select minimum subset cand set unaccepted belief pursue focus modification union focus belief minimum subset attacking evidence bel appear sufficient convince user bel algorithm check attacking bel accomplish goal providing evidence bel predicted successful focus modification bel step attacking bel predicted fail algorithm considers effect attacking bel unaccepted proposed evidence combining previous prediction process step combined evidence predicted fail system sufficient evidence change user view bel focus modification bel nil step step algorithm invoke function predict make use belief revision mechanism galliers discussed section predict user acceptance unacceptance based system knowledge user belief evidence presented logan result select focus modification set user belief need modified order change user belief unaccepted level belief negation belief posted system mutual belief achieved order perform modify action",
        "study communication social psychology shown evidence improves persuasiveness message luchok mccroskey reynolds burgoon petty cacioppo hample quantity evidence indicates optimal evidence use high quality evidence consistent persuasive effect reinard hand grice maxim quantity specifies contribute information required important collaborative agent selects sufficient effective excessive evidence justify intended mutual belief convince user belief bel system selects appropriate justification identifying belief support bel applying filtering heuristic system determine justification bel needed predicting informing user bel sufficient convince bel justification presented justification predicted necessary system construct justification chain support bel piece evidence support bel system predicts user accept evidence justification user predicted accept piece evidence evid system augment evidence presented user posting evid mutual belief achieved selecting proposition serve justification result recursive process return chain belief justification support bel set belief forming justification chain identified system select set belief chain presented user predicted convince user bel system construct singleton set justification chain select set containing justification presented predicted convince user bel single justification chain predicted sufficient change user belief new set constructed combining single justification chain selection process repeated produce set possible candidate justification chain heuristic applied select heuristic prefers evidence system confident high quality evidence produce attitude change evidence form luchok mccroskey system justify belief high confidence user accept second heuristic prefers evidence novel user study shown evidence persuasive unknown hearer wyer morley heuristic based grice maxim quantity prefers justification chain contain fewest belief",
        "dialogue agent autonomous agent deliberates accept reject contribution current speaker speaker assume proposal assertion accepted examination corpus occurring problem solving dialogue show agent indicate acceptance rejection speaker infer hearer understands accepts current contribution based indirect evidence provided hearer dialogue contribution paper propose model role redundant utterance providing evidence support inference mutual understanding acceptance model requires theory mutual belief support mutual belief strength explains function class redundant utterance explained account contributes theory dialogue showing mutual belief inferred absence master slave assumption",
        "valid rule conversation tell people know grice quantity maxim interpreted way contribution informative required grice suggests assert presupposed attempt stalnaker notion informative judged background presupposed proposition conversants assume known believed proposition known common ground lewis grice formulation redundancy rule permeate computational analysis natural language notion cooperativity consider following excerpt middle advisory dialogue harry talk host ray caller yup knock standard information theoretic term redundant assertion paraphrase said adding belief common ground truth question harry yup knock ray repeat harry assertion harry paraphrase ray claim redundant utterance iru main discourse function provide evidence support assumption underlying inference mutual belief center proposition proposition salient grosz paper focus leaving future work consider notion evidence reason agent need evidence belief partial information state world effect action agent belief preference goal true come modelling effect linguistic action action different physical action agent prior belief preference goal ascertained direct inspection mean difficult speaker verify action achieved expected result giving receiving evidence critical process establishing mutual belief monitored conversants characterization iru redundant follows axiomatization action dialogue deterministic model model consists number simplifying assumption proposition believed believed proposition representing belief intention added context unilateral action conversant agent omniscient context discourse undifferentiated set proposition specific relation claim assumption dropped order explain function iru dialogue discus assumption section show assumption dropped section discus section show iru facilitate inference relation adjacent proposition",
        "account proposed common ground augmented based lewis shared environment model common knowledge lewis clark marshall model mutual belief depend evidence available conversants number underlying assumption environment mutual belief induction schemait believed population situation hold reason believe hold reason believe hold situation mutual belief induction schema context said schema support weak model mutual belief akin mutual assumption mutual supposition prince belief inferred based evidence belief depend underlying assumption defeasible model implemented gallier theory autonomous belief revision corresponding system galliers key model type evidence provide better support belief type type evidence considered categorized ordered based source evidence hypothesis default inference linguistic physical clark marshall galliers ordering reflects relative defeasibility different assumption strength assumption decrease relative defeasibility claim paper role iru ensure assumption supported evidence decreasing defeasibility mutual belief depend galliers mutual belief depend defeasible inference process inference depend evidence support stronger evidence defeat weaker evidence mutual belief supported inference defeated linguistic information addition adopt assumption chain reasoning strong weakest link weakest link assumption strength belief depending set underlying assumption min strength plausible mean strength belief depends strength underlying assumption inference rule depend multiple premise strength inferred belief weakest supporting belief representation mutual belief differs common representation term iterated conjunction litman allen relocates information mental state environment utterance occur allows represent different kind evidence mutual belief control reasoning discrepancy mutual belief discovered evidence assumption inspected consist infinite list statement",
        "section examines assumption deterministic model proposition representing belief intention added context unilateral action conversant assumption examined section key claim section agent monitor effect utterance action action addressee taken evidence effect speaker utterance utterance intended effect hypothesis point utterance irrespective intention speaker distinguishes account assume utterance action succeed succeed addressee believed litman allen grosz sidner adopt assumption participant dialogue trying achieve purpose grosz sidner aspect structure dialogue arises structure purpose relation minimal purpose dialogue utterance understood goal prerequisite achieving goal dialogue commitment future action achieving mutual belief understanding instance type activity agent perform collaborate achieve purpose dialogue claim model achievement mutual belief understanding extended achievement goal dialogue understanding unproblematic process managed goal achieving process clark schaefer mutual understanding relies evidence utterance number underlying assumption assumption given inference rule schema mean say intending convey lead mutual belief understands certain assumption assumption copresent attending utterance event heard utterance belief utterance realizes intended meaning evidence type annotation indicates strength evidence supporting assumption assumption start supported evidence evidence type hypothesis addressee action assumption strength modified claim class iru address assumption underlying inference mutual understanding type iru assumption addressed evidence type provided given figure provided section possible intends saying realizes certain inference understanding include making inference add additional assumption assuming inferred relies assumption belief license context say prompt repetition paraphrase making inference explicit provide linguistic evidence attention prompt huh provide evidence attention repetition paraphrase making inference explicit demonstrate complete hearing addition paraphrase making inference explicit provides linguistic evidence proposition paraphraser belief previous utterance realizes inference provide evidence inference inferrer belief realized proposition license context case iru address assumption order infer mutual understanding achieved assumption hypothesis default upgraded support type linguistic result iru fact different iru address different assumption lead",
        "perception iru better evidence understanding paraphrase stronger evidence understanding repeat clark schaefer addition utterance addressee upgrade strength underlying assumption default figure course default evidence weaker linguistic evidence basis default inference discussed section example section repeat harry assertion upgrade evidence assumption hearing attention associated utterance hypothesis linguistic assumption proposition realized remains default instantiates inference rule understanding follows weakest link assumption belief understanding default following excerpt harry utterance said falling intonational contour unlikely question utterance result instantiation inference rule follows case belief understanding supported linguistic evidence supporting assumption supported linguistic evidence paraphrase provides excellent evidence agent understood agent meant addition iru leave proposition salient discourse moved topic centering function iru left future work",
        "section discus assumption determistic model agent omniscient assumption challenged number case occurring dialogue inference follow said explicit restrict inference discus arebased information provided dialogue licensed application gricean maxim scalar implicature inference example logical omniscience assumption mean context entailed buy existing pension plan existing pension plan buy following excerpt demonstrates structure realizes utterance realizes utterance make inference explicit given particular tax year propositional content inferrable assumption harry inference supported inference evidence type according model achieving mutual understanding outlined section utterance provides linguistic evidence harry belief proposition realized utterance license inference context context consists discussion tax year selects narrow focus pitch accent implicates tax year joe eligible hirschberg utterance reinforces implicature harry make upgrade evidence underlying assumption license linguistic subcase ensuring certain inference involves juxtaposition proposition case challenge assumption context discourse undifferentiated set proposition specific relation assumption discourse model semantic model context stalnaker following segment jane describes financial situation harry choice settlement annuity interrupt belief information suggest course action tell money provide support course action produce inference follows told getting dollar year general relation hold belief intention model jane trouble calculating month month amount little year accept statement intended provide necessary support relation ie juxtaposition fact advice money conveys fact getting dollar year reason adopt goal taking money stated",
        "subcase ensuring certain inference involves juxtaposition proposition case challenge assumption context discourse undifferentiated set proposition specific relation assumption discourse model semantic model context stalnaker following segment jane describes financial situation harry choice settlement annuity interrupt belief information suggest course action tell money provide support course action produce inference follows told getting dollar year general relation hold belief intention model jane trouble calculating month month amount little year accept statement intended provide necessary support relation ie juxtaposition fact advice money conveys fact getting dollar year reason adopt goal taking money stated",
        "section examine assumption proposition representing belief intention added context unilateral action conversant suggested assumption replaced adopting model agent behavior provides evidence mutual understanding achieved discussed effect resource bound case ensuring providing evidence certain inference dependent said understanding compensating resource bound issue model dialogue agent autonomous agent autonomy mean number reason utterance conveying proposition achieve intended effect cohere belief think relevant believe contribute common goal prefer believing exclusive action want modify additional constraint important distinguish agent accepting belief intending perform action described understanding conveyed account legislate helpful agent adopt belief intention acceptance depends agent believed litman allen grosz sidner agent decide revise belief galliers acceptance given acceptance inferred dialogue situation operation simple principle cooperative dialogue collaborative principle conversants provide evidence detected discrepancy belief possible principle claim evidence conflict apparent order default inference acceptance understanding going prompt repetition paraphrase making inference explicit function evidence conflict belief intention propositional content redundant realized question intonation inference acceptance blocked dialogue harry ruth ruth ensures understood harry provides explicit evidence non acceptance based autonomous preference money invested following example joe make statement provides propositional content conflict harry statement provides evidence non acceptance statement based prior belief case evidence conflict given evidence contrary goal discourse require achievement acceptance inference acceptance licensed default defeated stronger evidence principle conversant bring objection conversation point relevant belief inference following belief added common ground default result retraction belief result belief revised operation principle help conversants avoid replanning resulting inconsistency belief provides way manage augmentation common ground",
        "point note example subset type iru occur dialogue use term antecedent refer recent utterance added proposition context paper focused case iru adjacent antecedent remote realizes proposition antecedent said conversant antecedent respect subset data alternate hypothesis examined distributional analysis subset corpus iru dialogue consisting turn relation iru antecedent context show token occur antecedent antecedent consist speaker repeating said consist speaker repeating conversant said data paper focus account data example section alternative account ray repetition question kind raise number issue form question question andwhy denied iru realized rising question intonation redundant question question syntax followed affirmation question possible answer question ray questioning heard use intonational contour conveys fact ruth example efficiency argument imagine cost ray effort question answer iru followed affirmation correct right yup followed denial content odd question answer hypothesis iru result radio talk environment silence tolerated agent produce iru think feel point note iru occur dialogue radio carletta second question agent produce iru trivial statement know utterance correlate typical stalling behavior false start pause filled pause uhhh dead air hypothesis rely assumption unpredictable interval agent think claim iru related goal support inferencing address assumption underlying mutual belief random order prove possible test hypothesis important proposition repeated paraphrased explicit based analyzing information repeated requested caller opening question request information harry possible test iru realizes proposition play role final plan harry caller negotiate type strong evidence dead air hypothesis left future work",
        "example section alternative account ray repetition question kind raise number issue form question question andwhy denied iru realized rising question intonation redundant question question syntax followed affirmation question possible answer question ray questioning heard use intonational contour conveys fact ruth example efficiency argument imagine cost ray effort question answer iru followed affirmation correct right yup followed denial content odd question answer",
        "hypothesis iru result radio talk environment silence tolerated agent produce iru think feel point note iru occur dialogue radio carletta second question agent produce iru trivial statement know utterance correlate typical stalling behavior false start pause filled pause uhhh dead air hypothesis rely assumption unpredictable interval agent think claim iru related goal support inferencing address assumption underlying mutual belief random order prove possible test hypothesis important proposition repeated paraphrased explicit based analyzing information repeated requested caller opening question request information harry possible test iru realizes proposition play role final plan harry caller negotiate type strong evidence dead air hypothesis left future work",
        "apparent account type utterance examined redundant reason model belief transfer dialogue characterize redundant follows combination fact representation belief model binary effect utterance action assumed hold hold default listener believed mean account represent fact belief supported kind evidence evidence stronger weaker follows model assume agent autonomous control mental state belief revision autonomous process agent choose accept new belief revise old belief galliers grosz sidner occurrence iru dialogue ramification model dialogue iru direct effect dialogue model requires model mutual belief specifies mutual belief inferred mutual belief weak mutual supposition function iru address assumption mutual belief based assumption proposition representing belief intention added context unilateral action conversant dropped account replaces assumption model evidence hearer considered establish mutual belief claim understanding acceptance monitored model outlined different type dialogue including dialogue agent constructing mutual belief support future action agent decide augment strength evidence belief addressed work work includes analyzing corpus respect iru play role final plan negotiated conversants",
        "determining number difficult problem translating japanese english japanese noun phrase marked respect number noun equivalent english singular plural form verb inflect agree number subject kuno addition grammatical marking countability order generate english know given noun phrase countable uncountable countable singular plural problem human translating japanese english knowledge language draw machine translation system need knowledge codified way generating article number important rest sentence generated lot research devoted murata nagao proposed method determining referentiality property number noun japanese sentence machine translation english research extended include actual english generation paper describes method extract information relevant countability number japanese text combine knowledge countability number english countability english discussed noun phrase noun level noun phrase countability english affected referential property generic referential ascriptive present method determining referential use japanese noun phrase process determining noun phrase countability number described followed example sentence translated proposed method discussion result processing described paper implemented ntt communication science laboratory experimental machine translation system alt j ikehara new processing generation article discussed detail paper improved percentage noun phrase generated determiner number",
        "adopt definition countability english given allan countable noun phrase defined follows head constituent fall scope denumerator countable head constituent plural countable phrase fall scope domain denumerator mean denumerated reference quantified denumerator number discrete entity noun english head countable noun phrase particular noun phrase head fall scope denumerator denumerated noun phrase headed noun singular plural form form equipment scissors require classifier classifier head countable noun phrase original noun attached complement prepositional phrase headed pair scissors piece equipment noun head countable noun phrase depends interpreted inherent countability preference countability preference discussed section noun countability preference determines behave different environment classify noun countability preference major minor described basic type countable uncountable countable noun knife singular plural form determiner noun furniture plural form extreme vast number noun cake countable uncountable noun phrase singular plural form noun depends referent thought discrete unit possible determine translating japanese english divide noun group countable refer discrete entity cake weakly countable refer unbounded referent beer major type countability preference pluralia tanta noun plural form scissors denumerated modified subdivide pluralia tanta type use classifier pair denumerated pair scissors clothes pair pluralia tanta singular form modifier scissor movement tanta clothes use plural form modifier clothes horse need countable word similar meaning substituted denumerated garment suit minor type subset countable uncountable noun indicated treated supersets collective noun share property countable noun addition singular plural verb agreement singular form noun government decided semi countable noun share property uncountable noun modified example knowledge japanese information countability number stored japanese english noun transfer dictionary given table information noun countability preference found standard dictionary entered english native speaker test help determine given noun countability preference described bond ogura discus use noun countability preference japanese english",
        "adopt definition countability english given allan countable noun phrase defined follows head constituent fall scope denumerator countable head constituent plural countable phrase fall scope domain denumerator mean denumerated reference quantified denumerator number discrete entity noun english head countable noun phrase particular noun phrase head fall scope denumerator denumerated noun phrase headed noun singular plural form form equipment scissors require classifier classifier head countable noun phrase original noun attached complement prepositional phrase headed pair scissors piece equipment noun head countable noun phrase depends interpreted inherent countability preference countability preference discussed section",
        "noun countability preference determines behave different environment classify noun countability preference major minor described basic type countable uncountable countable noun knife singular plural form determiner noun furniture plural form extreme vast number noun cake countable uncountable noun phrase singular plural form noun depends referent thought discrete unit possible determine translating japanese english divide noun group countable refer discrete entity cake weakly countable refer unbounded referent beer major type countability preference pluralia tanta noun plural form scissors denumerated modified subdivide pluralia tanta type use classifier pair denumerated pair scissors clothes pair pluralia tanta singular form modifier scissor movement tanta clothes use plural form modifier clothes horse need countable word similar meaning substituted denumerated garment suit minor type subset countable uncountable noun indicated treated supersets collective noun share property countable noun addition singular plural verb agreement singular form noun government decided semi countable noun share property uncountable noun modified example knowledge japanese information countability number stored japanese english noun transfer dictionary given table information noun countability preference found standard dictionary entered english native speaker test help determine given noun countability preference described bond ogura discus use noun countability preference japanese english machine translation",
        "stage generating countability number translated english noun phrase determine referentiality distinguish kind referentiality generic referential ascriptive noun phrase general statement class generic example mammoth extinct way generic noun phrase expressed english described section noun phrase one refer specific referent example dog chase cat number countability determined property referent noun phrase ascribe property example hathi elephant number countability noun phrase property describing process determining referentiality noun phrase shown figure test processed order shown possible simple criterion implemented dictionary chosen example test modified aimed generic applied translating muke transfer dictionary includes information case generic heuristic method determining noun phrase generic reference test predicate marked dictionary applies class evolve extinct sentence taken generic test alt j semantic hierarchy test sentence generic example mammoth animal semantic category animal sentence judged stating fact true mammoth generic generic noun phrase countable head noun expressed way huddleston gen noun phrase indefinite mammoth mammal gen noun phrase definite mammoth mammal gen article mammoth mammal noun pluralia tanta expressed gen furniture expensive gen modified gen noun phrase interpreted having definite reference countable uncountable gen cake delicious cake delicious combination shown table noun phrase generic reference marked use kind generic noun phrase acceptable context example mammoth evolved noun phrase ambiguous example elephant speaker like particular elephant elephant use gen acceptable context alt generates generic noun phrase bare noun phrase number noun phrase determined countability preference noun phrase heading countable noun pluralia tanta plural singular",
        "generic noun phrase countable head noun expressed way huddleston gen noun phrase indefinite mammoth mammal gen noun phrase definite mammoth mammal gen article mammoth mammal noun pluralia tanta expressed gen furniture expensive gen modified gen noun phrase interpreted having definite reference countable uncountable gen cake delicious cake delicious combination shown table noun phrase generic reference marked use kind generic noun phrase acceptable context example mammoth evolved noun phrase ambiguous example elephant speaker like particular elephant elephant use gen acceptable context alt generates generic noun phrase bare noun phrase number noun phrase determined countability preference noun phrase heading countable noun pluralia tanta plural singular",
        "following discussion deal referential ascriptive noun phrase generic noun phrase discussed section definition noun phrase countability given section useful analyzing english sufficient translating japanese english case impossible tell japanese form syntactic shape translated noun phrase fall scope denumerator equivalent distinguish countable uncountable quantifier little determine countability generate number need use combination information japanese original sentence default information japanese english transfer dictionary possible detailed information entered transfer dictionary allow translation process simple process determining noun phrase countability number shown figure process carried transfer stage information available japanese original selected english translation task determining countability number simpler define combination different countabilities noun different countability preference use dictionary effect common type major noun countability preference shown table phrase modified japanese english pair translated denumerators denumerated example noun modified onoono denumerated singular modified ryouhou denumerated plural pluralia tantum noun denumerated environment translated prepositional complement classifier default classifier stored stored dictionary uncountable noun pluralia tanta noun phrase subject countable denumerated mass environment shown table countability noun countable uncountable countable noun countable denumerator countable noun countable plural mass countable environment object collect collect cake uncountable singular mass uncountable environment ate cake fact collect cake ate cake possible japanese distinguish system best choice way human translator rule implemented generate translation widest application example generating ate cake true speaker ate cake ate cake ate cake true speaker ate cake choice english translation modifier depend countability noun phrase example kazukazu takusan translated implies modificant discrete entity noun phrase modifies translated denumerated plural carry nuance alt j translate noun phrase modified mass uncountable takusan head countable translate noun different noun countability preference combination countable uncountable possible example countable noun uncountable noun phrase elephant referred individual elephant source meat expressed uncountable noun phrase ate slice elephant generate following rule noun quantified classifier kire",
        "processing described implemented alt j tested new processing generate article constructed set test sentence collection newspaper article result summarized table newspaper article tested average noun phrase sentence sentence judged correct noun phrase correct introduction proposed method improved percentage correct sentence example translation introduction new processing given translation proposed processing implemented marked old translation produced alt j proposed processing marked new noun phrase headed otona adult judged prescriptive complement copular naru proposed method translates number subject die entered lexicon verb subject generic mammoth countable generic noun phrase translated bare plural old version recognizes denumerated noun phrase headed uncountable noun tofu requires classifier generate correct structure generate classifier pluralia tanta scissors version proposed method subject copula countable complement judged denumerated proposed method complement headed uncountable noun embedded prepositional complement classifier main problem remaining rule determining noun phrase referentiality fine estimate referentiality determined percentage noun phrase generated article number improved test set studied remaining require knowledge sentence translated biggest problem noun phrase requiring world knowledge expressed dictionary default noun phrase generated heuristic method proposed problem noun phrase countability number deduced information sentence like extend method use information future",
        "address problem acquiring case frame pattern selectional pattern large corpus data particular propose method learning dependency case frame slot view problem learning case frame pattern learning multi dimensional discrete joint distribution random variable represent case slot formalize dependency case slot probabilistic dependency random variable number parameter multi dimensional joint distribution exponential general infeasible estimate practice overcome difficulty settle approximating target joint distribution product low order component distribution based corpus data particular propose employ efficient learning algorithm based mdl principle realize task experimental result indicate certain class verb accuracy achieved disambiguation experiment improved acquired knowledge dependency",
        "address problem acquiring case frame pattern selectional pattern large corpus data acquisition case frame pattern involves following subproblems extracting case frame corpus data generalizing case frame slot case frame learning dependency exist generalized case frame slot paper propose method learning dependency case frame slot dependency meant relation exists case slot constrains possible value assumed case slot illustrative example consider following sentence airline company subject verb fly value slot arg direct object value slot arg airplane airline company example indicate possible value case slot depend general case slot exist dependency different case slot knowledge dependency useful task natural language processing analysis sentence involving multiple prepositional phrase asnote example slot considered dependent attachment site prepositional phrase case slot determined high accuracy confidence method proposed date learns dependency case slot natural language processing literature past research distributional pattern case slot learned method resolving ambiguity based assumption case slot independent hindle rooth chang sekine resnik grishman sterling alshawi carter abe dependency case slot considered brill resnik ratnaparkhi collins brook provision effective method learning dependency case slot investigation usefulness acquired dependency disambiguation natural language processing task important contribution field paper view problem learning case frame pattern learning multi dimensional discrete joint distribution random variable represent case slot formalize dependency case slot probabilistic dependency random variable number parameter exist multi dimensional joint distribution exponential allow n ary dependency general infeasible estimate high accuracy data size available practice clear random variable case slot dependent significance likely target joint distribution approximated product component distribution low order reducing number parameter need considered approach paper problem approximate joint distribution product lower order component distribution suzuki proposed algorithm learn multi dimensional joint distribution expressible dendroid distribution efficient sound suzuki employ suzuki algorithm learn case frame pattern dendroid distribution conducted experiment acquire case frame pattern penn tree bank bracketed corpus experimental result indicate class verb accuracy achieved disambiguation experiment improved acquired knowledge dependency case slot",
        "suppose data type shown figure given instance case frame verb fly extracted corpus conventional technique explained introduction problem learning case frame pattern viewed estimating underlying multi dimensional joint distribution give rise data research assume case frame instance head generated joint distribution type index stand head random variable represents case slot paper use case slot mean surface case slot treat obligatory case optional case number random variable equal number preposition english model classified type probabilistic model according type value random variable assumes assumes word special symbol value refer corresponding model word based model indicates absence case slot question assumes word class value corresponding model called class based model take value model slot based model value indicates presence case slot question absence example data figure generated word based model data figure class based model simplicity possible case slot corresponding subject direct object phrase phrase given specific probability value word based model contrast given specific probability class based model denote word class assigned specific probability slot based model formulate dependency case slot probabilistic dependency random variable model absence constraint number parameter model exponential slot based model parameter infeasible estimate practice simplifying assumption deal difficulty random variable case slot independent example analysis following alternative interpretation given wish select appropriate interpretation heuristic word based method disambiguation random variable case slot assumed dependent calculate following value word based likelihood select interpretation corresponding higher likelihood value hand assume random variable independent need calculate compare abe independence assumption case class based model slot based model slot based model independence assumption following probability compared hindle rooth random variable case slot independent reduce number parameter note independence assumption number parameter slot based model illustrated section assumption valid practice true practice case slot fact dependent overwhelming majority independent fact case slot obligatory optional target joint distribution approximable product component distribution low order fact small number parameter lead approach approximating target joint distribution",
        "loss generality dimensional joint distribution written asfor permutation let denote plausible assumption dependency random variable variable depends variable note assumption simplest relax independence assumption example joint distribution random variable written approximated follows satisfies assumption distribution referred dendroid distribution literature dendroid distribution represented dependency forest set dependency tree node represent random variable directed arc represent dependency exist random variable labeled number parameter specifying probabilistic dependency dendroid distribution considered restricted form bayesian network pearl difficult representation joint distribution figure disregarding actual numerical value probabilistic parameter turn problem select best dendroid distribution possible one approximate target joint distribution based input data generated problem investigated area machine learning related field classical method chow liu algorithm estimating multi dimensional joint distribution dependency tree way efficient sound chow liu suzuki extended algorithm estimate target joint distribution dendroid distribution dependency forest suzuki allowing possibility learning group random variable independent random variable case slot case frame pattern independent feature crucial context employ suzuki algorithm learning case frame pattern algorithm calculates mutual information node random variable sort node pair descending order respect mutual information put link node pair largest mutual information value provided exceeds certain threshold depends node pair adding link create loop current dependency graph repeat process node pair left unprocessed show detail algorithm denotes number possible value assumed input data size denotes logarithm base number parameter dendroid distribution order maximum number random variable time complexity algorithm order algorithm work illustrative example data given figure node random variable value mutual information threshold node pair shown table calculation algorithm construct dependency forest shown figure mutual information large result indicates slot arg considered dependent arg considered dependent weaker degree algorithm derived minimum description length mdl principle rissanen rissanen rissanen rissanen rissanen principle statistical estimation information theory known strategy estimation mdl guaranteed optimal applying mdl assume",
        "given data generated probabilistic model belongs certain class model selects model class explains data tends case simpler model poorer fit data complex model fit data trade simplicity model goodness fit data resolve trade disciplined way selects model simple fit data current problem simple model mean model dependency mdl provides sound way learn dependency significant given data interesting feature mdl incorporates input data size model selection criterion reflected case derivation threshold data small threshold large node tend linked resulting simple model case slot judged independent reasonable small data size case slot determined dependent significance",
        "conducted experiment test performance proposed method method acquiring case frame pattern particular tested effective pattern acquired method structural disambiguation experiment describe result experimentation section experiment tried acquire slot based case frame pattern extracted case frame wall street journal wsj bracketed corpus penn tree bank marcus training data verb case frame example appeared training data show verb appeared data number occurrence acquired slot based case frame pattern verb conducted fold cross validation evaluate test data perplexity acquired case frame pattern tenth case frame verb training data saving remains test data acquire case frame pattern calculated perplexity test data repeated process time calculated average perplexity show average perplexity obtained selected verb calculated average perplexity independent slot model acquired based assumption case slot independent experimental result shown table indicate use dendroid model achieve perplexity reduction compared independent slot model safe dendroid model suitable representing true model case frame independent slot model acquired dependency knowledge pp attachment disambiguation experiment case frame verb training data entire bracketed corpus training data wanted utilize training data possible extracted pattern wsj tagged corpus test data pattern matching technique described smadja took care ensure tagged non bracketed corpus overlap bracketed corpus test data bracketed corpus overlap tagged corpus acquired case frame pattern training data show example result case frame pattern dendroid distribution verb buy note model slot etc dependent arg arg independent found verb arg slot dependent preposition slot show verb dependency arg case slot positive exceed certain threshold arg prep dependency found method agree human intuition case example test data pattern slot arg prep verb determined dependent dependency stronger threshold attached verb example comparison tested disambiguation method based independence assumption proposed abe example show result experiment dendroid stand method independent information dependency improve disambiguation accuracy data",
        "use existing method perform disambiguation rest data improve disambiguation accuracy entire test data knowledge found verb having inter dependent preposition slot show verb case slot positive dependency exceeds certain threshold dependency found method agree human intuition test data pattern example involves verb preposition slot dependency exceeding attached verb example slot judged dependent show result experimentation dendroid independent represent method dependency find test data dependency present use dependency knowledge improve accuracy disambiguation method experimental result inconclusive stage verb case frame experiment acquire class based case frame pattern proposed method selected verb verb attempted acquire case frame pattern generalized case slot case frame method proposed abe obtain class based case slot replaced word based case slot data obtained class based case slot resulted class based case frame example shown figure data input learning algorithm acquired case frame pattern verb found case slot determined dependent case frame pattern number parameter class based model large compared size data available experimental result verifies validity practice assumption statistical natural language processing class based case slot word based case slot independent data size available provided current version penn tree bank empirical finding worth noting independence assumption based human intuition best knowledge test large data size required estimate class based model conducted following experiment defined artificial class based model generated data according distribution data estimate class based model dendroid distribution evaluated estimated model measuring number dependency dependency arc distance estimated model true model generated data observed learning curve relationship number dependency estimated model data size estimation relationship distance estimated true model data size defined model conducted experiment show result experiment artificial model averaged trial number parameter model model model number dependency estimate model data size required large time number parameter class based",
        "experiment tried acquire slot based case frame pattern extracted case frame wall street journal wsj bracketed corpus penn tree bank marcus training data verb case frame example appeared training data show verb appeared data number occurrence acquired slot based case frame pattern verb conducted fold cross validation evaluate test data perplexity acquired case frame pattern tenth case frame verb training data saving remains test data acquire case frame pattern calculated perplexity test data repeated process time calculated average perplexity show average perplexity obtained selected verb calculated average perplexity independent slot model acquired based assumption case slot independent experimental result shown table indicate use dendroid model achieve perplexity reduction compared independent slot model safe dendroid model suitable representing true model case frame independent slot model acquired dependency knowledge pp attachment disambiguation experiment case frame verb training data entire bracketed corpus training data wanted utilize training data possible extracted pattern wsj tagged corpus test data pattern matching technique described smadja took care ensure tagged non bracketed corpus overlap bracketed corpus test data bracketed corpus overlap tagged corpus acquired case frame pattern training data show example result case frame pattern dendroid distribution verb buy note model slot etc dependent arg arg independent found verb arg slot dependent preposition slot show verb dependency arg case slot positive exceed certain threshold arg prep dependency found method agree human intuition case example test data pattern slot arg prep verb determined dependent dependency stronger threshold attached verb example comparison tested disambiguation method based independence assumption proposed abe example show result experiment dendroid stand method independent information dependency improve disambiguation accuracy data use existing method perform disambiguation rest data improve disambiguation accuracy entire test data knowledge found verb having inter dependent preposition slot show",
        "verb case slot positive dependency exceeds certain threshold dependency found method agree human intuition test data pattern example involves verb preposition slot dependency exceeding attached verb example slot judged dependent show result experimentation dendroid independent represent method dependency find test data dependency present use dependency knowledge improve accuracy disambiguation method experimental result inconclusive stage",
        "verb case frame experiment acquire class based case frame pattern proposed method selected verb verb attempted acquire case frame pattern generalized case slot case frame method proposed abe obtain class based case slot replaced word based case slot data obtained class based case slot resulted class based case frame example shown figure data input learning algorithm acquired case frame pattern verb found case slot determined dependent case frame pattern number parameter class based model large compared size data available experimental result verifies validity practice assumption statistical natural language processing class based case slot word based case slot independent data size available provided current version penn tree bank empirical finding worth noting independence assumption based human intuition best knowledge test large data size required estimate class based model conducted following experiment defined artificial class based model generated data according distribution data estimate class based model dendroid distribution evaluated estimated model measuring number dependency dependency arc distance estimated model true model generated data observed learning curve relationship number dependency estimated model data size estimation relationship distance estimated true model data size defined model conducted experiment show result experiment artificial model averaged trial number parameter model model model number dependency estimate model data size required large time number parameter class based model tends parameter current data size available penn tree bank table accurate estimation dependency case frame verb",
        "conclude paper following remark primary contribution research reported paper proposed method learning dependency case frame slot sound efficient providing effective tool acquiring case dependency information slot based model case slot found dependent result demonstrate dependency information dependency exist structural disambiguation result improved word based class based model case slot judged independent data size available penn tree bank empirical finding verifies independence assumption practice statistical natural language processing proposed use dependency forest represent case frame pattern possible complicated probabilistic dependency graph bayesian network appropriate representing case frame pattern require data problem collect sufficient data crucial issue addition methodology learning case frame pattern probabilistic dependency graph problem determine obligatory optional case based dependency acquired data addressed",
        "method constructing thesaurus clustering word based corpus data proposed hindle brown pereira tokunaga realization automatic construction method possible tosave cost constructing thesaurus hand subjectivity inherent hand thesaurus andmake adapt natural language processing system new domain paper propose new method automatic construction thesaurus view problem clustering word estimating joint distribution cartesian product partition set noun general set word partition set verb general set word propose estimation algorithm simulated annealing energy function based minimum description length mdl principle mdl principle motivated sound principle data compression estimation information theory statistic strategy statistical estimation mdl guaranteed optimal evaluated effectiveness method particular compared performance mdl based simulated annealing algorithm hierarchical word clustering based maximum likelihood estimator mle short found mdl based method performs mle based method evaluated method conducting pp attachment disambiguation experiment thesaurus constructed found disambiguation result improved word occur corpus classified method based corpus data propose combine use constructed thesaurus hand thesaurus disambiguation conducted experiment order test effectiveness strategy experimental result indicate combining constructed thesaurus hand thesaurus widens coverage disambiguation method maintaining high accuracy",
        "method constructing thesaurus based corpus data consists following step extract co occurrence data case frame data adjacency data corpus starting single class word composing class divide merge word class based co occurrence data similarity distance measure approach called divisive agglomerative repeat step stopping condition met construct thesaurus tree method propose consists step available data figure frequency data co occurrence data verb object extracted corpus step view problem clustering word estimating probabilistic model representing probability distribution generates data assume target model defined following way define noun partition given set noun verb partion given set verb noun partition set satisfying verb partition defined paper member noun partition noun cluster member verb partition verb cluster refer member cartesian product noun partition verb partition cluster define probabilistic model joint distribution written random variable assumes value fixed noun partition value fixed verb partition given cluster assume element generated equal probability figure show example model given rise data figure paper assume observed data generated model belonging class model described select model best explains data result obtain noun cluster verb cluster problem setting based intuitive assumption similar word occur context equal likelihood explicit equation selecting model explains given data equivalent finding appropriate classification word based co occurrence",
        "turn question strategy criterion employ estimating best model choice mdl minimum description length principle rissanen rissanen rissanen rissanen rissanen known principle data compression statistical estimation information theory stipulates best probability model given data model requires code length encoding model given data relative refer code length model model description length data data description length apply mdl problem estimating model consisting pair partition described context model cluster model figure tends simpler term number parameter tends poorer fit data contrast model cluster model figure complex tends fit data trade relationship simplicity clustering model goodness fit data model description length quantifies simplicity complexity model data description length quantifies fit data mdl model minimizes sum total type description length selected follows describe detail description length calculated current context simulated annealing algorithm based mdl describe description length model calculated model specified cartesian product noun partition verb partition number parameter let denote size noun partition size verb partition free parameter model model data total description length computed sum model description length description length parameter data description length refer model description length employ binary noun clustering method fixed decide applied cluster obtained view noun entity verb feature cluster entity based feature subset set noun binary noun partition different subset special case subset set number possible binary noun partition binary noun partition need bit calculated asis calculated bywhere denotes input data size number free parameter model known bit describe parameter minimize description length rissanen calculated bywhere denotes observed frequency noun verb pair estimated probability calculated followswhere denotes observed frequency noun verb pair belonging cluster description length model defined manner wish select model having minimum description length output result clustering model description length model practice need calculate compare description length data figure model figure shown table table show value needed calculation description length model calculation indicate according",
        "describe description length model calculated model specified cartesian product noun partition verb partition number parameter let denote size noun partition size verb partition free parameter model model data total description length computed sum model description length description length parameter data description length refer model description length employ binary noun clustering method fixed decide applied cluster obtained view noun entity verb feature cluster entity based feature subset set noun binary noun partition different subset special case subset set number possible binary noun partition binary noun partition need bit calculated asis calculated bywhere denotes input data size number free parameter model known bit describe parameter minimize description length rissanen calculated bywhere denotes observed frequency noun verb pair estimated probability calculated followswhere denotes observed frequency noun verb pair belonging cluster description length model defined manner wish select model having minimum description length output result clustering model description length model practice need calculate compare description length data figure model figure shown table table show value needed calculation description length model calculation indicate according mdl model selected model",
        "method word clustering proposed date objective vary table exhibit simple comparison work related work method proposed pereira relevant context pereira proposed method soft clustering word belong number distinct class certain probability clustering desirable property example word sense ambiguity input data treated unified manner restrict attention clustering word belong class interested comparing thesaurus constructed method existing hand thesaurus note hand thesaurus based hard clustering",
        "section elaborate merit method statistical natural language processing number parameter probabilistic model estimated large model difficult estimate reasonable data size available practice problem referred data sparseness problem smooth estimated probability existing smoothing technique dagan gale church calculate similarity measure smoothed probability cluster word according guarantee employed smoothing method way consistent clustering method method based mdl resolve issue unified fashion employing model embody assumption word belonging cluster occur context equal likelihood method achieves smoothing effect effect clustering process domain smoothing coincide cluster obtained clustering coarseness fineness clustering determines degree smoothing effect fall corollary imperative possible estimation original motivation mdl principle simulated annealing algorithm employ maximum likelihood estimator mle criterion best probabilistic model mdl suggests selects model maximizes likelihood data equivalent minimizing data description length defined section mdl generalizes mle take account complexity model presence model varying complexity mle tends overfit data output model complex tailored fit specific input data employ mle criterion simulated annealing algorithm result selecting fine model small cluster probability estimated contrast employing mdl effect smoothing method estimation superiority mdl mle supported convincing theoretical finding barron cover yamanishi instance speed convergence model selected mdl true model known optimal model selected mdl converge true model rate number parameter true model mle rate size domain context total number element consistency desirable property mdl shared mle number parameter model selected mdl converge true model rissanen property mdl verified present context shown section particular compared performance employing mdl based simulated annealing based mle word clustering",
        "describe experimental result section compared performance employing mdl criterion simulated annealing algorithm employing mle simulation experiment constructed true model word co occurrence generated data according distribution data estimate model clustering word measured distance true model estimated model algorithm mle shown figure data description length replaces total description length step figure plot relation number obtained noun cluster leaf node obtained thesaurus tree input data size averaged trial number noun cluster true model figure plot distance data size averaged trial result indicate mdl converges true model mle mle tends select model overfitting data mdl tends select model simple fit data conducted simulation experiment model found tendency figure figure analogous result number noun cluster true model conclude employ mdl mle word clustering extracted case frame bracketed wsj wall street journal corpus penn tree bank marcus co occurrence data constructed number thesaurus based data method show example thesaurus occurred noun data constructed based appearance subject object verb obtained thesaurus agree human intuition degree example classified noun cluster stock share classified noun cluster meaningful useful sense data size large general tendency observed example thesaurus obtained method shown figure speaking obtained thesaurus agrees intuition secondary concern main purpose use constructed thesaurus help improve disambiguation task evaluated method constructed thesaurus attachment disambiguation experiment training data case frame experiment extracted test data pattern data corpus training data word appear position constructed thesaurus based co occurrence head slot value frame training data disambiguation test need thesaurus consisting word applied learning method proposed abe learn case frame pattern constructed thesaurus input training data learn conditional distribution vary internal node certain cut thesaurus tree show example case frame pattern obtained method figure show leaf node dominated internal node appearing case frame pattern table compare estimated based case frame pattern determine attachment site larger",
        "attach verb larger attach including conclude decision show result pp attachment disambiguation experiment term coverage accuracy coverage refers proportion percentage test pattern disambiguation method decision base line refers method attaching word based mle thesaurus mdl thesaurus stand word based estimate thesaurus constructed employing mle thesaurus constructed method coverage mdl thesaurus outperformed word based maintaining high accuracy drop indicating constructed thesaurus improve disambiguation result term coverage tested method proposed abe learning case frame pattern existing thesaurus particular method wordnet miller training data conducted pp attachment disambiguation experiment obtained case frame pattern result experiment wordnet table term coverage wordnet outperforms mdl thesaurus term accuracy mdl thesaurus outperforms wordnet result interpreted follows constructed thesaurus domain dependent capture domain dependent feature achieves high accuracy hand training data available insufficient coverage smaller hand thesaurus practice make sense combine type thesaurus constructed thesaurus coverage coverage hand thesaurus current state word clustering technique requires data size available tends demanding strategy practical result combined method mdl thesaurus wordnet table experimental result show employing combined method increase coverage disambiguation tested mdl thesaurus wordnet default stand learned thesaurus wordnet lexical association value proposed hindle rooth default attaching best disambiguation result obtained combined method improves accuracy reported abe",
        "compared performance employing mdl criterion simulated annealing algorithm employing mle simulation experiment constructed true model word co occurrence generated data according distribution data estimate model clustering word measured distance true model estimated model algorithm mle shown figure data description length replaces total description length step figure plot relation number obtained noun cluster leaf node obtained thesaurus tree input data size averaged trial number noun cluster true model figure plot distance data size averaged trial result indicate mdl converges true model mle mle tends select model overfitting data mdl tends select model simple fit data conducted simulation experiment model found tendency figure figure analogous result number noun cluster true model conclude employ mdl mle word clustering",
        "extracted case frame bracketed wsj wall street journal corpus penn tree bank marcus co occurrence data constructed number thesaurus based data method show example thesaurus occurred noun data constructed based appearance subject object verb obtained thesaurus agree human intuition degree example classified noun cluster stock share classified noun cluster meaningful useful sense data size large general tendency observed example thesaurus obtained method shown figure speaking obtained thesaurus agrees intuition secondary concern main purpose use constructed thesaurus help improve disambiguation task",
        "evaluated method constructed thesaurus attachment disambiguation experiment training data case frame experiment extracted test data pattern data corpus training data word appear position constructed thesaurus based co occurrence head slot value frame training data disambiguation test need thesaurus consisting word applied learning method proposed abe learn case frame pattern constructed thesaurus input training data learn conditional distribution vary internal node certain cut thesaurus tree show example case frame pattern obtained method figure show leaf node dominated internal node appearing case frame pattern table compare estimated based case frame pattern determine attachment site larger attach verb larger attach including conclude decision show result pp attachment disambiguation experiment term coverage accuracy coverage refers proportion percentage test pattern disambiguation method decision base line refers method attaching word based mle thesaurus mdl thesaurus stand word based estimate thesaurus constructed employing mle thesaurus constructed method coverage mdl thesaurus outperformed word based maintaining high accuracy drop indicating constructed thesaurus improve disambiguation result term coverage tested method proposed abe learning case frame pattern existing thesaurus particular method wordnet miller training data conducted pp attachment disambiguation experiment obtained case frame pattern result experiment wordnet table term coverage wordnet outperforms mdl thesaurus term accuracy mdl thesaurus outperforms wordnet result interpreted follows constructed thesaurus domain dependent capture domain dependent feature achieves high accuracy hand training data available insufficient coverage smaller hand thesaurus practice make sense combine type thesaurus constructed thesaurus coverage coverage hand thesaurus current state word clustering technique requires data size available tends demanding strategy practical result combined method mdl thesaurus wordnet table experimental result show employing combined method increase coverage disambiguation tested mdl thesaurus wordnet default stand learned thesaurus wordnet lexical association value proposed hindle rooth default attaching best disambiguation result obtained combined method improves accuracy reported abe",
        "word unknown lexicon present substantial problem speech po tagging real world text assign single po tag word token provided known part speech word principle word looked lexicon word token missing lexicon tagging real world text word po guesser place employ analysis word feature word leading trailing character figure possible po category set rule basis ending character unknown word assign set possible po tag supplied xerox tagger kupiec similar approach taken weischedel unknown word guessed given probability unknown word particular po capitalisation feature ending brill system rule us ending guessing motivated rule described best method reported achieve tagging accuracy unknown word brill weischedel major topic development word po guesser strategy acquisition guessing rule rule based tagger described voutilainen equipped set guessing rule hand crafted knowledge english morphology intuition appealing approach empirical automatic acquisition rule available lexical resource zhang kim system automated learning morphological word formation rule described system divide string region training example infers correspondence underlying morphological feature outline transformation based learner learns guessing rule pre tagged training corpus statistical based suffix learner presented schmid pre tagged training corpus construct suffix tree suffix associated information measure learning process system unsupervised accuracy obtained rule reach current state art require prepared training data pre tagged training corpus training example etc paper describe new automatic technique learning speech guessing rule technique require prepared training data employ unsupervised statistical learning lexicon supplied tagger word frequency obtained raw corpus learning implemented staged process feedback setting certain parameter set guessing rule acquired evaluated result evaluation acquisition tuned rule set",
        "pointed requirement technique automatic learning speech guessing rule prepared training data pre tagged training corpus training example etc approach decided reuse data come tagger viz lexicon source information prepared task text corpus approach require corpus pre annotated use raw form experiment lexicon word frequency derived brown corpus francis kucera number reason choosing brown corpus data training important one brown corpus provides model general multi domain language use general language regularity induced tagger come data trained brown corpus useful comparison evaluation mean restricts described technique tag set lexicon corpus despite fact training performed particular lexicon particular corpus obtained guessing rule suppose domain corpus independent training dependent feature tag set use acquisition word po guessing rule step procedure includes rule extraction rule scoring rule merging phase rule extraction phase set word guessing rule morphological prefix guessing rule morphological suffix guessing rule ending guessing rule extracted lexicon cleaned coincidental case scoring phase rule scored accordance accuracy guessing scored rule included final rule set merging phase rule scored high included final rule set merged general rule scored depending score added final rule set word guessing rule describe word guessed given word known example rule say prefixing string word act past form verb vbd participle vbn produce adjective instance applying rule word undeveloped segment prefix remaining developed found lexicon vbd vbn conclude word undeveloped adjective po set guessing rule called initial class class po set guessed word called resulting class r class example vbd vbn class rule r class english language morphological word formation realised affixation prefixation suffixation affixation straightforward concatenation affix stem majority case obey simple concatenative regularity decided concentrate simple concatenative case kind morphological rule learned suffix rule rule applied tail word prefix rule rule applied beginning word example say stripping suffix unknown word produce word po class unknown word class vbd vbn rule work instance book booked water watered etc extract rule special operator applied pair word lexicon try segment affix leftmost string subtraction suffix rightmost string subtraction",
        "prefix subtraction result non string creates morphological rule storing po class shorter word class po class longer word r class example operator applied possible lexicon entry pair rule produced application extracted pair frequency count incremented different set guessing rule prefix suffix morphological rule frequency produced set guessing rule need cut infrequent rule bias learning process eliminate rule frequency certain threshold filtering reduces rule set leave coincidental case rule morphological guessing rule ending guessing rule require main form unknown word listed lexicon rule guess po class word basis ending character looking stem lexicon rule able cover unknown word morphological guessing rule accuracy high example ending guessing rulesays word end ing adjective noun gerund morphological rule rule ask check substring preceeding ing ending word particular po tag ending guessing rule look morphological rule class void collect rule set upper limit ending length equal character collect lexicon possible word ending length po class word ending detected appear operator example word different operator produce ending guessing rule operator applied entry lexicon way described operator morphological rule infrequent rule filtered course acquired rule good plausible guess word class rule accurate guessing rule frequent application acquired rule need estimate effective rule worth retaining final rule set estimation perform statistical experiment follows rule calculate number time rule applied word token raw corpus number time gave right answer task rule disambiguate word po provide possible po rule correct majority time applied good rule rule wrong time bad rule included final rule set perform experiment rule rule set produced rule extraction phase word token corpus guess po set rule rule applicable word example guessing rule strip particular suffix current word corpus suffix classify word rule incompatible rule applicable word rule applicable word perform look lexicon word compare result",
        "guess information listed lexicon guessed po set po set stated lexicon count success failure value guessing rule correlate estimated proportion success proportion positive outcome rule application total number trial fact attempt apply rule compatible word corpus smooth zero positive negative outcome probability good indicator rule accuracy suffers large estimation error insufficient training data example rule detected work total number observation estimate high smoothed version reliable estimate tiny size sample smoothing method proposed reduce estimation error different reason smoothing method suitable case approach tackle problem calculating lower confidence limit rule estimate seen minimal expected value rule draw large number sample certain confidence assume training data rule estimate worse limit lower confidence limit calculated function favour rule higher estimate obtained larger sample rule high estimate estimate obtained small sample rule lower estimate large sample valued higher smoothed zero positive negative outcome probability estimation rule value fact resembles tzoukermann scoring po disambiguation rule french tagger main difference function value assumed corresponds confidence standard approach adopt high confidence value range adopted confidence calculate score ith rule important consideration scoring word guessing rule longer affix ending rule confident coincidental small sample example estimate word ending obtained sample word estimate word ending fulness obtained sample word later case representative sample size need adjust estimation error accordance length affix ending good way divide value increase increase length experiment obtained length affix ending estimation error changed rule affix ending length estimation error reduced length etc longer length smaller sample considered representative confident rule estimation threshold certain level let rule score higher threshold included final rule set method setting threshold based empirical evaluation rule set described section scored lower threshold merged general rule scored",
        "threshold included final rule set merge rule scored threshold affix ending initial class score resulting rule higher score merged rule number positive observation increase number trial remains successful application merging resulting rule substitute merged one perform rule merging rule set rule included final set sorted score scored rule merged score resulting rule exceed threshold case added final rule set process applied merges rule scored threshold",
        "morphological word guessing rule describe word guessed given word known example rule say prefixing string word act past form verb vbd participle vbn produce adjective instance applying rule word undeveloped segment prefix remaining developed found lexicon vbd vbn conclude word undeveloped adjective po set guessing rule called initial class class po set guessed word called resulting class r class example vbd vbn class rule r class english language morphological word formation realised affixation prefixation suffixation affixation straightforward concatenation affix stem majority case obey simple concatenative regularity decided concentrate simple concatenative case kind morphological rule learned suffix rule rule applied tail word prefix rule rule applied beginning word example say stripping suffix unknown word produce word po class unknown word class vbd vbn rule work instance book booked water watered etc extract rule special operator applied pair word lexicon try segment affix leftmost string subtraction suffix rightmost string subtraction prefix subtraction result non string creates morphological rule storing po class shorter word class po class longer word r class example operator applied possible lexicon entry pair rule produced application extracted pair frequency count incremented different set guessing rule prefix suffix morphological rule frequency produced set guessing rule need cut infrequent rule bias learning process eliminate rule frequency certain threshold filtering reduces rule set leave coincidental case rule morphological guessing rule ending guessing rule require main form unknown word listed lexicon rule guess po class word basis ending character looking stem lexicon rule able cover unknown word morphological guessing rule accuracy high example ending guessing rulesays word end ing adjective noun gerund morphological rule rule ask check substring preceeding ing ending word particular po tag ending guessing rule look morphological rule class void collect rule set upper limit ending length equal character collect lexicon possible word ending length po class word ending detected appear operator example word different operator produce ending guessing rule operator applied",
        "morphological word guessing rule describe word guessed given word known example rule say prefixing string word act past form verb vbd participle vbn produce adjective instance applying rule word undeveloped segment prefix remaining developed found lexicon vbd vbn conclude word undeveloped adjective po set guessing rule called initial class class po set guessed word called resulting class r class example vbd vbn class rule r class english language morphological word formation realised affixation prefixation suffixation affixation straightforward concatenation affix stem majority case obey simple concatenative regularity decided concentrate simple concatenative case kind morphological rule learned suffix rule rule applied tail word prefix rule rule applied beginning word example say stripping suffix unknown word produce word po class unknown word class vbd vbn rule work instance book booked water watered etc extract rule special operator applied pair word lexicon try segment affix leftmost string subtraction suffix rightmost string subtraction prefix subtraction result non string creates morphological rule storing po class shorter word class po class longer word r class example operator applied possible lexicon entry pair rule produced application extracted pair frequency count incremented different set guessing rule prefix suffix morphological rule frequency produced set guessing rule need cut infrequent rule bias learning process eliminate rule frequency certain threshold filtering reduces rule set leave coincidental case rule",
        "morphological guessing rule ending guessing rule require main form unknown word listed lexicon rule guess po class word basis ending character looking stem lexicon rule able cover unknown word morphological guessing rule accuracy high example ending guessing rulesays word end ing adjective noun gerund morphological rule rule ask check substring preceeding ing ending word particular po tag ending guessing rule look morphological rule class void collect rule set upper limit ending length equal character collect lexicon possible word ending length po class word ending detected appear operator example word different operator produce ending guessing rule operator applied entry lexicon way described operator morphological rule infrequent rule filtered",
        "course acquired rule good plausible guess word class rule accurate guessing rule frequent application acquired rule need estimate effective rule worth retaining final rule set estimation perform statistical experiment follows rule calculate number time rule applied word token raw corpus number time gave right answer task rule disambiguate word po provide possible po rule correct majority time applied good rule rule wrong time bad rule included final rule set perform experiment rule rule set produced rule extraction phase word token corpus guess po set rule rule applicable word example guessing rule strip particular suffix current word corpus suffix classify word rule incompatible rule applicable word rule applicable word perform look lexicon word compare result guess information listed lexicon guessed po set po set stated lexicon count success failure value guessing rule correlate estimated proportion success proportion positive outcome rule application total number trial fact attempt apply rule compatible word corpus smooth zero positive negative outcome probability good indicator rule accuracy suffers large estimation error insufficient training data example rule detected work total number observation estimate high smoothed version reliable estimate tiny size sample smoothing method proposed reduce estimation error different reason smoothing method suitable case approach tackle problem calculating lower confidence limit rule estimate seen minimal expected value rule draw large number sample certain confidence assume training data rule estimate worse limit lower confidence limit calculated function favour rule higher estimate obtained larger sample rule high estimate estimate obtained small sample rule lower estimate large sample valued higher smoothed zero positive negative outcome probability estimation rule value fact resembles tzoukermann scoring po disambiguation rule french tagger main difference function value assumed corresponds confidence standard approach adopt high confidence value",
        "range adopted confidence calculate score ith rule important consideration scoring word guessing rule longer affix ending rule confident coincidental small sample example estimate word ending obtained sample word estimate word ending fulness obtained sample word later case representative sample size need adjust estimation error accordance length affix ending good way divide value increase increase length experiment obtained length affix ending estimation error changed rule affix ending length estimation error reduced length etc longer length smaller sample considered representative confident rule estimation threshold certain level let rule score higher threshold included final rule set method setting threshold based empirical evaluation rule set described section",
        "rule scored lower threshold merged general rule scored threshold included final rule set merge rule scored threshold affix ending initial class score resulting rule higher score merged rule number positive observation increase number trial remains successful application merging resulting rule substitute merged one perform rule merging rule set rule included final set sorted score scored rule merged score resulting rule exceed threshold case added final rule set process applied merges rule scored threshold",
        "important question arise rule acquisition stage choose scoring threshold performance rule set produced different threshold task assigning set po tag word similar task document categorisation document assigned set descriptor represent content performance assignment measured recall percentage po assigned guesser word percentage po guesser assigned total number po assigned word proportion word guesser able classify experiment measured word precision word recall micro average type data use stage evaluated guessing rule actual lexicon word lexicon closed class word word shorter character guessed different guessing strategy result compared information word lexicon evaluation experiment measured performance guessing rule training corpus word computed metric previous experiment multiplied result corpus frequency particular word averaged frequent word greatest influence aggreagte measure concentrated finding best threshold rule set rule set produced different threshold recorded metric chose set best aggregate table result experiment shown best threshold detected ending rule point suffix rule prefix rule notice slight difference result obtained lexicon corpus corpus result better training technique targeted rule set frequent case corpus lexicon average ending guessing rule detected cover unknown word precision interpreted word different po po class ending guessing rule assign time recall required po assigned guess comparison xerox word ending guesser taken base line model detect substantial increase precision cheerful increase coverage mean xerox guesser creates ambiguity disambiguator assigning po example handle unknown word fact decrease performance lower comparison ending guessing rule morphological rule better precision better accuracy guessing word guessed morphological rule guessed correct recall precision coverage morphological rule lower ending guessing one suffix rule prefix rule obtaining optimal rule set performed experiment word sample included training lexicon corpus gathered word lexicon developed wall street journal corpus collected frequency word corpus experiment",
        "obtained similar metric coverage dropped ending xerox rule set suffix rule set come surprise main form required suffix rule missing lexicon experiment evaluated morphological rule add improvement conjunction ending guessing rule evaluated detail conjunctive application xerox guesser boost performance previous experiment measured precision recall coverage lexicon corpus demonstrates result experiment table show xerox guesser applied guesser measure drop performance xerox guesser applied guesser sufficient change performance noticed prof rule set supercedes xerox rule set second table show cascading application morphological rule set ending guessing rule increase overall precision guessing make improvement base line xerox guesser precision coverage",
        "direct evaluation rule set gave ground comparison selection performing guessing rule set task unknown word guessing subtask overall speech tagging process interested advantage rule set affect tagging performance performed independent evaluation impact word guesser tagging accuracy evaluation tried different tagger tagger implementation lisp implemented hmm xerox tagger described kupiec tagger rule based tagger brill tagger come data word guessing component pre trained brown corpus gave search space combination xerox tagger equipped original xerox guesser brill tagger original guesser xerox tagger cascading guesser brill tagger cascading guesser word failed guessed guessing rule applied standard method classifying common noun capitalised sentence proper noun base line result measured performance tagger known word word sample evaluation tagging accuracy unknown word pay attention metric measure accuracy tagging unknown word metric give exact measure tagger unknown word case account known word mi tagged guesser perspective aspect measure overall tagging performance brown corpus model general language model principle restriction type text performance lower model specialised particular sublanguage want stress primary task evaluate tagger performance word guessing module worry tuning tagger text brown corpus model tagged text different origin brown corpus text seen training phase mean tagger guesser trained text word unknown lexicon text performed tagging experiment experiment tagged text brown corpus lexicon supplied tagger unknown word occur text second experiment tagged text lexicon contained closed class short word small lexicon contained entry entry original brown corpus lexicon word considered unknown guessed guesser obtained stable result experiment typical example tagging text word text detected unknown word tagged text different combination tagger word guesser fledged lexicon result tagging summarised table xerox tagger original guesser unknown word tagged accuracy unknown word measured xerox tagger equipped",
        "cascading guesser accuracy unknown word increased upto situation detected brill tagger general accurate xerox cascading guesser performed brill original guesser boosting performance unknown word accuracy tagger set unknown word known lexicon detected tagger second experiment tagged text way small lexicon word text unknown small lexicon result tagging summarised table accuracy tagger unknown word known lexicon lower previous experiment xerox tagger brill tagger performance guesser lower previous experiment fact semi closed class adverb etc missing small lexicon accuracy tagging unknown word dropped general best result unknown word obtained cascading guesser brill tagger xerox type mi taggings caused guesser occured type guesser provided broader po class unknown word tagger difficulty disambiguation broader class case ing word general act noun adjective gerund direct lexicalization restrict search space case word going adjective noun gerund second type mi tagging caused wrong assignment po guesser case irregular word example cattle guessed singular noun fact plural noun nns",
        "presented technique unsupervised statistical acquisition rule guess possible part speech word unknown lexicon technique require prepared training data us training lexicon word frequency collected raw corpus training data type guessing rule learned prefix morphological rule suffix morphological rule ending guessing rule select performing guessing rule set suggested evaluation methodology dedicated performance speech guesser tagging accuracy unknown word text unseen guesser tagger training phase showed tagging induced cascading guesser accurate quoted result known author cascading guesser outperformed guesser supplied xerox tagger guesser supplied brill tagger accuracy unknown word cascading guesser detected tagging fledged lexicon tagging closed class short word lexicon unknown word known lexicon accuracy tagging detected make accuracy drop caused cascading guesser general important conclusion evaluation experiment morphological guessing rule improve guessing performance accurate ending guessing rule applied ending guessing rule improve precision guessing result higher accuracy tagging unknown word acquired guessing rule employed cascading guesser fact standard nature form po guesser point rule set acquired presented technique accurate learning rule lexicon tagged corpus guesser task akin lexicon lookup tuned statistical scoring procedure account rule feature frequency distribution empirical way determine optimum collection rule acquired rule subject rigorous direct evaluation term precision recall coverage rule applied accurate rule important issue induction guessing rule set choice right data training approach guessing rule extracted lexicon actual corpus frequency word usage allow discrimination rule productive left imprint basic lexicon rule productive real life text major factor learning process lexicon guessing rule meant capture general language regularity lexicon general possible list possible po word large possible corresponding corpus include word lexicon large obtain reliable estimate word frequency distribution experiment lexicon word frequency derived brown corpus considered general model english resulted guessing rule set proved domain corpus independent producing similar result test text different origin general performance cascading guesser worse lookup general language lexicon room improvement extraction morphological rule attempt model non concatenative case english letter mutation",
        "occur letter main word possible account goal extract morphological rule letter mutation end account case like try try reduce reducing advise advisable expect increase coverage thesuffix morphological rule contribute overall guessing accuracy avenue improvement provide guessing rule probability emission po resulting po class information compiled improve accuracy tagging unknown word described rule acquisition evaluation method implemented modular set awk tool guesser extendable sub language specific regularity retrainable new tag set language provided language affixational morphology software produced guessing rule set available contacting author",
        "word vector reflecting word meaning expected enable numerical approach semantics early attempt vector representation psycholinguistics semantic differential approach osgood associative distribution approach deese derived psychological experiment early attempt automation wilks co occurrence statistic promising result co occurrence vector word sense disambiguation schuetze word clustering pereira co occurrence statistic requires huge corpus cover rare word developed word vector derived ordinary dictionary measuring inter word distance word definition niwa nitta method nature problem handling rare word paper examine usefulness distance vector semantic representation comparing co occurrence vector",
        "reference network word dictionary fig measure distance word network graph show word definition word nitta network shown fig small portion reference network collins english dictionary edition cd rom liberman head word m definition word example definition dictionary book word language listed word dictionary linked word book word language alphabetical word vector defined list distance word certain set selected word origin word fig marked unit book people assumed origin word principle origin word chosen experiment middle frequency word frequent word reference collins english dictionary ced distance vector dictionary derived follows th element distance length shortest path dictionary th origin begin assume link constant length actual definition link length given word definition word word expected related basis hypothesis distance reference network reflect associative distance word nitta reference networksreference network neural network vronis ide word sense disambiguation field artificial association spreading activation kozima furugori context coherence measurement distance vector word considered list activation strength origin node word node activated distance vector expected convey information entire network easier handle dictionariesas semantic representation word distance vector expected depend weakly particular source dictionary compared set distance vector ldoce procter cobuild sinclair verified difference smaller difference word definition niwa nitta describe technical detail derivation distance vector lengthdistance measurement reference network depends definition link length assumed simplicity link constant length simple definition unnatural reflect word frequency path low frequency word rare word implies strong relation measured shorter path use following definition link length take account word frequency show length link word fig denotes total number link denotes number direct link word vector normalized changing coordinate deviation coordinate average standard deviation distance origin coordinate changed deviation vector average standard deviation",
        "use ordinary co occurrence statistic measure co occurrence likelihood word mutual information estimate church hank occurrence density word corpus conditional probability density neighborhood word neighborhood defined word appearance word variety neighborhood definition surrounding word yarowsky distance word ignoring function word dagan logarithm defined argument estimate neglected accidental frequent church hank co occurence vector word defined list co occurrence likelihood word certain set origin word set origin word distance vector frequency measure co occurence likelihood case exceptional sparseness problem known co occurrence statistic corpus wall street journal cd rom liberman total word number word appeared total head word ced percentage word origin pair appeared total pair co occurrence likelihood measured value set",
        "compared vector representation following semantic task word sense disambiguation wsd based similarity context vector second learning meaning example word wsd precision co occurrence vector word corpus higher distance vector ced sense disambiguation semantic problem variety approach proposed solving example vronis ide reference network neural network hearst shallow syntactic similarity context cowie simulated annealing quick parallel disambiguation yarowsky co occurrence statistic word thesaurus category disambiguation method based similarity context vector originated wilks method context vector sum constituent word vector target word context vector context isthe similarity context measured angle vector inner product normalized vector word sense following context example infer sense word arbitrary context context example possible way infer sense choose sense average maximum selected method similar example important average similarity page show disambiguation precision word word selected sens shown graph sens chosen different collect sufficient number context example name sens chosen category name roget international thesaurus organ result distance vector shown dot co occurrence vector wsj word circle context size x axis example mean word target word word target word example sense taken wsj test context wsj number test context varies word word precision simple average respective precision sens result fig precision co occurrence vector higher distance vector case interest custom found case distance vector higher precision conclude co occurrence vector advantageous distance vector wsd based context similarity sparseness problem co occurrence vector case context consists plural word experiment vector representation measure learning meaning show change precision percentage agreement author combined judgement x axis indicates number example word pair nearest example example test word shown table case distance vector advantageous precision distance vector increased leveled precision co occurrence vector stayed conclude property positive negative reflected distance vector co occurrence vector sparseness problem supposed major factor case",
        "experiment discussed corpus size co occurrence vector set word wsj vector dimension co occurrence distance vector set supplementary data support parameter settingscorpus size co occurrence vector show change disambiguation precision corpus size co occurrence statistic increase word word word suit issue race context size number example sense graph level word corpus size word small dimension page show dependence disambiguation precision vector dimension forco occurrence anddistance vector co occurrence vector precision level dimension dimension size sufficient redundant distance vector case clear precision leveling increasing dimension",
        "word sense disambiguation semantic problem variety approach proposed solving example vronis ide reference network neural network hearst shallow syntactic similarity context cowie simulated annealing quick parallel disambiguation yarowsky co occurrence statistic word thesaurus category disambiguation method based similarity context vector originated wilks method context vector sum constituent word vector target word context vector context isthe similarity context measured angle vector inner product normalized vector word sense following context example infer sense word arbitrary context context example possible way infer sense choose sense average maximum selected method similar example important average similarity page show disambiguation precision word word selected sens shown graph sens chosen different collect sufficient number context example name sens chosen category name roget international thesaurus organ result distance vector shown dot co occurrence vector wsj word circle context size x axis example mean word target word word target word example sense taken wsj test context wsj number test context varies word word precision simple average respective precision sens result fig precision co occurrence vector higher distance vector case interest custom found case distance vector higher precision conclude co occurrence vector advantageous distance vector wsd based context similarity sparseness problem co occurrence vector case context consists plural word",
        "experiment vector representation measure learning meaning show change precision percentage agreement author combined judgement x axis indicates number example word pair nearest example example test word shown table case distance vector advantageous precision distance vector increased leveled precision co occurrence vector stayed conclude property positive negative reflected distance vector co occurrence vector sparseness problem supposed major factor case",
        "experiment discussed corpus size co occurrence vector set word wsj vector dimension co occurrence distance vector set supplementary data support parameter settingscorpus size co occurrence vector show change disambiguation precision corpus size co occurrence statistic increase word word word suit issue race context size number example sense graph level word corpus size word small dimension page show dependence disambiguation precision vector dimension forco occurrence anddistance vector co occurrence vector precision level dimension dimension size sufficient redundant distance vector case clear precision leveling increasing dimension",
        "text sequence word coherent structure meaning word text depends structure text structure text essential task text understanding grosz sidner valuable indicator structure text lexical cohesion halliday hasan cohesion relationship word classified follows reiteration semantic relation reiteration word easy capture morphological analysis relation word focus paper recognize computer consider lexical cohesion semantic similarity word computed spreading activation association waltz pollack semantic network constructed english dictionary edited lexicographer dictionary set associative relation shared people linguistic community similarity word mapping set word lexicon following example suggest feature similarity value increase strength semantic relation following section examines related work order clarify nature semantic similarity describes semantic network constructed english dictionary explains measure similarity spreading activation semantic network show application similarity measure computing similarity text measuring coherence text discus theoretical aspect similarity",
        "word language organized kind relationship syntagmatic relation word arranged sequential text paradigmatic relation word associated word defined syntagmatic paradigmatic relation similarity based co occurrence data extracted corpus church hank definition dictionary wilks similarity based association data extracted thesaurus morris hirst psychological experiment osgood paper concentrate paradigmatic similarity paradigmatic relation established sentence sentence boundary syntagmatic relation seen sentence syntax deal sentence structure rest section focus related work measuring paradigmatic similarity psycholinguistic approach thesaurus based approach proposed method measuring similarity pioneering work semantic differential osgood analysis meaning word range different dimension opposed adjective end figure locates word semantic space work knowledge representation related osgood semantic differential describe meaning word special symbol microfeatures waltz pollack hendler correspond semantic dimension following problem arise semantic differential procedure measurement meaning procedure based denotative meaning word connotative emotion attached word difficult choose relevant dimension dimension required sufficient semantic space hirst roget thesaurus knowledge base determining word related example semantic relation truck car drive car captured following way method capture type semantic relation emotional situational relation paraphrasing superordinate cat pet systematic relation north east non systematic relation theatre film thesaurus provide information semantic difference word juxtaposed category strength semantic relation word dealt paper reason thesaurus designed help writer find relevant word provide meaning word",
        "psycholinguist proposed method measuring similarity pioneering work semantic differential osgood analysis meaning word range different dimension opposed adjective end figure locates word semantic space work knowledge representation related osgood semantic differential describe meaning word special symbol microfeatures waltz pollack hendler correspond semantic dimension following problem arise semantic differential procedure measurement meaning procedure based denotative meaning word connotative emotion attached word difficult choose relevant dimension dimension required sufficient semantic space",
        "morris hirst roget thesaurus knowledge base determining word related example semantic relation truck car drive car captured following way method capture type semantic relation emotional situational relation paraphrasing superordinate cat pet systematic relation north east non systematic relation theatre film thesaurus provide information semantic difference word juxtaposed category strength semantic relation word dealt paper reason thesaurus designed help writer find relevant word provide meaning word",
        "analyse word meaning term semantic space defined semantic network called paradigme constructed glossme subset english dictionary dictionary closed paraphrasing system natural language headword defined phrase composed headword derivation dictionary viewed look tangled network word adopted longman dictionary contemporary english ldoce closed system english unique feature headword defined word longman defining vocabulary ldv derivation consists word headword ldoce based survey restricted vocabulary west reduced version ldoce called glossme entry ldoce headword included ldv ldv defined glossme glossme composed ldv closed subsystem english entry consist word word entry average item glossme headword word class unit corresponding numbered definition entry ldoce unit head det part head phrase definition describes broader meaning headword det part restrict meaning head figure translated glossme semantic network paradigme entry glossme mapped node paradigme node unnamed link node link node average show sample node node consists headword word class activity value set link rfrant rfr rfrant node consists subrfrants correspond unit glossme shown figure morphological analysis map word brownish second unit link node word colour link adjective noun rfr node record node referring example rfr set link node ex link rfrants rfr provides information extension intension shown rfrant link thickness computed frequency word glossme information normalized subrfrant rfr subrfrant thickness example subrfrant computed order unit represents significance definition describes structure paradigme detail",
        "dictionary closed paraphrasing system natural language headword defined phrase composed headword derivation dictionary viewed look tangled network word adopted longman dictionary contemporary english ldoce closed system english unique feature headword defined word longman defining vocabulary ldv derivation consists word headword ldoce based survey restricted vocabulary west reduced version ldoce called glossme entry ldoce headword included ldv ldv defined glossme glossme composed ldv closed subsystem english entry consist word word entry average item glossme headword word class unit corresponding numbered definition entry ldoce unit head det part head phrase definition describes broader meaning headword det part restrict meaning head figure",
        "translated glossme semantic network paradigme entry glossme mapped node paradigme node unnamed link node link node average show sample node node consists headword word class activity value set link rfrant rfr rfrant node consists subrfrants correspond unit glossme shown figure morphological analysis map word brownish second unit link node word colour link adjective noun rfr node record node referring example rfr set link node ex link rfrants rfr provides information extension intension shown rfrant link thickness computed frequency word glossme information normalized subrfrant rfr subrfrant thickness example subrfrant computed order unit represents significance definition describes structure paradigme detail",
        "similarity word computed spreading activation paradigme node hold activity move link node computes activity value time follows sum weighted activity time node referred rfrant rfr activity given outside time activate node let output function sum activity value appropriate proportion limit output value give detail spreading activation node certain period time cause activity spread paradigme produce activated pattern activated pattern get equilibrium step reach actual equilibrium pattern produced represents meaning node word related node morphological analysis activated pattern produced word suggests similarity headword ldv similarity computed following way figure reset activity node paradigme strength step significance word activated pattern produced paradigme activity value node word significance defined normalized information word corpus west example word red appears time word corpus word appears time computed follows estimated significance word excluded word list west average significance word class interpolation enlarged west word corpus example let consider similarity red orange produce activated pattern paradigme figure case node adjective noun activated strength compute observe similarity red orange obtained follows procedure described compute similarity word ldv derivation program procedure spreading activation morphological analysis common lisp compute second workstation sparcstation similarity word work indicator lexical cohesion following example illustrate increase strength semantic relation similarity increase co occurrence tendency word example note direction equal meaningful word higher similarity meaningless word function word lower similarity similarity increase significance represent meaningfulness note reflective similarity depends significance similarity word ldv derivation measured paradigme similarity extra word measured paradigme treating extra word word list definition ldoce note included ldv derivation similarity word list defined follows figure activated pattern produced activating strength step output function limit value shown figure high activity pattern produced phrase red alcoholic drink overlapped pattern implies bottle wine example similarity linguistics stylistics extra word computed follows extra word computable",
        "activating node certain period time cause activity spread paradigme produce activated pattern activated pattern get equilibrium step reach actual equilibrium pattern produced represents meaning node word related node morphological analysis activated pattern produced word suggests similarity headword ldv similarity computed following way figure reset activity node paradigme strength step significance word activated pattern produced paradigme activity value node word significance defined normalized information word corpus west example word red appears time word corpus word appears time computed follows estimated significance word excluded word list west average significance word class interpolation enlarged west word corpus example let consider similarity red orange produce activated pattern paradigme figure case node adjective noun activated strength compute observe similarity red orange obtained follows",
        "procedure described compute similarity word ldv derivation program procedure spreading activation morphological analysis common lisp compute second workstation sparcstation similarity word work indicator lexical cohesion following example illustrate increase strength semantic relation similarity increase co occurrence tendency word example note direction equal meaningful word higher similarity meaningless word function word lower similarity similarity increase significance represent meaningfulness note reflective similarity depends significance",
        "similarity word ldv derivation measured paradigme similarity extra word measured paradigme treating extra word word list definition ldoce note included ldv derivation similarity word list defined follows figure activated pattern produced activating strength step output function limit value shown figure high activity pattern produced phrase red alcoholic drink overlapped pattern implies bottle wine example similarity linguistics stylistics extra word computed follows extra word computable compute similarity headword ldoce derivation",
        "section show application similarity word text analysis measuring similarity text measuring text coherence text word list syntactic structure similarity text computed similarity extra word described following example suggest similarity text indicates strength coherence relation worth noting meaningless iteration word function word influence text similarity text similarity provides semantic space text retrieval recall similar text given text activated pattern text produced paradigme compute compare similarity figure consider reflective similarity text use notation computed follows activated pattern shown figure represents average meaning represents cohesiveness semantic closeness semantic compactness related distortion clustering following example suggest indicates strength coherence coherent incoherent cohesive text incoherent following example show cohesiveness incoherent text sentence selected ldoce incoherent cohesive capture aspect text coherence based lexical cohesion word",
        "structure paradigme represents knowledge system english activated state produced represents word meaning section discus nature structure state paradigme nature similarity computed set possible activated pattern produced paradigme considered semantic space state represented point semantic space dimensional hypercube edge corresponds word ldv selected according following information word frequency written english range context word appears ldv potential covering concept found world implies completeness ldv dimension semantic space semantic differential procedure adjective dimension semantic measurement us dimension completeness objectivity method applied construct semantic network ordinary dictionary defining vocabulary restricted network large spread activity small complete network measuring similarity proposed similarity based denotational intensional definition dictionary ldoce connotational extensional knowledge cause unexpected result measuring similarity example consider following similarity nature dictionary definition indicate sufficient condition headword example definition tree ldoce tell leaf tree tall plant wooden trunk branch life year bush plant treelike form drawing branching form showing family relationship definition followed picture leafy tree providing reader connotational extensional stereotype tree proposed method definition ldoce treated word list phrase syntactic structure consider following definition lift imagine moving movement represented activated pattern produced phrase meaning phrase sentence text represented pattern changing time need static paradigmatic relation paradox arises measuring similarity text text coherence seen section difference similarity text similarity word list coherence text cohesiveness word list similarity word concerned assume activated pattern paradigme approximate meaning word picture express story",
        "set possible activated pattern produced paradigme considered semantic space state represented point semantic space dimensional hypercube edge corresponds word ldv selected according following information word frequency written english range context word appears ldv potential covering concept found world implies completeness ldv dimension semantic space semantic differential procedure adjective dimension semantic measurement us dimension completeness objectivity method applied construct semantic network ordinary dictionary defining vocabulary restricted network large spread activity small complete network measuring similarity",
        "proposed similarity based denotational intensional definition dictionary ldoce connotational extensional knowledge cause unexpected result measuring similarity example consider following similarity nature dictionary definition indicate sufficient condition headword example definition tree ldoce tell leaf tree tall plant wooden trunk branch life year bush plant treelike form drawing branching form showing family relationship definition followed picture leafy tree providing reader connotational extensional stereotype tree",
        "proposed method definition ldoce treated word list phrase syntactic structure consider following definition lift imagine moving movement represented activated pattern produced phrase meaning phrase sentence text represented pattern changing time need static paradigmatic relation paradox arises measuring similarity text text coherence seen section difference similarity text similarity word list coherence text cohesiveness word list similarity word concerned assume activated pattern paradigme approximate meaning word picture express story",
        "described measurement semantic similarity word similarity word computed spreading activation semantic network paradigme constructed subset english dictionary ldoce compute similarity word ldv similarity word ldoce similarity word provides new method analysing structure text applied computing similarity text measuring cohesiveness text suggests coherence text seen section applying text segmentation grosz sidner youmans capture shift coherent scene story future research intend deal syntagmatic relation word text lie texture paradigmatic syntagmatic relation word hjelmslev provides dimension associative system word screen meaning word projected like picture dimension syntactic process treated film projected paradigme enables measure similarity text syntactic process word list regard paradigme field interaction text episode memory interaction hearing reading know schank meaning word sentence text projected uniform way paradigme seen section project text episode recall relevant episode interpretation text",
        "semantic network paradigme constructed small closed english dictionary glossme entry glossme mapped node paradigme following way figure entry glossme map unit subrfrant corresponding node paradigme word mapped link link following way let reciprocal number appearance root form glossme head let doubled node corresponds ex red divide proportion frequency link link node thickness set link link thickness normalize thickness link node compute thickness subrfrant following way let number subrfrants note normalize thickness rfr node paradigme following way node paradigme let rfr set subrfrant link let node referred let thickness new link rfr link thickness set link link thickness normalize thickness link",
        "node semantic network paradigme computes activity value time follows activity time collected node referred rfrant rfr activity given outside time output function limit value activity plausible subrfrant defined follows thickness j th subrfrant sum weighted activity node referred j th subrfrant defined follows thickness k th link activity time node referred k th link weighted activity node referred rfr thickness k th link activity time node referred k th link",
        "following work example marslen wilson carpenter altmann steedman accepted semantic interpretation human sentence processing occur sentence boundary clausal boundary accepted need incremental interpretation computational application early computational implementation motivated use incremental interpretation way dealing structural lexical ambiguity survey given haddock sentence following different syntactic par attachment ambiguity stabler par ruled structural preference parsing late closure minimal attachment frazier extraction correct set plausible reading requires use real world knowledge interpretation allows line semantic filtering par initial fragment implausible anomalous interpretation rejected preventing ambiguity multiplying parse proceeds line semantic filtering sentence processing drawback sentence processing serial architecture syntactic semantic processing performed parallel saving computation obtained line filtering balanced additional cost performing semantic computation par fragment ruled syntactic consideration sophisticated way packing ambiguity parsing use graph structured stack packed parse forest tomita task judging plausibility anomaly according context real world knowledge difficult problem domain contrast statistical technique lexeme co occurrence provide simple mechanism imitate semantic filtering case example judging bank financial institution plausible bank riverbank noun phrase rich bank compare number co occurrence lexeme rich bank riverbank rich bank financial institution analysed corpus statistical technique appropriate plausibility affected local context example consider ambiguous sentence contextssuch case involve reasoning interpretation immediate context opposed judging likelihood particular linguistic expression given application domain cooper discussion usefulness line semantic filtering processing complete sentence debatable filtering plausible role play interactive real time environment interactive spell checker wirn argument incremental parsing environment choice semantic filtering line end sentence concentration early literature incremental interpretation semantic filtering distracted application provide controversial application consider detail graphical interface dialogue foundation intelligent graphic project fig considered way natural language input computer aided design system particular application studied computer aided kitchen design user professional designer interpretation considered useful enabling immediate visual feedback feedback provide confirmation example highlighting object referred successful definite description user improved chance achieving successful reference example set possible referent definite noun phrase highlighted",
        "word word processing user know little information required successful reference dialogue particular task oriented dialogue characterised large number self repair levelt carletta hesitation insertion replacement common find interruption requesting extra clarification disagreement end sentence possible sentence started dialogue participant finished involving understanding dialogue include information extraction conversational database computer monitoring conversation useful include feature human dialogue man machine dialogue example interruption early signalling error ambiguity consider example self repair add extra information modifier correct piece information case information corrected material incorporated final message example consider corrected material main source data come provides antecedent pronoun corrected material tell man old wife pronoun bound quantifier boy system understand dialogue involving self repair require ability interpret use grammar includes self repair syntactic construction akin non constituent coordination relationship coordination self correction noted levelt system generate self repair require incremental interpretation assuming process system performs line monitoring output akin levelt model human self repair mechanism suggested generation self repair useful case severe time constraint changing background information carletta compelling argument incremental interpretation provided considering dialogue involving interruption following dialogue train corpus gross requires interpretation speaker end sentence allow objection apposition engine avon engine example potential use interruption human computer interaction following example interpretation end sentence constituent boundary verb phrase user command completed",
        "section shall review work providing semantic representation lambda expression word word layered model sentence processing build syntax tree sentence extract semantic representation adapt incremental perspective need able provide syntactic structure sort fragment sentence able extract semantic representation possibility explored categorial grammar tradition steedman provide grammar treat initial fragment constituent syntax tree semantics calculated alternative possibility link partial syntax tree formed non constituent functional semantic representation example fragment missing noun phrase john like associated semantics function entity truth value partial syntax tree given fig associated semantic representation like john categorial approach incremental interpretation approach use partial syntax tree difficulty case left recursion sentence fragment mary think john possible partial syntax tree provided fig possible partial tree fact different tree possible completed sentence large number intermediate node lower node lower example john embedded gerund mary think john leaving mistake turn embedded mary think john leaving mistake surprising embedded sentence sentence modifier requiring node mary think john home embedded mary think john home tired problem arbitrary number different partial tree particular fragment reflected current approach incremental interpretation incomplete word word example incomplete parser proposed stabler moortgat system simple parser deal left recursive grammar m system based lambek calculus problem infinite number possible tree fragment replaced corresponding problem initial fragment having infinite number possible type complete incremental parser word word proposed pulman based arc eager left corner parsing resnik enable complete word word parsing requires way encoding infinite number partial tree possibility use language describing tree express fact john dominated node specify dominated d theory marcus representation formed word word extracting default syntax tree strengthening dominance link immediated dominance link possible second possibility factor recursive structure grammar phrase structure grammar creating equivalent tree adjoining grammar joshi parser resulting grammar allows linear parsing infinitely parallel system absorption word performed constant time choice point finite number possible new partial tag tree tag tree represents infinite number tree formed adjunction possible extract default semantic",
        "value taking semantics tag tree assuming adjunction similar system proposed shieber johnson possibility suggested considering semantic representation appropriate word word parse number different partial tree fragment mary think john semantics fragment represented lambda expression consider lambda abstraction functional item type thought way encoding infinite set partial semantic tree structure example eventual semantic structure embed john depth second expression functional item type allows eventual structure main sentence embedded possibility provide syntactic correlate lambda expression practice provided interested mapping string word semantic representation need explicit syntax tree constructed use type syntactic lambda expression expression approach taken milward order provide complete word word incremental interpretation simple lexicalised grammar lexicalised version formal dependency grammar simple categorial grammar processing sentence mary introduced john susan word word approach milward provides following logical form corresponding sentence fragment absorbed input level representation appropriate meaning incomplete sentence proposition function proposition chater argued derived meaning judged plausibility turned quantified proposition example judging plausibility judge plausibility proposition mary introduced generalized quantifier notation form quantifier variable restrictor body lambda expression built word word proposition formed need retracted resulting inference example mary introduced inappropriate final sentence mary introduced rough algorithm follows parse new word wordform new lambda expression combining lambda expression formed parsing word lexical semantics wordform proposition quantifying lambda abstracted variable entail retract conclusion plausibility implausible block derivation worth noting need retraction failure extract correct commitment proposition semantic content fragment mary introduced fact possible find pair possible continuation negation mary introduced mary introduced proposition compatible proposition negation trivial proposition chater discussion considered semantic representation involve quantifier existential quantifier introduced mechanism sentence quantifier ambiguity concerning quantifier wider scope example sentence preferred reading kid climbed tree universal quantifier scope existential sentence preferred reading universal quantifier scope existential preference",
        "established end sentence example sentence preference outer scope reading quantifier interpret child preference time grammar inner scope reading quantifier intuitive evidence backed considering garden path effect quantifier scope ambiguity called jungle path barwise original example following showed preference particular scope established overturned preference established end sentence potential sentence end need garden path effect example following psycholinguistic experimentation concerned scope preference point preference established kurtzman macdonald intuitive evidence hypothesis scope preference established end sentence leaf open possibility case scoping information interest hearer preference determined quantifier similar problem dealing fragment tree impossible predict level embedding noun phrase john fragment mary think john impossible predict scope quantifier fragment respect large number quantifier appear sentence problem avoided form packing simple way use unscoped logical form quantifier left situ similar representation hobbs shieber quasi logical form alshawi example fragment man give book given following representation quantified term consists quantifier variable restrictor body convert lambda expression unscoped proposition replace occurrence argument existential quantifier term case obtain scoped proposition obtained quantifier scoping algorithm lewin inside algorithm free variable constraint hobbs shieber proposition formed judged plausibility imitate jungle path phenomenon plausibility judgement need feed scoping procedure fragment example man taken scoped book processing fragment man gave book preference preserved determining scope sentence man gave book child quantifier scoping end sentence new quantifier scoped relative existing quantifier operator negation intensional verb etc preliminary implementation achieves annotating semantic representation node name recording quantifier discharged node order",
        "section shall review work providing semantic representation lambda expression word word layered model sentence processing build syntax tree sentence extract semantic representation adapt incremental perspective need able provide syntactic structure sort fragment sentence able extract semantic representation possibility explored categorial grammar tradition steedman provide grammar treat initial fragment constituent syntax tree semantics calculated alternative possibility link partial syntax tree formed non constituent functional semantic representation example fragment missing noun phrase john like associated semantics function entity truth value partial syntax tree given fig associated semantic representation like john categorial approach incremental interpretation approach use partial syntax tree difficulty case left recursion sentence fragment mary think john possible partial syntax tree provided fig possible partial tree fact different tree possible completed sentence large number intermediate node lower node lower example john embedded gerund mary think john leaving mistake turn embedded mary think john leaving mistake surprising embedded sentence sentence modifier requiring node mary think john home embedded mary think john home tired problem arbitrary number different partial tree particular fragment reflected current approach incremental interpretation incomplete word word example incomplete parser proposed stabler moortgat system simple parser deal left recursive grammar m system based lambek calculus problem infinite number possible tree fragment replaced corresponding problem initial fragment having infinite number possible type complete incremental parser word word proposed pulman based arc eager left corner parsing resnik enable complete word word parsing requires way encoding infinite number partial tree possibility use language describing tree express fact john dominated node specify dominated d theory marcus representation formed word word extracting default syntax tree strengthening dominance link immediated dominance link possible second possibility factor recursive structure grammar phrase structure grammar creating equivalent tree adjoining grammar joshi parser resulting grammar allows linear parsing infinitely parallel system absorption word performed constant time choice point finite number possible new partial tag tree tag tree represents infinite number tree formed adjunction possible extract default semantic",
        "value taking semantics tag tree assuming adjunction similar system proposed shieber johnson possibility suggested considering semantic representation appropriate word word parse number different partial tree fragment mary think john semantics fragment represented lambda expression consider lambda abstraction functional item type thought way encoding infinite set partial semantic tree structure example eventual semantic structure embed john depth second expression functional item type allows eventual structure main sentence embedded possibility provide syntactic correlate lambda expression practice provided interested mapping string word semantic representation need explicit syntax tree constructed use type syntactic lambda expression expression approach taken milward order provide complete word word incremental interpretation simple lexicalised grammar lexicalised version formal dependency grammar simple categorial grammar",
        "processing sentence mary introduced john susan word word approach milward provides following logical form corresponding sentence fragment absorbed input level representation appropriate meaning incomplete sentence proposition function proposition chater argued derived meaning judged plausibility turned quantified proposition example judging plausibility judge plausibility proposition mary introduced generalized quantifier notation form quantifier variable restrictor body lambda expression built word word proposition formed need retracted resulting inference example mary introduced inappropriate final sentence mary introduced rough algorithm follows parse new word wordform new lambda expression combining lambda expression formed parsing word lexical semantics wordform proposition quantifying lambda abstracted variable entail retract conclusion plausibility implausible block derivation worth noting need retraction failure extract correct commitment proposition semantic content fragment mary introduced fact possible find pair possible continuation negation mary introduced mary introduced proposition compatible proposition negation trivial proposition chater discussion",
        "considered semantic representation involve quantifier existential quantifier introduced mechanism sentence quantifier ambiguity concerning quantifier wider scope example sentence preferred reading kid climbed tree universal quantifier scope existential sentence preferred reading universal quantifier scope existential preference established end sentence example sentence preference outer scope reading quantifier interpret child preference time grammar inner scope reading quantifier intuitive evidence backed considering garden path effect quantifier scope ambiguity called jungle path barwise original example following showed preference particular scope established overturned preference established end sentence potential sentence end need garden path effect example following psycholinguistic experimentation concerned scope preference point preference established kurtzman macdonald intuitive evidence hypothesis scope preference established end sentence leaf open possibility case scoping information interest hearer preference determined",
        "dealing quantifier similar problem dealing fragment tree impossible predict level embedding noun phrase john fragment mary think john impossible predict scope quantifier fragment respect large number quantifier appear sentence problem avoided form packing simple way use unscoped logical form quantifier left situ similar representation hobbs shieber quasi logical form alshawi example fragment man give book given following representation quantified term consists quantifier variable restrictor body convert lambda expression unscoped proposition replace occurrence argument existential quantifier term case obtain scoped proposition obtained quantifier scoping algorithm lewin inside algorithm free variable constraint hobbs shieber proposition formed judged plausibility imitate jungle path phenomenon plausibility judgement need feed scoping procedure fragment example man taken scoped book processing fragment man gave book preference preserved determining scope sentence man gave book child quantifier scoping end sentence new quantifier scoped relative existing quantifier operator negation intensional verb etc preliminary implementation achieves annotating semantic representation node name recording quantifier discharged node order",
        "dynamic semantics adopts view meaning sentence lie truth condition way change representation information interpreter groenendijk stokhof glance view suited incremental interpretation groenendijk stokhof claim compositional nature dynamic predicate logic enables interpret text line manner processing interpreting basic unit come context created interpretation text quote misleading suggests direct mapping incremental semantics dynamic semantics possible incremental semantics expect information state interpreter updated word word contrast dynamic semantics order state updated determined semantic structure left right order lewin discussion example dynamic predicate logic groenendijk stokhof state threaded antecedent conditional consequent restrictor quantifier body interpreting input state evaluation john buy output state antecedent car impress case threading semantic structure opposite order order clause appear sentence intuitive justification direction threading dynamic semantics provided considering appropriate order evaluation proposition database natural order evaluate conditional add antecedent consequent proven sentence level simple narrative text presentation order natural order evaluation coincide ordering anaphor antecedent justify left right threading threading semantic structure threading left right disallows example optional cataphora example example compulsory cataphora threading antecedent conditionals consequent fails example possible sentence donkey reading indefinite consequent sentence reading talking particular student outer existential typical student generic reading noted zeevat use kind ordered threading tend fail bach peter sentence kind example possible use standard dynamic semantics prior level reference resolution reorder antecedent anaphor example converted donkey sentence consider threading possible world update semantics veltman need distinguish order evaluation order presentation clear cut trying perform threading left right order interpretation sentence john left mary left processing proposition john left set world refined world john left consider processing mary left want reintroduce world mary john left allowed update semantics eliminative new piece information refine set world worth noting difficulty trying combine eliminative semantics left right threading apply constraint based semantics update semantics us incremental refinement set possible referent example effect processing rabbit noun phrase rabbit hat provide set",
        "rabbit processing refines set rabbit processing hat refines set rabbit hat consider processing rabbit box time rabbit processed rabbit remaining consideration rabbit rule possibility noun phrase referring rabbit case parallel earlier example mary introduced inappropriate final sentence mary introduced discussion argued possible thread state dynamic eliminative semantics left right word word taken argument use semantics incremental interpretation required indirect approach present implementation semantic structure akin logical form built word word structure evaluated dynamic semantics threading performed according structure logical form",
        "present implementation performs mapping sentence fragment scoped logical representation illustrate operation consider following discourse assume sentence processed concentrate processing fragment implementation consists module word word incremental parser lexicalised version dependency grammar milward take fragment sentence map unscoped logical form module replaces lambda abstracted variable existential quantifier situ pronoun coindexing procedure replaces pronoun variable variable sentence preceding context quantifier scoping algorithm based lewin evaluation procedure based lewin take logical form containing free variable evaluates dynamic semantics context given preceding sentence output new logical form representing context variable bound present coverage module module naive coindexing procedure allows pronoun coindexed quantified variable proper noun context current sentence",
        "tagging mean hidden markov model hmm recognised effective technique assigning part speech corpus robust efficient manner attractive feature technique algorithm independent natural language applied knowledge engineering localised choice tagset method training training make use tagged corpus untagged corpus initial bootstrapping probability attention given technique effective example cutting suggest way training trigram tagger merialdo elworthy consider quality seeding data needed construct accurate tagger training tagger given language major knowledge engineering required localised choice tagset design appropriate tagset subject external internal criterion external criterion tagset capable making linguistic example syntactic morphological distinction required output corpus past included varying amount detail example penn treebank tagset marcus omits number distinction lob brown tagsets based garside francis kucera case surface form word allows distinction recovered needed auxiliary verb tag verb penn separated lob tagset second design criterion tagsets internal making tagging effective possible example common error tagger lob brown tagsets mistagging word subordinating conjunction preposition vice versa macklovitch higher level syntactic analysis indicating phrasal structure required predict tag correct information available fixed context tagger penn treebank us single tag case leaving resolution required process tagsets distinguish transitive intransitive verb tagger use context word able right prediction sort found corpus susanne parsed tagged problem tagset design important inflected language greek hungarian syntactic variation realised inflectional system represented tagset huge number tag impossible implement train simple tagger passing problem appears language inflected possible large work tagger word word morphological analysis language ambiguity tagging useful rich tagset criterion designed given careful consideration paper report experiment address internal design criterion looking tagging accuracy varies tagset modified english french swedish choice language dictated corpus available represent different degree complexity inflectional system system marking little plurality noun restricted range verb property little complexity gender number person marked swedish detailed marking gender number definiteness",
        "case subsidiary issue look tagger performs unknown word one seen training data usual approach hypothesise tag tagset unknown word one word tag enumerated advance closed class tag tagger perform unknown word tagsets derived taking initial tagset corpus manual tagging corpus condensing set tag represent grammatical distinction gender single tag change applied training corpus allows produce corpus tagged according different scheme having tag corpus change tagsets motivated grammatical consideration error observed account general look result tagging accuracy change size tagset change naive approach adopted goal continuing knowledge free tradition work hmm tagging aim experiment determine bigger tagset smaller external criterion requiring human intervention choose tagset result language turn different general conclusion overall contribution paper external criterion dominate tagset design limit knowledge free preliminary work note hard reason effect changing tagset argued smaller tagset improve tagging accuracy put burden tagger fine distinction information theoretic term number decision required smaller tagger need contribute information decision smaller tagset mean word possible tag handled detail tagset help tagger property adjacent word support choice tag transition tag contribute information tagger need example determiner noun marked number tagger model agreement simple noun phrase having higher probability singular determiner followed singular noun singular determiner followed plural noun help deciding point view dominate",
        "experiment conducted corpus word swedish text eci multilingual cd rom word english french corpus international telecommunication union text experiment corpus train model small sample text test data second experiment corpus training remainder testing importance second test includes unknown word difficult tag tagsets modified substituting simplified tag original one e running training test procedure modified corpus change tagset listed result follow identify tagset include given distinction uppercase letter one lowercase letter example tagset mark gender change based inflection gender masculine neuter common gender tagset number singular plural definiteness definite indefinite case nominative genitive change based inflection gender masculine feminine number singular plural person identified tagset verb treat avoir etre verb change varied language consisted removing finer subdivision major class grouping change little hoc intended good distribution tagset size combination tried reduce specific conjunction class common class simplify adjective class simplify noun adverb class simplify pronoun class number singular plural distinction removed use class verb size resulting tagsets degree ambiguity corpus resulted appear figure quoted ambiguous unknown word factor effect varying degree ambiguity tagset change fact approximate way accounting ambiguity length ambiguous sequence account accuracy deteriorate long sequence ambiguous word short one test run good turing correction probability estimate estimating probability transition tag tag count transition training corpus divided total frequency tag added count transition total tag frequency adjusted purpose correction correct corpus provide training data largest tagsets correction found slight reduction accuracy swedish improve french english accuracy suggesting needed experiment unknown word gave accuracy ambiguous word swedish french english result english low example penn treebank tagger give accuracy long sequence ambiguous word result appear table figure include degree ambiguity number word corpus tag hypothesised accuracy plotted size tagset figure",
        "number point correspond index tagsets listed pattern swedishlarger tagset give higher accuracy result spread accuracy tagsets mark gender gender marked tends larger accuracy spread swedish size tagsets ranged tag swedish french english discussed clear happen larger tagsets experiment based susanne corpus tagsets ranging tag suggest trend higher accuracy continues bigger tagsets second experiment test corpus included unknown word seen training tagger hypothesis open class tag result interesting look accuracy unknown word accuracy word ambiguous found training corpus result outline swedishsimilar result known word experiment unknown word smaller tagsets higher accuracy ambiguous word pattern accuracy similar experiment unknown word pattern accuracy similar tagsets include gender giving accuracy giving word gave similar result test word weak tendency higher accuracy smaller tagsets accuracy ambiguous word swedish french english corresponding accuracy unknown word list result giving tagset size degree ambiguity accuracy known ambiguous unknown word ambiguous word accuracy plotted figure come result consistent relationship size tagsets tagging accuracy common pattern larger tagset higher accuracy notable exception french gender marking key factor swedish unknown word reverse trend english unknown word clear trend fit difficulty suggested reasoning effect tagset size main conclusion paper knowledge engineering component setting tagger concentrate optimising tagset external criterion internal criterion tagset size sufficient generality taken account prior knowledge property language surprising useful experimental confirmation linguistics matter engineering",
        "experiment conducted corpus word swedish text eci multilingual cd rom word english french corpus international telecommunication union text experiment corpus train model small sample text test data second experiment corpus training remainder testing importance second test includes unknown word difficult tag tagsets modified substituting simplified tag original one e running training test procedure modified corpus change tagset listed result follow identify tagset include given distinction uppercase letter one lowercase letter example tagset mark gender change based inflection gender masculine neuter common gender tagset number singular plural definiteness definite indefinite case nominative genitive change based inflection gender masculine feminine number singular plural person identified tagset verb treat avoir etre verb change varied language consisted removing finer subdivision major class grouping change little hoc intended good distribution tagset size combination tried reduce specific conjunction class common class simplify adjective class simplify noun adverb class simplify pronoun class number singular plural distinction removed use class verb size resulting tagsets degree ambiguity corpus resulted appear figure quoted ambiguous unknown word factor effect varying degree ambiguity tagset change fact approximate way accounting ambiguity length ambiguous sequence account accuracy deteriorate long sequence ambiguous word short one test run good turing correction probability estimate estimating probability transition tag tag count transition training corpus divided total frequency tag added count transition total tag frequency adjusted purpose correction correct corpus provide training data largest tagsets correction found slight reduction accuracy swedish improve french english accuracy suggesting needed",
        "experiment unknown word gave accuracy ambiguous word swedish french english result english low example penn treebank tagger give accuracy long sequence ambiguous word result appear table figure include degree ambiguity number word corpus tag hypothesised accuracy plotted size tagset figure number point correspond index tagsets listed pattern swedishlarger tagset give higher accuracy result spread accuracy tagsets mark gender gender marked tends larger accuracy spread swedish size tagsets ranged tag swedish french english discussed clear happen larger tagsets experiment based susanne corpus tagsets ranging tag suggest trend higher accuracy continues bigger tagsets second experiment test corpus included unknown word seen training tagger hypothesis open class tag result interesting look accuracy unknown word accuracy word ambiguous found training corpus result outline swedishsimilar result known word experiment unknown word smaller tagsets higher accuracy ambiguous word pattern accuracy similar experiment unknown word pattern accuracy similar tagsets include gender giving accuracy giving word gave similar result test word weak tendency higher accuracy smaller tagsets accuracy ambiguous word swedish french english corresponding accuracy unknown word list result giving tagset size degree ambiguity accuracy known ambiguous unknown word ambiguous word accuracy plotted figure come result consistent relationship size tagsets tagging accuracy common pattern larger tagset higher accuracy notable exception french gender marking key factor swedish unknown word reverse trend english unknown word clear trend fit difficulty suggested reasoning effect tagset size main conclusion paper knowledge engineering component setting tagger concentrate optimising tagset external criterion internal criterion tagset size sufficient generality taken account prior knowledge property language surprising useful experimental confirmation linguistics matter engineering",
        "final observation experiment accuracy unknown word low test bad swedish tagger experiment took simple minded approach unknown word alternative limit possible tag simple morphological analysis examination surface form word example variant english tagger experiment module reduces range possible tag based testing surface characteristic capitalisation word ending improved unknown word accuracy result unknown word argument favouring larger tagsets tendency higher accuracy tentative experiment contribution morphological surface analysis french swedish carried language unknown word second experiment looked lexicon trained corpus tag swedish unknown word came inflected class single tag french figure case unknown word tag provides hope inflectional analysis help unknown word confirmation list french unknown word given french grammarian predicted good guess correct tag morphology word narrow possible tag research needed determine realistic estimate turn",
        "shown simple experiment changing tagset show relationship tagset size accuracy weak consistent language folklore tagging community smaller tagsets held obtaining good accuracy suggested important choose tagset required application optimise tagger follow work apply similar test language provide confirmation result language family similar characteristic identified conclusion corpus tagged hand large tagset reduced smaller application demand major factor set danger introducing human error manual tagging process increasing cognitive load human annotator",
        "bernard lang defines parsing calculation intersection fsa input cfg viewing input parsing fsa string combine approach speech understanding system parsing take word lattice input word string certain technique robust parsing modelled finite state transducer paper investigate generalize approach unification grammar particular concentrate calculation intersection fsa dcg shown existing parsing algorithm extended fsa input termination property change undecidable intersection fsa dcg dcg line parsable discus approach cope problem",
        "paper concerned syntactic analysis phase natural language understanding system input system sequence word following bernard lang argue fruitful input finite state automaton fsa model case uncertain actual input uncertain input necessary case formed textual input case speech input example natural language understanding system interfaced speech recognition component chance compenent uncertain actual string word uttered produce word lattice promising hypothesis single sequence word generalizes word lattice example certain technique deal formed input characterized finite state transducer lang composition input string finite state transducer result fsa input syntactic parsing approach allows treatment missing extraneous interchanged misused word teitelbaum saito tomita nederhof bertsch technique use case written spoken language input case possible application concern treatment phenomenon repair carter allow input fsa including cycle etc mentioned technique result cycle ordinary word graph defines finite language fsa course define infinite number sentence emerge treat unknown sequence word sentence unknown part unknown length lang suggested acl reviewer try model haplology phenomenon english sentence chef joe hat joe restaurant finite state transducer straightforward approach lead finite state automaton cycle shown computation intersection fsa cfg requires minimal generalization existing parsing algorithm replace usual string position name state fsa straightforward complexity process cubic number state fsa case ordinary parsing number state equal lang billot lang assuming right hand side grammar rule category paper investigate technique applied case grammar constraint based grammar cfg specificity grammar definite clause grammar dcg pereira warren dcg simple example family constraint based grammar formalism natural language analysis generation main finding paper extended member family constraint based grammar formalism",
        "calculation intersection cfg fsa simple bar hillel context free grammar defining intersection constructed keeping track state name non terminal category symbol rule rule transition rule intersection fsa cfg cfg derives parse tree grammar called parse forest grammar construction show intersection fsa cfg cfg practical interest reason construction yield enormous rule useless fact large parse forest grammar define language intersection ordinary recognizers parser cfg generalized construct intersection yielding typical case smaller grammar intersection simple case parser terminate illustrate parser generalized accept fsa input present simple parser context free grammar represented definite clause specification follows wish define set terminal non terminal symbol understood rule defined relation symbol rh prefixed case terminal case non terminal relation defines start symbol language defined order illustrate ordinary parser compute intersection fsa cfg consider definite clause specification parser parser run polynomial time implemented earley deduction xoldt resolution warren assumed input string represented predicate predicate effect construct parse forest grammar predicate succeeds effect asserts argument rule parse forest grammar sentence obtain parse forest grammar reader verifies grammar generates isomorphism single parse tree example assuming course start symbol parse forest grammar parse forest grammar complex symbol non terminal atomic symbol terminal consider definite clause specification fsa define transition relation relation start state relation hold final state relation hold following fsa defining regular language number followed given need changed use parser computation intersection fsa cfg input sentence definition given obtain following parse forest grammar start symbol isthus use parser infinite set input sentence represented fsa parser able come parse forest grammar possible derivation grammar construct following abbreviated parse tree figure construction bar hillel yielded grammar rule",
        "section want generalize idea described cfg dcg note problem calculating intersection dcg fsa solved generalization construction bar hillel use method end large forest grammar guaranteed contain solution interested method generate small subset intersection want parse forest grammar straightforward approach generalize existing recognition algorithm technique calculating intersection fsa cfg applied case dcgs order compute intersection dcg fsa assume fsa represented represented notation context free grammar course category symbol order term arbitrary complexity note loss generality account dcgs having external action defined curly brace use existing technique parsing dcgs confronted undecidability problem recognition problem dcgs undecidable pereira warren fortiori problem deciding intersection fsa dcg undecidable undecidability result circumvented considering subset dcgs recognized example restrict attention dcgs context free skeleton contain cycle line parsable grammar decidable pereira warren existing constraint based parsing algorithm terminate grammar exhibit property string finite number possible derivation line parsability possible way ensuring case observation helpful establishing insight concerning interesting subclass dcgs termination guaranteed case fsa input reason source recursion dcg fsa cycle saw cfg hold infinite number analysis given fsa cfg course imply undecidability question intersection fsa line parsable dcg undecidable yes problem undecidable ullman algorithm take input instance problem determines answer instance yes instance problem consists particular choice parameter problem use post correspondence problem pcp known undecidable problem mentioned intersection problem decidable solve pcp following definition example pcp taken ullman instance pcp consists list string alphabet instance solution sequence integer sequence solution instance pcp example assume let solution instance pcp sequence obtaining sequence illustration figure pcp solution let pcp solution general problem pcp solution decidable result proved ullman showing halting problem turing machine encoded instance post correspondence problem simple algorithm encode instance",
        "pcp pair consisting fsa line parsable dcg way question solution pcp equivalent question intersection fsa dcg pcpfor length list define dcg rule th member th member rulefurthermore rulealso start category dcg fsa consists single state start state final state single transition fsa generates dcg line parsable underlying idea algorithm simple pair string list lexical entry deriving terminal string represented difference list encoding general combination rule concatenates string concatenates b string rule state order construct succesful category list match resulting dcg fsa pair example pcp given figure question intersection fsa line parsable dcg undecidable problem decidable case exist algorithm solving problem algorithm solve pcp pcp solution encoding given fsa line parsable dcg pcp problem known undecidable intersection question undecidable following approach undecidability problem taken limit power fsalimit power dcgcompromise completenesscompromise soundnessthese approach discussed turn fsarather assuming input parsing fsa generality assume input ordinary word graph fsa cycle technique robust processing rise cycle example processing unknown sequence word case noise input clear word uttered noise clear loose practical term cycle verify question intersection word graph line parsable dcg decidable reduces checking dcg derives finite number string dcganother approach limit size category employed gpsg f tag approach case dealing dcgs cfgs shown insufficient general description natural language completenesscompleteness context mean parse forest grammar contains possible par possible compromise way parser guaranteed terminate miss parse tree example assume edge fsa associated probability possible define threshold partial result derived probability higher threshold possible cycle fsa anytime cycle probability decrease cycle encountered threshold cut derivation course implies intersection considered procedure fact intersection threshold case intersection problem line parsable dcgs fsa decidable soundnesssoundness context",
        "understood property parse tree parse forest grammar valid parse tree possible way ensure termination remove constraint dcg parse according context free skeleton resulting parse forest grammar general time practical variation conceived follows dcg context free skeleton skeleton obtained removing constraint grammar rule compute intersection skeleton input fsa result parse forest grammar add corresponding constraint dcg grammar rule parse forest grammar advantage result sound complete size parse forest grammar optimal consequence guaranteed parse forest grammar contains parse tree course possible experiment different way taking context free skeleton including information possible useful",
        "question intersection fsa line parsable dcg undecidable yes problem undecidable ullman algorithm take input instance problem determines answer instance yes instance problem consists particular choice parameter problem use post correspondence problem pcp known undecidable problem mentioned intersection problem decidable solve pcp following definition example pcp taken ullman instance pcp consists list string alphabet instance solution sequence integer sequence solution instance pcp example assume let solution instance pcp sequence obtaining sequence illustration figure pcp solution let pcp solution general problem pcp solution decidable result proved ullman showing halting problem turing machine encoded instance post correspondence problem simple algorithm encode instance pcp pair consisting fsa line parsable dcg way question solution pcp equivalent question intersection fsa dcg pcpfor length list define dcg rule th member th member rulefurthermore rulealso start category dcg fsa consists single state start state final state single transition fsa generates dcg line parsable underlying idea algorithm simple pair string list lexical entry deriving terminal string represented difference list encoding general combination rule concatenates string concatenates b string rule state order construct succesful category list match resulting dcg fsa pair example pcp given figure question intersection fsa line parsable dcg undecidable problem decidable case exist algorithm solving problem algorithm solve pcp pcp solution encoding given fsa line parsable dcg pcp problem known undecidable intersection question undecidable",
        "following approach undecidability problem taken limit power fsalimit power dcgcompromise completenesscompromise soundnessthese approach discussed turn fsarather assuming input parsing fsa generality assume input ordinary word graph fsa cycle technique robust processing rise cycle example processing unknown sequence word case noise input clear word uttered noise clear loose practical term cycle verify question intersection word graph line parsable dcg decidable reduces checking dcg derives finite number string dcganother approach limit size category employed gpsg f tag approach case dealing dcgs cfgs shown insufficient general description natural language completenesscompleteness context mean parse forest grammar contains possible par possible compromise way parser guaranteed terminate miss parse tree example assume edge fsa associated probability possible define threshold partial result derived probability higher threshold possible cycle fsa anytime cycle probability decrease cycle encountered threshold cut derivation course implies intersection considered procedure fact intersection threshold case intersection problem line parsable dcgs fsa decidable soundnesssoundness context understood property parse tree parse forest grammar valid parse tree possible way ensure termination remove constraint dcg parse according context free skeleton resulting parse forest grammar general time practical variation conceived follows dcg context free skeleton skeleton obtained removing constraint grammar rule compute intersection skeleton input fsa result parse forest grammar add corresponding constraint dcg grammar rule parse forest grammar advantage result sound complete size parse forest grammar optimal consequence guaranteed parse forest grammar contains parse tree course possible experiment different way taking context free skeleton including information possible useful",
        "syntactic natural language parser shown inadequate processing ambiguous large vocabulary text evidenced poor performance domain wall street journal movement parsing based approach text processing general paper describe spatter statistical parser based decision tree learning technique construct complete parse sentence achieves accuracy rate published result work based following premise grammar complex detailed develop interesting domain parsing model rely lexical contextual information analyze sentence existing n gram modeling technique inadequate parsing model experiment comparing spatter ibm computer manual parser spatter outperforms grammar based parser evaluating spatter penn treebank wall street journal corpus parseval measure spatter achieves precision recall crossing bracket sentence sentence word precision recall crossing bracket sentence word length",
        "parsing natural language sentence viewed making sequence disambiguation decision determining speech word choosing possible constituent structure selecting label constituent disambiguation problem parsing addressed enumerating possibility declaring knowledge aid disambiguation process approach proved brittle interesting natural language problem work address problem discovering disambiguation criterion decision parsing process given set possible feature act disambiguator candidate disambiguator word sentence relationship word relationship constituent constructed parsing process natural language rule absolute disambiguation criterion discovered work applied decision pursued non according probability choice probability estimated statistical decision tree model probability complete parse tree sentence product decision conditioned previous decision decision sequence construct unique parse parser selects parse decision sequence yield highest cumulative probability combining stack decoder search breadth algorithm probabilistic pruning possible identify highest probability parse sentence reasonable memory time claim work statistic large corpus parsed sentence combined information theoretic classification training algorithm produce accurate natural language parser aid complicated knowledge base grammar claim justified constructing parser called spatter statistical pattern recognizer based linguistic information comparing performance state art grammar based parser common task remains shown accurate broad coverage parser improve performance text processing application subject future experiment important point work statistical model natural language restricted simple context insensitive model problem parsing long distance lexical information crucial disambiguate interpretation local model probabilistic context free grammar inadequate work illustrates existing decision tree technology construct estimate model choose element context contribute disambiguation decision parameter trained existing resource begin describing decision tree modeling showing decision tree model equivalent interpolated n gram model describe training parsing procedure spatter present result experiment comparing spatter grammarian rule based statistical parser recent result showing spatter applied wall street journal domain",
        "work paper depends replacing human decision making skill automatic decision making algorithm decision consideration involve identifying constituent constituent label natural language sentence human decision maker parsing solve problem enumerating feature sentence affect disambiguation decision indicating parse select based feature value grammarian accomplishing critical task identifying feature relevant decision deciding choice select based value relevant feature tree classification algorithm account task accomplish task grammarian find difficult assigning probability distribution possible choice decision tree provide ranking system specifies order preference possible choice give measure relative likelihood choice selected decision tree decision making device assigns probability possible choice based context decision element future vocabulary set choice history context decision probability determined asking sequence question context ith question asked determined answer previous question instance consider speech tagging problem question decision tree ask word tagged answer decision tree need ask question clear decision tree assign tag probability answer question bear decision tree ask question tag previous word answer question determiner decision tree stop asking question assign tag high probability tag lower probability answer question noun decision tree need ask question good estimate probability tagging decision decision tree described paragraph shown figure question asked decision tree represented tree node oval figure possible answer question associated branch emanating node node defines probability distribution space possible decision node decision tree stop asking question leaf node leaf node represent unique state decision making problem context lead leaf node probability distribution decision decision tree model different interpolated n gram model fact equivalent representational power main difference modeling technique model parameterized parameter estimated let clear mean n gram model n gram model refers markov process probability particular token generating dependent value previous token generated process definition n gram model parameter number unique token generated process let define n gram model model defines probability distribution random variable given value random variable assumption definition random variable range vocabulary number parameter n gram model definition n gram model represented decision tree model",
        "question instance speech tagging model interpreted gram model variable denoting word tagged variable denoting tag previous word variable denoting tag word word gram tagging model decision tree model asks sequence question word tagged tag previous word tag word word decision tree model represented n gram model represented interpolated n gram model proof assertion given section standard approach estimating n gram model step process step count number occurrence n gram training corpus process determines empirical distribution second step smoothing empirical distribution separate held corpus step improves empirical distribution finding unreliable parameter estimate adjusting based reliable information technique smoothing deleted interpolation interpolation estimate model linear combination empirical model allfor example model interpolated follows history optimal value function estimated backward algorithm baum decision tree model represented interpolated n gram model follows leaf node decision tree represented sequence question answer history value lead decision tree leaf leaf node defines probability distribution based value question answer question asked path root leaf term interpolated n gram model decision tree defined interpolated n gram model function defined point showing equivalence n gram model decision tree model clear power decision tree model expressiveness acquired large modeling problem grows parameter space n gram model grows infeasible estimate smoothed model deleted interpolation grows large likelihood deleted interpolation process converge optimal optimal parameter setting small hand decision tree learning algorithm increase size model training data allows consider large history space n gram model large value number parameter resulting model remain constant depending number training example leaf distribution decision tree empirical estimate relative frequency count training data assign probability event occur necessary smooth empirical n gram model necessary smooth empirical decision tree model decision tree learning algorithm work developed past year ibm speech recognition group growing algorithm adaptation cart algorithm breiman detailed description discussion decision tree algorithm work magerman important point omitted discussion decision tree fact binary question decision tree question value decomposed sequence binary question classification tree",
        "decision tree decision making device assigns probability possible choice based context decision element future vocabulary set choice history context decision probability determined asking sequence question context ith question asked determined answer previous question instance consider speech tagging problem question decision tree ask word tagged answer decision tree need ask question clear decision tree assign tag probability answer question bear decision tree ask question tag previous word answer question determiner decision tree stop asking question assign tag high probability tag lower probability answer question noun decision tree need ask question good estimate probability tagging decision decision tree described paragraph shown figure question asked decision tree represented tree node oval figure possible answer question associated branch emanating node node defines probability distribution space possible decision node decision tree stop asking question leaf node leaf node represent unique state decision making problem context lead leaf node probability distribution decision",
        "decision tree model different interpolated n gram model fact equivalent representational power main difference modeling technique model parameterized parameter estimated let clear mean n gram model n gram model refers markov process probability particular token generating dependent value previous token generated process definition n gram model parameter number unique token generated process let define n gram model model defines probability distribution random variable given value random variable assumption definition random variable range vocabulary number parameter n gram model definition n gram model represented decision tree model question instance speech tagging model interpreted gram model variable denoting word tagged variable denoting tag previous word variable denoting tag word word gram tagging model decision tree model asks sequence question word tagged tag previous word tag word word decision tree model represented n gram model represented interpolated n gram model proof assertion given section standard approach estimating n gram model step process step count number occurrence n gram training corpus process determines empirical distribution second step smoothing empirical distribution separate held corpus step improves empirical distribution finding unreliable parameter estimate adjusting based reliable information technique smoothing deleted interpolation interpolation estimate model linear combination empirical model allfor example model interpolated follows history optimal value function estimated backward algorithm baum decision tree model represented interpolated n gram model follows leaf node decision tree represented sequence question answer history value lead decision tree leaf leaf node defines probability distribution based value question answer question asked path root leaf term interpolated n gram model decision tree defined interpolated n gram model function defined",
        "let clear mean n gram model n gram model refers markov process probability particular token generating dependent value previous token generated process definition n gram model parameter number unique token generated process let define n gram model model defines probability distribution random variable given value random variable assumption definition random variable range vocabulary number parameter n gram model definition n gram model represented decision tree model question instance speech tagging model interpreted gram model variable denoting word tagged variable denoting tag previous word variable denoting tag word word gram tagging model decision tree model asks sequence question word tagged tag previous word tag word word decision tree model represented n gram model represented interpolated n gram model proof assertion given section",
        "standard approach estimating n gram model step process step count number occurrence n gram training corpus process determines empirical distribution second step smoothing empirical distribution separate held corpus step improves empirical distribution finding unreliable parameter estimate adjusting based reliable information technique smoothing deleted interpolation interpolation estimate model linear combination empirical model allfor example model interpolated follows history optimal value function estimated backward algorithm baum decision tree model represented interpolated n gram model follows leaf node decision tree represented sequence question answer history value lead decision tree leaf leaf node defines probability distribution based value question answer question asked path root leaf term interpolated n gram model decision tree defined interpolated n gram model function defined",
        "point showing equivalence n gram model decision tree model clear power decision tree model expressiveness acquired large modeling problem grows parameter space n gram model grows infeasible estimate smoothed model deleted interpolation grows large likelihood deleted interpolation process converge optimal optimal parameter setting small hand decision tree learning algorithm increase size model training data allows consider large history space n gram model large value number parameter resulting model remain constant depending number training example leaf distribution decision tree empirical estimate relative frequency count training data assign probability event occur necessary smooth empirical n gram model necessary smooth empirical decision tree model decision tree learning algorithm work developed past year ibm speech recognition group growing algorithm adaptation cart algorithm breiman detailed description discussion decision tree algorithm work magerman important point omitted discussion decision tree fact binary question decision tree question value decomposed sequence binary question classification tree value example question word represented binary question question determined growing classification tree word vocabulary described brown question represent different binary partition word vocabulary question defined possible identify word asking question discussion use binary decision tree question magerman",
        "spatter parsing algorithm based interpreting parsing statistical pattern recognition process parse tree sentence constructed starting sentence word leaf tree structure labeling extending node node rooted labeled tree constructed pattern recognition process driven decision tree model described previous section parse tree viewed n ary branching tree node tree labeled non terminal label speech label parse tree interpreted geometric pattern constituent set edge meet tree node instance noun phrase brown cow consists edge extending right edge extending left cow edge extending brown spatter parse tree encoded term elementary component feature word tag label extension feature fixed vocabulary element given feature vocabulary having unique representation word feature value word tag feature value speech tag set label feature value non terminal set extension following value right node child constituent left node child constituent node child constituent unary node child unary constituent root node root tree word sentence parse tree leaf node word feature value ith leaf node ith word sentence word feature value internal node intended contain lexical head node constituent deterministic lookup table based label internal node label child approximate linguistic notion spatter representation sentenceis shown figure node constructed left right constraint constituent node constructed child constructed order node example sentence constructed indicated figure consists main decision tree model speech tagging model node extension model node labeling model decision tree model grown following question word tag label extension left current node node node node current node child current node second child node listed decision tree ask number child span node tagging model value previous word tag asked differ head word previous constituent training algorithm proceeds follows training corpus divided set tree growing tree smoothing parsed sentence tree growing corpus correct state sequence traversed state transition event history answer question state future value action taken state state event training example decision tree growing process appropriate feature tree tagging event growing tagging",
        "tree etc decision tree grown smoothed tree smoothing corpus variation deleted interpolation algorithm described magerman parsing procedure search highest probability parse tree probability parse product probability action constructing parse according decision tree model size search space number speech tag number word sentence number non terminal label possible compute probability parse specific search algorithm important search error search error occurs highest probability parse found parser highest probability parse space par search procedure us phase approach identify highest probability parse sentence parser us stack decoding algorithm find complete parse sentence stack decoder found complete parse reasonable probability switch breadth mode pursue partial par explored stack decoder second mode discard partial parse probability lower probability highest probability completed parse search mode spatter guarantee find highest probability parse limitation search technique sentence modeled search exhaust available memory completing phase search error occur sentence spatter wrong performance lossed search error search algorithm guarantee highest probability parse found sentence parsed",
        "parse tree viewed n ary branching tree node tree labeled non terminal label speech label parse tree interpreted geometric pattern constituent set edge meet tree node instance noun phrase brown cow consists edge extending right edge extending left cow edge extending brown spatter parse tree encoded term elementary component feature word tag label extension feature fixed vocabulary element given feature vocabulary having unique representation word feature value word tag feature value speech tag set label feature value non terminal set extension following value right node child constituent left node child constituent node child constituent unary node child unary constituent root node root tree word sentence parse tree leaf node word feature value ith leaf node ith word sentence word feature value internal node intended contain lexical head node constituent deterministic lookup table based label internal node label child approximate linguistic notion spatter representation sentenceis shown figure node constructed left right constraint constituent node constructed child constructed order node example sentence constructed indicated figure",
        "spatter consists main decision tree model speech tagging model node extension model node labeling model decision tree model grown following question word tag label extension left current node node node node current node child current node second child node listed decision tree ask number child span node tagging model value previous word tag asked differ head word previous constituent training algorithm proceeds follows training corpus divided set tree growing tree smoothing parsed sentence tree growing corpus correct state sequence traversed state transition event history answer question state future value action taken state state event training example decision tree growing process appropriate feature tree tagging event growing tagging tree etc decision tree grown smoothed tree smoothing corpus variation deleted interpolation algorithm described magerman",
        "parsing procedure search highest probability parse tree probability parse product probability action constructing parse according decision tree model size search space number speech tag number word sentence number non terminal label possible compute probability parse specific search algorithm important search error search error occurs highest probability parse found parser highest probability parse space par search procedure us phase approach identify highest probability parse sentence parser us stack decoding algorithm find complete parse sentence stack decoder found complete parse reasonable probability switch breadth mode pursue partial par explored stack decoder second mode discard partial parse probability lower probability highest probability completed parse search mode spatter guarantee find highest probability parse limitation search technique sentence modeled search exhaust available memory completing phase search error occur sentence spatter wrong performance lossed search error search algorithm guarantee highest probability parse found sentence parsed",
        "absence system spatter evaluated comparing ranking parse treebank analysis test sentence parser applied different domain ibm computer manual wall street journal experiment us ibm computer manual domain consists sentence extracted ibm computer manual training test sentence annotated university lancaster lancaster treebank us speech tag non terminal label treebank described great detail black main reason applying spatter domain ibm spent previous year developing rule based unification style probabilistic context free grammar parsing domain purpose experiment estimate spatter ability learn syntax domain treebank depending interpretive expertise grammarian parser trained sentence lancaster treebank test set included new sentence length range word mean length word sentence test sentence experiment reported ibm parser black black ibm parser evaluated crossing bracket measure represents percentage sentence constituent parser parse violates constituent boundary constituent correct parse year grammar development ibm parser achieved crossing bracket score test set spatter scored experiment intended illustrate spatter ability parse ambiguous large vocabulary domain experiment use wall street journal domain annotated penn treebank version penn treebank us speech tag non terminal label wsj portion penn treebank divided section numbered experiment spatter trained section contains sentence test result reported section contains sentence test data future experiment penn treebank tokenized sentence detected human annotator test result reported reflect par word sequence tag sequence spatter pre tag sentence use best tag sequence parsing us probabilistic model assign tag word considers possible tag sequence according probability assigned model information legal tag word extracted test corpus fact information word test corpus sake efficiency sentence word fewer included experiment test set spatter take average second sentence sgi megabyte ram evaluate spatter performance domain parseval measure defined black bracketsno constituent violate constituent boundary constituent treebank parse precision recall measure consider constituent label evaluation parse treebank label set coincide label given grammar spatter us syntactic label set penn treebank make sense report labelled precision labelled recall measure computed considering constituent correct label match label treebank show result spatter",
        "experiment us ibm computer manual domain consists sentence extracted ibm computer manual training test sentence annotated university lancaster lancaster treebank us speech tag non terminal label treebank described great detail black main reason applying spatter domain ibm spent previous year developing rule based unification style probabilistic context free grammar parsing domain purpose experiment estimate spatter ability learn syntax domain treebank depending interpretive expertise grammarian parser trained sentence lancaster treebank test set included new sentence length range word mean length word sentence test sentence experiment reported ibm parser black black ibm parser evaluated crossing bracket measure represents percentage sentence constituent parser parse violates constituent boundary constituent correct parse year grammar development ibm parser achieved crossing bracket score test set spatter scored",
        "experiment intended illustrate spatter ability parse ambiguous large vocabulary domain experiment use wall street journal domain annotated penn treebank version penn treebank us speech tag non terminal label wsj portion penn treebank divided section numbered experiment spatter trained section contains sentence test result reported section contains sentence test data future experiment penn treebank tokenized sentence detected human annotator test result reported reflect par word sequence tag sequence spatter pre tag sentence use best tag sequence parsing us probabilistic model assign tag word considers possible tag sequence according probability assigned model information legal tag word extracted test corpus fact information word test corpus sake efficiency sentence word fewer included experiment test set spatter take average second sentence sgi megabyte ram evaluate spatter performance domain parseval measure defined black bracketsno constituent violate constituent boundary constituent treebank parse precision recall measure consider constituent label evaluation parse treebank label set coincide label given grammar spatter us syntactic label set penn treebank make sense report labelled precision labelled recall measure computed considering constituent correct label match label treebank show result spatter evaluated penn treebank wall street journal section illustrate performance spatter function sentence length performance degrades sentence word performs sentence indicates frequency sentence length test corpus",
        "discourse strategy strategy communicating agent strategy choice decision talk let agent talk choice conversational agent utterance include relevant optional information communicated example consider speaker strategic choice include said determines speaker choice existing dialogue system mode dealing optional information include optional information known hearer include optional information moore paris mode extreme possibility knowledge previous work proposed principle include optional information way testing proposed principle affected conversants processing ability task communication channel domain paper present new experimental method determining discourse strategy effective present experimental result strategy supporting deliberation method based earlier simulation work carletta pollack carletta pollack ringuette outline hypothesis factor affect strategy effective present new method testing role hypothesized factor experimental result section effective strategy support deliberation determined cognitive task variable",
        "deliberation process agent decides believe galliers doyle strategy support deliberation explicit warrant strategy warrant hearer deliberating accept reject speaker proposal analysis proposal corpus problem solving dialogue show communicating agent include warrant proposal suggest number hypothesis factor affect decision walker pollack situation agent want agent accept proposal helpful agent nonautonomous accept proposal warrant deliberates accept know competing option best option tell warrant warrant make dialogue explicit warrant strategy inefficient situation hold consider situation autonomous agent galliers deliberates proposal know option compete proposal decide accept warrant agent omit warrant believed speaker said believed hearer knew walnut st route shorter consider said discussing indian restaurant lunch warrant included despite fact common knowledge conversants inclusion violates rule tell people fact know rule hold known warrant type redundant utterance henceforth iru frequent occurring dialogue walker warrant iru suggests cognitive limitation factor chooses know warrant adopting proposal critical warrant salient warrant accessible working memory prince baddeley warrant salient infer retrieve warrant information obtain external source order evaluate proposal strategy choice depend model attentional state cost retrieval inference opposed communication word decide easier warrant require infer retrieve task determines penalty leaving warrant implicit relying infer retrieve task require agent agree reason adopting proposal order ensure robustness situation environmental change task management union negotiation require agent agree action carried agent reason wanting action affecting success task summarizes hypothesis proposing hypothetical decision tree agent choice use explicit warrant strategy choice hypothesized depend cognitive property know attentional state processing capability property task communication channel knowledge previous work dialogue assumed agent tell agent fact agent know hypothesis figure plausible relationship cognitive effort dialogue behavior explored hypothesis required way test hypothesized relationship task cognitive factor effective discourse strategy describes new method testing hypothesis effective discourse strategy dialogue",
        "design world experimental environment testing relationship discourse strategy task parameter agent cognitive capability similar single agent tileworld simulation environment pollack ringuette hank world agent parametrized discourse strategy effect strategy measured range cognitive task parameter paper compare explicit warrant strategy implicit strategy strategy supporting deliberation strategy tested design world presented walker walker rambow walker design world task requires agent carry dialogue order negotiate agreement design floor plan room house whittaker design house plan requires agent agree design room design room agent know design house plan requires start set furniture piece design room negotiate agreement agent carry mean end reasoning furniture piece floor plan end reasoning generates option option content proposal agent piece furniture room illustrates agent communication designing room including artificial language agent communicate gloss generated language italic receiving proposal agent deliberates accept reject proposal doyle potential warrant support deliberation provide way evaluating agent performance piece furniture score score proposition piece furniture stored agent memory beginning dialogue reject proposal deliberation lead believe know better option believe precondition proposal hold content rejection determined collaborative planning principle abstracted analyzing different type problem solving dialogue walker whittaker walker example kim reject proposal give reason option counter proposal inferred accepted rejected walker whittaker walker proposal accepted option content proposal mutual intention contributes final design plan power sidner potential final design plan negotiated dialogue shown figure design world experiment reported compare implicit strategy explicit warrant strategy parametrized different discourse strategy placing different expansion discourse plan plan library plan plan proposal rejection acceptance clarification opening closing variation discussed variation expansion proposal implicit strategy expansion discourse plan proposal proposal decomposes communicative act propose dialogue design world agent communicate implicit strategy proposal shown utterance implicit strategy includes warrant proposal leaving agent retrieve memory explicit warrant strategy expands proposal discourse act warrant followed propose utterance agent know point value piece furniture warrant irus experiment example warrant proposal name agent use explicit warrant strategy numbered version string iei help experimenter track simulation data file",
        "iei stand implicit acceptance explicit warrant implicit opening closing fact green rug worth point support deliberation adopt intention putting green rug study explicit warrant strategy model occurring example point information hearer deliberate accept reject proposal believed introduced range factor motivated corpus analysis hypothesized determine explicit warrant effective strategy section discus design world support parametrization factor agent architecture deliberation mean end reasoning based irma architecture tileworld simulation environment pollack ringuette addition model attention working memory awm includes fuller discussion design world deliberation mean end reasoning mechanism underlying mechanism assumed collaborative planning hypothesized warrant salient agent shown example design world salience modeled awm model adapted landauer awm model simple landauer showed parameterized fit empirical result human memory learning baddeley consists dimensional space proposition acquired perceiving world stored chronological sequence according location moving memory pointer sequence memory locus storage constitutes random walk memory locus short distance previous item encountered multiple time stored multiple time hintzmann block agent retrieves item memory search start current pointer location spread spherical fashion restricted particular search radius radius defined hamming distance example current memory pointer locus locus distance actual location calculated modulo memory size limit search radius defines capacity attention working memory defines stored belief intention salient radius search awm model parameter design world agent resource bound attentional capacity experiment memory radius parameter varies awm give attention agent awm mean agent know accessible parameter let distinguish agent ability access information stored memory effort involved advantage awm model shown reproduce simulation result human memory learning search start current pointer location item stored retrieved predicting recency effect baddeley item stored multiple location retrieved model predicts frequency effect hintzmann block item stored chronological sequence model produce natural associativity effect landauer deliberation mean end reasoning operate salient belief attention produce concomitant inferential limitation belief salient deliberation mean end reasoning mean mistake agent planning process plausible cognitive basis fail access belief allow produce optimal plan mistake planning belief world changed result planning salient preceding discourse agent",
        "attentional capacity proposition agent know salient proposal hypothetical factor relative cost retrieval communication give way measure number retrieval memory term number location searched find proposition effort required retrieval step parameter cost inference step cost communicated message cost parameter support modeling cognitive architecture varying cost retrieval model different assumption memory example retrieval free item working memory accessible stored register fast parallel access awm set retrieval free model approximates slow spreading activation effortful agent ability access memory given time awm set retrieval free model slow spreading activation timeout effort exceeds certain agent ability access memory sense fix absolute value retrieval inference communication cost parameter relation human processing design world support exploring issue relative cost process relative cost vary depending language agent communicating property communication channel smart agent time demand task norman bobrow vary relative cost communication retrieval hypothesized explicit warrant strategy beneficial relationship warrant proposal believed definition success task design world parameter standard task require shared warrant nonmatching belief task give score negotiated plan agreed warrant evaluate performance compare explicit warrant strategy implicit strategy situation vary task requirement agent attentional capacity cost retrieval inference communication resulting design house plan parametrized bycommcost cost sending message infcost cost inference andretcost cost retrieval memory performance score task specific standard task summarize point value furniture piece act final design nonmatching belief task agent point plan agree reason underlying action contributes plan way performance defined reflects fact agent meant collaborate task cost deducted raw score cost agent communication inference retrieval performance measure collaborative effort clark schaefer brennan parameter cognitive effort fixed discourse strategy awm setting varied test benefit different discourse strategy different assumption cognitive effort cognitive demand task impossible corpus analysis simulate dialogue parameter setting strategy performance distribution evaluated significance dialogue kolmogorov smirnov sample test siegel strategy beneficial compared strategy set fixed parameter setting difference distribution kolmogorov smirnov",
        "design world task requires agent carry dialogue order negotiate agreement design floor plan room house whittaker design house plan requires agent agree design room design room agent know design house plan requires start set furniture piece design room negotiate agreement agent carry mean end reasoning furniture piece floor plan end reasoning generates option option content proposal agent piece furniture room illustrates agent communication designing room including artificial language agent communicate gloss generated language italic receiving proposal agent deliberates accept reject proposal doyle potential warrant support deliberation provide way evaluating agent performance piece furniture score score proposition piece furniture stored agent memory beginning dialogue reject proposal deliberation lead believe know better option believe precondition proposal hold content rejection determined collaborative planning principle abstracted analyzing different type problem solving dialogue walker whittaker walker example kim reject proposal give reason option counter proposal inferred accepted rejected walker whittaker walker proposal accepted option content proposal mutual intention contributes final design plan power sidner potential final design plan negotiated dialogue shown figure",
        "design world experiment reported compare implicit strategy explicit warrant strategy parametrized different discourse strategy placing different expansion discourse plan plan library plan plan proposal rejection acceptance clarification opening closing variation discussed variation expansion proposal implicit strategy expansion discourse plan proposal proposal decomposes communicative act propose dialogue design world agent communicate implicit strategy proposal shown utterance implicit strategy includes warrant proposal leaving agent retrieve memory explicit warrant strategy expands proposal discourse act warrant followed propose utterance agent know point value piece furniture warrant irus experiment example warrant proposal name agent use explicit warrant strategy numbered version string iei help experimenter track simulation data file iei stand implicit acceptance explicit warrant implicit opening closing fact green rug worth point support deliberation adopt intention putting green rug study explicit warrant strategy model occurring example point information hearer deliberate accept reject proposal believed",
        "section introduced range factor motivated corpus analysis hypothesized determine explicit warrant effective strategy section discus design world support parametrization factor agent architecture deliberation mean end reasoning based irma architecture tileworld simulation environment pollack ringuette addition model attention working memory awm includes fuller discussion design world deliberation mean end reasoning mechanism underlying mechanism assumed collaborative planning hypothesized warrant salient agent shown example design world salience modeled awm model adapted landauer awm model simple landauer showed parameterized fit empirical result human memory learning baddeley consists dimensional space proposition acquired perceiving world stored chronological sequence according location moving memory pointer sequence memory locus storage constitutes random walk memory locus short distance previous item encountered multiple time stored multiple time hintzmann block agent retrieves item memory search start current pointer location spread spherical fashion restricted particular search radius radius defined hamming distance example current memory pointer locus locus distance actual location calculated modulo memory size limit search radius defines capacity attention working memory defines stored belief intention salient radius search awm model parameter design world agent resource bound attentional capacity experiment memory radius parameter varies awm give attention agent awm mean agent know accessible parameter let distinguish agent ability access information stored memory effort involved advantage awm model shown reproduce simulation result human memory learning search start current pointer location item stored retrieved predicting recency effect baddeley item stored multiple location retrieved model predicts frequency effect hintzmann block item stored chronological sequence model produce natural associativity effect landauer deliberation mean end reasoning operate salient belief attention produce concomitant inferential limitation belief salient deliberation mean end reasoning mean mistake agent planning process plausible cognitive basis fail access belief allow produce optimal plan mistake planning belief world changed result planning salient preceding discourse agent attentional capacity proposition agent know salient proposal hypothetical factor relative cost retrieval communication give way measure number retrieval memory term number location searched find proposition effort required retrieval step parameter cost inference step",
        "cost communicated message cost parameter support modeling cognitive architecture varying cost retrieval model different assumption memory example retrieval free item working memory accessible stored register fast parallel access awm set retrieval free model approximates slow spreading activation effortful agent ability access memory given time awm set retrieval free model slow spreading activation timeout effort exceeds certain agent ability access memory sense fix absolute value retrieval inference communication cost parameter relation human processing design world support exploring issue relative cost process relative cost vary depending language agent communicating property communication channel smart agent time demand task norman bobrow vary relative cost communication retrieval hypothesized explicit warrant strategy beneficial relationship warrant proposal believed definition success task design world parameter standard task require shared warrant nonmatching belief task give score negotiated plan agreed warrant",
        "evaluate performance compare explicit warrant strategy implicit strategy situation vary task requirement agent attentional capacity cost retrieval inference communication resulting design house plan parametrized bycommcost cost sending message infcost cost inference andretcost cost retrieval memory performance score task specific standard task summarize point value furniture piece act final design nonmatching belief task agent point plan agree reason underlying action contributes plan way performance defined reflects fact agent meant collaborate task cost deducted raw score cost agent communication inference retrieval performance measure collaborative effort clark schaefer brennan parameter cognitive effort fixed discourse strategy awm setting varied test benefit different discourse strategy different assumption cognitive effort cognitive demand task impossible corpus analysis simulate dialogue parameter setting strategy performance distribution evaluated significance dialogue kolmogorov smirnov sample test siegel strategy beneficial compared strategy set fixed parameter setting difference distribution kolmogorov smirnov sample test significant positive direction awm setting strategy detrimental difference negative direction beneficial detrimental difference strategy",
        "section discus result comparing explicit warrant discourse strategy implicit discourse strategy determine strategy beneficial test factor outlined figure warrant salient warrant required task cost retrieval communication vary retrieval indeterminate performance explicit warrant strategy implicit strategy shown difference plot figure figure performance difference plotted y axis awm setting shown x axis plot dotted line awm setting explicit warrant strategy beneficial depending difference significant test point represents difference mean run strategy particular setting plot summarize result simulated dialogue awm setting strategy agent use explicit warrant strategy efficient retrieval cost show explicit warrant strategy detrimental standard task comparison implicit strategy retrieval memory free making warrant salient displaces information piece furniture agent attention limited standard task agent required share belief value proposal remembering piece important remembering value figure show explicit warrant beneficial retrieval tenth cost communication inference awm value performance explicit warrant implicit belief necessary deliberation salient proposal awm awm parameter setting agent ability search belief warrant saving processing time substantial lowest awm setting strategy beneficial displaces information piece awm figure contrast figure retrieval associated cost saving retrieval balance loss raw score strategy detrimental experiment relative cost retrieval explicit warrant beneficial setting change relative cost different process situation change strategy beneficial show communication cost inference retrieval free explicit warrant strategy detrimental awm explicit warrant strategy increase number utterance required perform task double number message proposal communication expensive compared retrieval communication cost dominate benefit change definition success task change strategy beneficial task nonmatching belief explicit warrant strategy beneficial retrieval free awm warrant information provided information needed order achieve matching belief warrant intended action strategy guarantee agent agree reason carrying particular course action fact retrieval indeterminate produce effect similar result obtained warrant required retrieval cost great surprise beneficial effect explicit warrant nonmatching belief task robust communication cost retrieval",
        "dialogue agent use explicit warrant strategy efficient retrieval cost show explicit warrant strategy detrimental standard task comparison implicit strategy retrieval memory free making warrant salient displaces information piece furniture agent attention limited standard task agent required share belief value proposal remembering piece important remembering value figure show explicit warrant beneficial retrieval tenth cost communication inference awm value performance explicit warrant implicit belief necessary deliberation salient proposal awm awm parameter setting agent ability search belief warrant saving processing time substantial lowest awm setting strategy beneficial displaces information piece awm figure contrast figure retrieval associated cost saving retrieval balance loss raw score strategy detrimental experiment relative cost retrieval explicit warrant beneficial setting",
        "change definition success task change strategy beneficial task nonmatching belief explicit warrant strategy beneficial retrieval free awm warrant information provided information needed order achieve matching belief warrant intended action strategy guarantee agent agree reason carrying particular course action fact retrieval indeterminate produce effect similar result obtained warrant required retrieval cost great surprise beneficial effect explicit warrant nonmatching belief task robust communication cost retrieval inference free explicit warrant implicit figure word extra warrant message incurs penalty point task nonmatching belief agent explicit warrant figure standard task cost parameter result suggests including warrant effective agent agree specific warrant attention limited extent",
        "paper discussed instance general problem design conversational agent include optional information presented tested number hypothesis factor contribute decision include warrant proposal showed warrant useful task requires agreement warrant warrant salient retrieval warrant indeterminate retrieval associated cost warrant hinder performance communication costly warrant displace information needed complete task awm warrant required shared method new experimental methodology computational linguistics support testing hypothesis beneficial discourse strategy carletta pollack ringuette design world environment based cognitive model attention support experiment interaction discourse strategy agent cognitive limitation use method focus work novel previous work focused determining underlying mechanism cooperative strategy investigating strategy effective knowledge previous work dialogue argued conversational agent resource limit major factor determining effective conversational strategy collaboration result presented suggest cooperative strategy defined cooperation arises interaction agent dialogue agent working memory agent dialogue adopting strategy make deliberative premise salient word strategy cooperative certain conversational partner particular task definition particular communication situation compared discourse strategy implicit explicit warrant warrant type discourse strategy called attention strategy walker main function manipulate agent attentional state thatsome iru strategy beneficial inferential complexity higher standard task rambow walker walker inference explicit help inference agent perform omniscient one walker work remains reason believe result domain independent simplicity design world task mean structure subcomponent task model resource based cost parameter support modeling different agent architecture explored effect different cost parameter explicit warrant strategy based simple relationship different fact expect occur domain fact belief warrant accepting proposal occur task work extend result showing cooperative strategy need cooperative investigate additional factor determine strategy effective",
        "categorial grammar particular lambek categorial grammar lcg known benefit formal treatment natural language syntax semantics outstanding benefit fact specific way complete grammar encoded term combinatory potential word give time recipe construction meaning word combined form larger linguistic entity framework equivalent weak generative capacity derive context free language superior cope natural way extraction unbounded dependency phenomenon instance special category assignment need stipulated handle relative clause containing trace analyzed hypothetical reasoning traceless clause trace hypothesis discharged combined relative pronoun illustrates proof logical behaviour natural deduction style proof type logic corresponds phrase structure tree like adopt analysis trace derive bill miss hypothesis phantom place trace hypothesis indicated index result bill miss analyzed hypothesis synthesis new unsaturated type required type consumed antecedent implication type simpler proof abstraction example relative pronoun complex type triggering extraction drawback pure lambek calculus allows called peripheral extraction example trace initial final relative clause inflexibility lambek calculus reason researcher study richer system today instance recent work moortgat give systematic depth study mixed lambek system integrate system nlp ingredient system obtained varying lambek calculus dimension adding permutation rule dropping assumption type combinator form sequence system talk associative non associative variant little use linguistic description moortgat mixed system different resource management mode different system left intact combination exploited different part grammar relative pronoun instance receive category implication requires argument lacking present paper study computational complexity variant lambek calculus lie lp semidirectional lambek calculus sdl derivability known np complete interesting study restriction use operator restriction leaf proposed linguistic application intact admit type argument type functional application functor prove gentzen style system amount disallowing left rule resulting system sdl stated need structural rule monolithic system structural connective ability abstracted formula permute encoded right rule purpose studying sdl sense suited theory grammar simplicity exhibit core logical behaviour richer system need include allow non peripheral extraction source complexity uncovered",
        "semidirectional lambek calculus sdl variant lambek original calculus syntactic type start defining lambek calculus extend obtain sdl called syntactic type built set propositional variable primitive type binary connective called product left implication right implication use capital letter denote formula capital end alphabet denote sequence formula concatenation sequence denoted usual formal framework logic gentzen style sequent calculus pair written type sequence type claim embodied sequent read formula derivable structured database show lambek original calculus need product obtain result complicate matter eliminate product consideration sequel semidirectional lambek calculus add additional connective implication equip right rule define polarity subformula sequent follows positive polarity negative polarity polarity polarity opposite polarity sequent consequence allowing rule proved induction derivable sequent appear positive polarity occur cut formula application subformula occurs prove occur final sequent assume final sequent rh primitive rule occuring subformula word supposed extraction read category assignment extraction cut elimination calculus forward adaptation cut elimination proof omit proof reason space cut free system enjoys usual lambek like logic subformula property proof subformulae goal sequent appear consideration heavy use known count invariant lambek system benthem expression resource consciousness logic b count function counting positive negative occurrence primitive type arbitrary type invariant state primitive b count rh lh derivable sequent noticing invariant true preserved rule state let parallel sdl consider fragment disallowed fragment fragment positive occurrence implication negative extend lexical map nonempty string terminal setting language generated lambek grammar defined set string exists sequence type denote language sdl grammar defined lambek grammar replaces grammar string parsing recognition problem asks question obvious generative capacity sdl grammar relate lambek grammar nondirectional lambek grammar based calculus lambek grammar generate context free language modulo missing word pentus generate permutation closure context free language benthem excludes context free regular language includes context sensitive one permutation closure sdl straightforward",
        "context free language generated sdl grammar use standard transformation arbitrary cfr grammar categorial grammar appear sdl proof lexical assignment sdl proof string judged grammatical sdl judged subset account cfr language observation extends language context free generated following grammar language use primitive type define lexical map follows distinguished primitive type simplify argumentation abbreviate type indicated observe sequent image string balanced primitive count contains occurrence accounting supernumerary balanced count number occurrence resource oriented speaking consume resp provides pair string containing number produced subformula property know cut free proof main formula abstraction right rule implication type primitive antecedent lh sequent proof subsequence additional type type interspersed formsince connective need introduced remains shown proof sequent given figure sequent marked seen derivable abstraction remarkable point ability cover language generate example substantiates claim moortgat inferential capacity mixed lambek system greater sum component part attentive reader noticed encoding extends language having group symbol language form note passing grammar rule irrelevant time grammar",
        "semidirectional lambek calculus sdl variant lambek original calculus syntactic type start defining lambek calculus extend obtain sdl called syntactic type built set propositional variable primitive type binary connective called product left implication right implication use capital letter denote formula capital end alphabet denote sequence formula concatenation sequence denoted usual formal framework logic gentzen style sequent calculus pair written type sequence type claim embodied sequent read formula derivable structured database show lambek original calculus need product obtain result complicate matter eliminate product consideration sequel semidirectional lambek calculus add additional connective implication equip right rule define polarity subformula sequent follows positive polarity negative polarity polarity polarity opposite polarity sequent consequence allowing rule proved induction derivable sequent appear positive polarity occur cut formula application subformula occurs prove occur final sequent assume final sequent rh primitive rule occuring subformula word supposed extraction read category assignment extraction cut elimination calculus forward adaptation cut elimination proof omit proof reason space cut free system enjoys usual lambek like logic subformula property proof subformulae goal sequent appear consideration heavy use known count invariant lambek system benthem expression resource consciousness logic b count function counting positive negative occurrence primitive type arbitrary type invariant state primitive b count rh lh derivable sequent noticing invariant true preserved rule state let parallel sdl consider fragment disallowed fragment fragment positive occurrence implication negative",
        "extend lexical map nonempty string terminal setting language generated lambek grammar defined set string exists sequence type denote language sdl grammar defined lambek grammar replaces grammar string parsing recognition problem asks question obvious generative capacity sdl grammar relate lambek grammar nondirectional lambek grammar based calculus lambek grammar generate context free language modulo missing word pentus generate permutation closure context free language benthem excludes context free regular language includes context sensitive one permutation closure sdl straightforward context free language generated sdl grammar use standard transformation arbitrary cfr grammar categorial grammar appear sdl proof lexical assignment sdl proof string judged grammatical sdl judged subset account cfr language observation extends language context free generated following grammar language use primitive type define lexical map follows distinguished primitive type simplify argumentation abbreviate type indicated observe sequent image string balanced primitive count contains occurrence accounting supernumerary balanced count number occurrence resource oriented speaking consume resp provides pair string containing number produced subformula property know cut free proof main formula abstraction right rule implication type primitive antecedent lh sequent proof subsequence additional type type interspersed formsince connective need introduced remains shown proof sequent given figure sequent marked seen derivable abstraction remarkable point ability cover language generate example substantiates claim moortgat inferential capacity mixed lambek system greater sum component part attentive reader noticed encoding extends language having group symbol language form note passing grammar rule irrelevant time grammar",
        "parsing problem sdl grammar np complete reduction partition problem known np complete problem cited garey johnson follows reduction given partition instance notational convenience abbreviate note abbreviation product free fragment notation stand forwe define sdl grammar follows word interested care word generated claim given partition problem solvable consider direction turn given solution choose type sequence construct sdl proof given solution set triple compute polynomial time mapping sends index element index solution obtain required sequence choose terminal type resp complete sequent solve let shorthand let stand sequence primitive type rule prove applying time obtain total final step havewhich completes proof andbe witnessing derivable sequent count sequent balanced sequence contain subformulae read solution sequent including occurrence verify balancedness primitive count hold number positive negative occurrence sequent completes proof reduction prof np hardness parsing problem need strong np completeness partition reduction us unary encoding parsing problem lie given grammar proof bound length string guess proof check polynomial time state following observe reduction rule irrelevant extend result",
        "defined variant lambek original calculus type allows abstracted category permute based sdl generate context free language parsing problem sdl shown np complete result indicates efficient parsing grammar allow large number unbounded dependency node problematic categorial framework fact problematic case correct analysis normal sentence mean parser try arbitrary bound number assumed practical grammar engineering devise motto avoid accumulation unbounded dependency mean theoretical think result sdl importance sdl exhibit core logical behaviour based logic account non peripheral extraction form permutation result increase understanding necessary computational property richer system knowledge question lambek calculus associated parsing problem np hard open",
        "paper concerned symmetrical coordination order conjuncts item coordinated conjunction altered affecting acceptability kind split constituent coordination conjunct form constituent according standard phrase structure grammar non constituent coordination non constituent coordination treated separate phenomenon van oirsouw discussion different mechanism proposed considering grammaticality judgement little justification division illustrate consider sentence final proper substring sentence book mary book etc conjunct initial substring conjunct middle substring example constituent coordination unnatural improved replace book heavier string book gardening example substring sentence form viable conjunct",
        "year series account coordination involving deletion mechanism gleitman van oirsouw example following antecedent sentence van oirsouw allows deletion word left right conjunction resulting sentence deletion account assume deletion performed identity word analyse mean word identical exception van oirsouw discus phonological morphological referential identity following example deletion case drive identical different syntactic category consider case zeugma unacceptable joke deleted word major syntactic category lexical meaning fix syntactic category lexical meaning weird coordination example consider example preposition attached verb saw noun man example attributed paul dekker conjuncts require mary handbag different syntactic structure bracketing appropriate conjunct friend mary handbag unacceptability example suggests word word identity insufficient deleted material identical syntactic structure identical lexical meaning compelling argument deletion semantic example lakoff peter argued deletion account inappropriate certain constituent coordination antecedent sentence john mary nonsensical ungrammatical consider number agreement inappropriate nonsensical antecedent possible consider non constituent coordination example consider antecedent following non constituent coordination primary reading scope contain living england semantic bracketing example expanded level level expanded constituent level whilst retaining appropriate semantics example expansion level give lakoff peter argument count standard deletion analysis count general argument unified treatment constituent non constituent coordination",
        "sight analysing non constituent coordination phrasal constituent coordination nonsensical case classified non constituent coordination conjuncts fail constituent standard phrase structure grammar constituent grammar example argued weaker notion constituency provided categorial grammar required allow conjuncts treated constituent steedman coordination exemplified schema conj shared material treated conjunct single copy conjunction embedded single syntax tree phrasal coordination schema requires conjunct given single type conjuncts conjunction type requirement pointed sag gave following counterexample sag deal example treating category feature bundle allowing coordination case feature common example conjuncts share feature manner stand account deal example following adverbial phrase manner prepositional phrase temp example problematic sag given jorgensen abeille alternative suggested morrill similar jorgensen abeille use following coordination schema conj ythis impose condition category share common new category ensure category appropriate context example acceptable coordination type subcategorises np aps difficult problem providing type possible conjuncts following conjunction pair noun phrase case unbounded right node raising noun phrase peter embedded different depth conjuncts main approach dealing example phrasal coordination introduce explicit product operator wood allowing type form second use calculus type undergo type raising dowty formed abstraction lambek calculus lambek effect treat fred book verb phrase missing verb advantage adopting general abstraction mechanism lambek calculus provides treatment example ability perform abstraction category functional type required allows shared material different syntactic analysis resulting acceptance sentence predicted deletion account identity lexical category lexical semantics respected identity syntactic structure obtain identical syntactic type friend manufacturer subtracting lexical type saw mary handbag sentence type type identical coordination place ability subtract type allows lambek calculus replicate deletion account suffers problem proposal restrict lambek calculus order prevent overgeneration pickering propose calculus dealt product operation abstraction category act function derivation account make good empirical prediction fail following example conjunct contains different number modifier different type adverbial phrase prepositional phrase",
        "subcategorisation order swapped conjuncts treatment non constituent coordination phrasal coordination require elaborate encoding conjunct type simple generalisation conjuncts coordinate provided acceptable syntactic context d approach processing strategy use syntactic context method turn reconsider explanation deletion explained saying string chomsky sue gave deleted notion identity described process instance chomsky merged second notion identity second instance sue gave merged word string deleting help problem deletion account outlined particular help exclude example suggest shared material identical syntactic structure started think term merging obvious step merging word string merging syntax tree goodall advocate treating coordination union phrase marker pasting tree identical node merging goodall visualise result term dimensional tree structure merged material plane syntax tree conjunct plane example consider d tree example given fig merged tree includes node dominate shared material sue gave conjuncts retain separate plane denoted star cross branch account deal example argues example different phenomenon incorporated d account moltmann technical difficulty goodall account van oirsouw moltmann fundamental problem concerning semantic interpretation coordinated structure moltmann provides revised complex d account based muadz coordination category example goodall proposes treatment similar sag problem dealing example different number modifier following syntactic structure appropriate tnt deliver node node structure tnt deliver edinburgh requires node node node node structure fail merge structure dominating shared material tnt deliver identical use ordered phrase structure tree excludes example summary d approach enforce identity syntactic structure shared material way characterising syntactic structure part standard phrase structure tree result strict requirement parallelism conjuncts consider processing strategy syntactic structure shared material characterised state parser",
        "let reconsider explanation deletion explained saying string chomsky sue gave deleted notion identity described process instance chomsky merged second notion identity second instance sue gave merged word string deleting help problem deletion account outlined particular help exclude example suggest shared material identical syntactic structure started think term merging obvious step merging word string merging syntax tree goodall advocate treating coordination union phrase marker pasting tree identical node merging goodall visualise result term dimensional tree structure merged material plane syntax tree conjunct plane example consider d tree example given fig merged tree includes node dominate shared material sue gave conjuncts retain separate plane denoted star cross branch account deal example argues example different phenomenon incorporated d account moltmann technical difficulty goodall account van oirsouw moltmann fundamental problem concerning semantic interpretation coordinated structure moltmann provides revised complex d account based muadz coordination category example goodall proposes treatment similar sag problem dealing example different number modifier following syntactic structure appropriate tnt deliver node node structure tnt deliver edinburgh requires node node node node structure fail merge structure dominating shared material tnt deliver identical use ordered phrase structure tree excludes example summary d approach enforce identity syntactic structure shared material way characterising syntactic structure part standard phrase structure tree result strict requirement parallelism conjuncts consider processing strategy syntactic structure shared material characterised state parser",
        "attempt treat coordination adapting pre existing parsing strategy example atns adapted wood dcgs dahl mccord chart parser haugeneder dahl mccord system similar system coverage wood sysconj system parser point history parse parse second conjunct according configuration found example parsing point encountering parser reaccess configuration parsing john gave stack consisting sentence verb phrase arc traversal verb second conjunct parsed according configuration merge stack configuration completing second conjunct par conjuncts parallel constituent completed example parsing sentence john gave mary book peter paper subjacencythe sysconj system par peter paper subjacency mary book subjacency conjoining level enclosing constituent example verb phase result similar starting sentence noted dahl mccord mechanism mean sysconj inherits problem nonsensical semantics plague deletion account john mary treated john mary mechanism cause problem dealing nested coordination sentence smallest constituent containing study medicine verb phase wanted study medicine coordination conjuncts occurs level difficult deal final conjunct wood dahl mccord use stack based configuration parsing history popped stack internal structure accessed coordination routine rule example following book completed conjunction reached processing account provide reasonable coverage coordination data exact prediction require detailed examination code suggests need level description dynamic grammar provide",
        "dynamic study state transition state specify state left right parser possible mapping state example milward provides dynamic description shift reduce parser dynamic description incremental parser based dependency grammar language dynamic formal declarative appropriate express linguistic generalisation dynamic grammar milward word regarded action performs change syntactic semantic context example parse sentence john like mary mapping initial state intermediate state final state use dynamic grammar describe shift reduce parser state encode current stack configuration related rule correspond shifting reducing large number different stack configuration stack arbitrary size dynamic shift reduce parsing involves use infinite number state differs atns wood finite number state augmented explicit recursion mechanism grammar presented rewrite grammar transition type usual example parse need lexical entry single combination rule schema state string word sentence type appropriate initial final state parse dynamic grammar substring sentence assigned type example like mary combined type appropriate level perform substring coordination grammar extended following combination rule given special transition type conj fsimilar sysconj allows coordination conjuncts map pair state similar encountering conjunction causing earlier stage parsing history popping stack parsing history available example ben gave book sue transition parse paper joe transition final state match state conjunction string combine resulting transition diagram follows iterated coordination example mary peter sue treated way iterated constituent coordination treated phrase structure grammar example transition type augmented feature denoting transition iterated coordination rule iterated type formed follows precise grammaticality prediction dynamic approach depend characterisation state depend particular parsing strategy specified dynamic general prediction consider conjuncts correspond category corresponding word conjuncts provide transition able coordinate reflection fact processing point parsing history predicts substring sentence coordinate substring sentence act conjunct convenience substring hypothesis hypothesis adopted work van oirsouw barry pickering work lambek calculus moortgat counterexample follows difficult exclude",
        "syntactic constraint excluding acceptable natural example conjuncts formed fragment different constituent following relative unacceptability example explained violation intonational requirement syntactic requirement steedman case dynamic grammar violate substring hypothesis string involves coordination internal state accessible interleaving coordination argument blocking coordination case involve breaking idiom structure standard case lexical subcategorisation example mark steedman case following noted precise grammaticality prediction depend kind parsing model encoded state milward dynamic specifies word word incremental parser lexicalised version dependency grammar state defined category similar category categorial grammar example parsing possible state sentence missing sentence modifier state appropriate initial state parse secretary resulting final state category sentence example dealt syntactic context distinguish subsequent modifier lack distinction modifier expected necessary prerequisite performing decidable word word incremental interpretation milward cooper problem categorial grammar account coordination reoccur dynamic account based parser milward example predicted acceptable following second batch example difficult exclude making change characterisation state feature tensed verb conjunct block difficult motivate grammar regarded formal system direct representation processing inbetween example packed parallel parser described milward actual parsing state packed version state grammar consider dynamic direct representation processing dependence linguistic data parsing state plausible parsing process corresponds extent actual human language processing brings intriguing possibility predict coordination fact known processing data vice example consider known example garden pathing choice use raced main verb reduced relative assumed fragment horse raced suggesting distinguished parsing state raced predicts unacceptability following",
        "theory semantic interpretation use term manipulation compute meaning sentence theory implemented language prolog simulate term operation order unification case obscuring underlying linguistic theory trick needed implementation example combinatory categorial grammar ccg steedman theory syntax semantic interpretation attractive characteristic handling coordination construct theory aspect ccg semantics simulated order unification simulation break interesting case ccg handle problem general ccg particular implementation language sufficient expressive power allow direct encoding solution given paper advance logic programming allow implementation semantic theory direct natural way ccg case study begin illustrating order unification inadequate coordination construct review proposed solution sentence logical form theory get derived raising john predicate take argument return sentence bill get coordination result following john bill applied predicate result reduction order unification need simulated having variable unify bill john possible jowsey moore thorough discussion suggests way overcome problem use explicit term encode reduction perform needed reduction example logical form produced representation reduced clause apply result small example writing apply predicate difficult semantic term complex trivial matter write reduction handle variable capture point desired determine semantic form different sentence predicate needed compare lambda form equivalence simple task logic variable meant interpreted bound variable requires additional layer programming proposes solution order unification handle sentence complex example determiner method introduce spurious binding removed example semantics simplified push order unification shown capable disadvantage technique possible category conjoined separate lexical entry required conjoinable category complex entry complex obscure theoretical background grammar formalism fundamental problem case concept free bound occurrence variable supported prolog need implemented additional programming possible problematic implement solution given paper use higher order logic programming language implement concept called syntax miller higher order syntax pfenning elliot allows natural elegant implementation grammatical theory lexical entry",
        "ccg grammatical formalism correspondence rule composition level syntax logical form word assigned category syntactical operation assign new category constituent corresponding semantic operation produce new constituent ccg rule shown figure implemented system described paper operation forward backward variant illustration semantic rule simulated order unification consider derivation constituent harry found harry category harry found transitive verb category lfin ccg formalism derivation follows harry get raised rule composed rule found result category type section seen use syntax allows expressed order unification simulated shown figure final ccg rule considered coordination rule specifies category coordinate schema family rule called generalized coordination semantic rule different case example unary function semantic rule function argument rule example processing rule resultwhich equivalent",
        "logic programming language based higher order hereditary harrop formula miller differs prolog order term unification replaced typed term higher order unification permit universal quantification implication goal clause crucial aspect paper feature permit usage syntax express logical form term computed ccg built term manipulation meta language object language ccg logical form expressed variable object language mapped variable meta language code fragment shown figure declares ccg logical form represented ccg represented untyped term type tm ab represents object level abstraction meta level expression ab meta level function type meta level abstraction written walked type meta level function type object level representation type found shown represented encodes application derivation harry found type raised harry value second figure show declares quantifier represented required sentence processed determiner exists encoded abstraction functional argument object level binding variable quantifier handled meta level abstraction simple constructor implication conjunction forall exists typical manner pereira shieber example sentence man found bone possible representation figure illustrates ccg operation encoded type meta level proposition intended usage apply argument type object level abstraction set equal application second queryr unifies function harry meta level application built reduction word object level function application handled meta level function application composition similar derivation harry found type raising forward composition type raised raise clause produce composed found result shown following query point reduction needed problem writing reducer prolog simple matter meta level reduction eliminate redexes produce final result wo complete declaration reducer key clause syntax capability direct implementation underlying linguistic formalism stark contrast order simulation shown figure",
        "primary goal abstract syntax support recursion abstraction bound variable lead interpretation bound variable scoped constant act constant visible term visible descent abstraction miller discussion evaluation functional program pushing evaluation abstraction reduce redexes level technique reducer briefly mentioned end previous section similar technique implement coordination descending argument coordinated describing implementation coordination necessary mention ccg category represented code shown figure cat declared primitive type conj noun category implementation declared constructor forward backward slash example ccg category transitive verb represented predicate atomic type declared true atomic category implementation coordination test termination recursion implementation coordination us capability universal quantification goal clause meta level operator written operational semantics state provable provable new variable type occur current signature word scoped constant current signature get expanded proof meant treated generic placeholder arbitrary proper type appear term instantiated logic variable proof significance restriction illustrated code coordination shown figure argument coord category term object level representation constituent category argument result coordination second argument earlier problematic example coordination john type raised bill category obtain john bill following query match clause coord instantiated tos toand logic variable waiting instantiation meta level reduction new scoped constant following goal called atomic type coord clause match withb instantiated tos ton tosince higher order unification instantiate extracting result original query isnote scoped constant arising proof universal quantification instantiationis prohibited extraction remove body abstraction use universal quantification extract term containing case give result direct implementation rule cooordination unary function process recursive descent scoped constant work member conj rule family example following querycorresponds rule use bound variable name obj sub cause difficulty use scoped constant meta level reduction higher order unification access manipulate inner term park requires careful consideration handling determiner coordination sentence",
        "shown higher order logic programming implement semantic theory ccg including difficult case handling coordination construct technique allow similar advantage variety theory argument approach taken relies formalism entail implementation issue difficult solution efficient implementation issue complex understood expected future work bring improvement example straightforward matter transform code logic called miller requires restricted form unification decidable linear time space declarative nature program open possibility application program transformation partial evaluation",
        "lexicalist approach incorporating technique shake bake generation beaven beaven whitelock combine linguistic advantage transfer arnold allegranza interlingual nirenburg dorr approach generation algorithm described date intractable paper describe alternative generation component polynomial time complexity bake translation assumes source grammar target grammar bilingual dictionary relates equivalent set lexical sign carrying semantic dependency established source language analysis stage target language generation stage translation process consists phase parsing phase output multiset bag source language sign instantiated rich linguistic information established parse ensure adequate translation lexical semantic transfer phase employ bilingual dictionary map bag instantiated source sign bag target language sign generation phase imposes order bag target sign guaranteed grammatical according monolingual target grammar ordering respect linguistic constraint transferred target sign shake bake generation algorithm whitelock combine target language sign technique known generate test effect arbitrary permutation sign input shift reduce parser test grammatical formedness formed system halt indicating success permutation tried process repeated complexity algorithm permutation input size explored find correct answer explored order verify answer shake bake approach employed technique improve generation efficiency example beaven employ chart avoid recalculating combination sign testing popowich proposes general technique storing rule application attempted brew avoids certain pathological case employing global constraint solution space researcher brown chen lee provide system bag generation guided probability approach guaranteed protracted search time exact answer required bag generation np complete brew novel generation algorithm polynomial complexity reduction theoretical complexity achieved placing constraint power target grammar operating instantiated sign restrictive data structure bag target language normalised commutative bracketing tncb tncb record dominance information derivation amenable incremental update allows employ greedy algorithm refine structure target constituent found generation succeeded change generation failed following section sketch basic algorithm consider provide initial guess provide informal proof efficiency",
        "begin describing fundamental greedy incremental generation algorithm crucial data structure employ tncb definition state key assumption suitable tncbs generation describe algorithm assume sign based grammar binary rule combine sign unifying daughter category returning mother commutative equivalent rule application linear ordering daughter lead successful rule application determines orthography mother shake bake generation algorithm attempt arrange bag target sign grammatical ordering ordering allows sign combine yield single sign found target derivation information assist algorithm derivation information cache previous result avoid exact recomputation stage improve previous guess reason believe improvement possible given adequate information previous stage target sign combine accident underlying semantics sign license linguistic data sign contain allows combine providing semantics specified example consider bag sign derived shake bake process represent phrase determiner adjective modify noun grammar allow construct phrase correct fail sign bag incorporated final result naive algorithm intervening computation intractable algorithm presented start observation phrase incorrect specification advantage recording constituent combined tncb designed allow constituent incorporated minimal recomputation tncb composed sign history derived child structure binary derivation tree child unordered nil triple second item tncb triple child tncbs value tncb sign formed combination child inconsistent representing fact combine undetermined established sign combine tncbs commutative distinguish structure shown figure section property important starting generation process introduce terminology tncb iswell formed iff value sign formed iff value inconsistent undetermined value undetermined iff demonstrated formed formed iff formed parent formed word maximal tncb largest formed component tncb tncbs tree like structure tncb undetermined formed ancestor tncbs contain define operation tncb define fourth transformation improves formed tncbs establish formedness undetermined node diagram use cross represent formed node black circle represent undetermined one maximal tncb deleted current position structure adjusted order maintain binary branching",
        "figure node deleted parent node new node representing combination marked undetermined maximal tncb conjoined maximal tncb combined rule figure seen maximal tncb composed node conjoined maximal tncb composed node giving tncb node new node formed maximal tncb inserted maximal tncb conjoined non maximal tncb combination licensed rule figure tncb composed node inserted tncb composed node node figure dominate node corresponding new combination node marked undetermined node said disrupted combination deletion subsequent conjunction adjunction figure illustrate conjunction left hand figure assume wish maximal tncb maximal tncb involves deleting tncb noting raising node replace node introduce node node node child deletion remove surplus node node case conjunction adjunction introduce new node case maintaining number node tree movement tncb undetermined demonstrated figure sign affected part recalculated combining evaluated child tncbs shake bake system whitelock employ bag generation algorithm assumed input generator collection instantiated sign scale bag generation necessary sufficient information transferred source language constrain subsequent search generation property required tncbs target grammar instantiated lexical sign precedence monotonicity order orthography combining sign orthography result determinate depend subsequent combination result undergo constraint say constituent fails combine permutation element making render combination possible allows evaluation occur linear time practice restriction requires rich information transferred previous translation stage ensure sign combination deterministic monotonicity maximal tncb adjoined highest possible place tncb result formed evaluated attempted conjunction fails fact conjunction special case adjunction node disrupted adjunction disrupts node attempted disrupts node monotonicity requires node disrupted control regime formed evaluated ensure termination generation algorithm step number lexical sign input process investigating mathematical characterisation grammar instantiated sign obey constraint found restriction problematic generator cycle phase test phase rewrite phase bag sign corresponding big brown dog barked passed generation phase step generation process convert arbitrary tncb structure figure order verify structure valid evaluate tncb test phase tncb",
        "evaluates orthography value desired result enter rewrite phase continuing spirit original shake bake generation process form arbitrary mutation tncb retest repeating test rewrite cycle found formed tncb failed intractable undirectedness search vast number possibility added derivation information contained tncbs property mentioned direct search improving evaluated result enter rewrite phase formed tncb operation improve operation maintains number node tree deletion maximal tncb remove formed node figure deletion site new undetermined node created formed destination site movement conjunction adjunction new formed node created ancestor new formed node formed movement verify case maximal tncbs conjoined node dominating new node formed undetermined evaluated remain formed formed adjoin maximal tncb tncb node dominating new formed node disrupted dominance monotonicity node disrupted adjunction formed evaluation node dominating disrupted node formed formed evaluation rewriting evaluating improve tncb consider contrived worst case starting point provided figure test phase discover single interior node formed scan tncb left right looking maximal tncb case past bark conjunction figure test phase fails provide formed tncb repeat rewrite phase time finding dog conjoin figure show state second pas test phase testing enter rewrite phase time note brown inserted maximal tncb dog barked adjoined dog figure combining dog parent sign reflects correct orthography correct linear precedence finding big conjoined brown dog try adjoin combine brown dog adjunction lower tncb attempted final result tncb figure orthography big brown dog barked generation formed basic constituent dog refined adjoining modifier place heart approach formed constituent grow dismantled generation fails maximal formed fragment built presented user allowing graceful degradation output quality",
        "assume sign based grammar binary rule combine sign unifying daughter category returning mother commutative equivalent rule application linear ordering daughter lead successful rule application determines orthography mother shake bake generation algorithm attempt arrange bag target sign grammatical ordering ordering allows sign combine yield single sign found target derivation information assist algorithm derivation information cache previous result avoid exact recomputation stage improve previous guess reason believe improvement possible given adequate information previous stage target sign combine accident underlying semantics sign license linguistic data sign contain allows combine providing semantics specified example consider bag sign derived shake bake process represent phrase determiner adjective modify noun grammar allow construct phrase correct fail sign bag incorporated final result naive algorithm intervening computation intractable algorithm presented start observation phrase incorrect specification advantage recording constituent combined tncb designed allow constituent incorporated minimal recomputation tncb composed sign history derived child structure binary derivation tree child unordered nil triple second item tncb triple child tncbs value tncb sign formed combination child inconsistent representing fact combine undetermined established sign combine tncbs commutative distinguish structure shown figure section property important starting generation process introduce terminology tncb iswell formed iff value sign formed iff value inconsistent undetermined value undetermined iff demonstrated formed formed iff formed parent formed word maximal tncb largest formed component tncb tncbs tree like structure tncb undetermined formed ancestor tncbs contain define operation tncb define fourth transformation improves formed tncbs establish formedness undetermined node diagram use cross represent formed node black circle represent undetermined one maximal tncb deleted current position structure adjusted order maintain binary branching figure node deleted parent node new node representing combination marked undetermined maximal tncb conjoined maximal tncb combined rule figure seen",
        "maximal tncb composed node conjoined maximal tncb composed node giving tncb node new node formed maximal tncb inserted maximal tncb conjoined non maximal tncb combination licensed rule figure tncb composed node inserted tncb composed node node figure dominate node corresponding new combination node marked undetermined node said disrupted combination deletion subsequent conjunction adjunction figure illustrate conjunction left hand figure assume wish maximal tncb maximal tncb involves deleting tncb noting raising node replace node introduce node node node child deletion remove surplus node node case conjunction adjunction introduce new node case maintaining number node tree movement tncb undetermined demonstrated figure sign affected part recalculated combining evaluated child tncbs",
        "shake bake system whitelock employ bag generation algorithm assumed input generator collection instantiated sign scale bag generation necessary sufficient information transferred source language constrain subsequent search generation property required tncbs target grammar instantiated lexical sign precedence monotonicity order orthography combining sign orthography result determinate depend subsequent combination result undergo constraint say constituent fails combine permutation element making render combination possible allows evaluation occur linear time practice restriction requires rich information transferred previous translation stage ensure sign combination deterministic monotonicity maximal tncb adjoined highest possible place tncb result formed evaluated attempted conjunction fails fact conjunction special case adjunction node disrupted adjunction disrupts node attempted disrupts node monotonicity requires node disrupted control regime formed evaluated ensure termination generation algorithm step number lexical sign input process investigating mathematical characterisation grammar instantiated sign obey constraint found restriction problematic",
        "generator cycle phase test phase rewrite phase bag sign corresponding big brown dog barked passed generation phase step generation process convert arbitrary tncb structure figure order verify structure valid evaluate tncb test phase tncb evaluates orthography value desired result enter rewrite phase continuing spirit original shake bake generation process form arbitrary mutation tncb retest repeating test rewrite cycle found formed tncb failed intractable undirectedness search vast number possibility added derivation information contained tncbs property mentioned direct search improving evaluated result enter rewrite phase formed tncb operation improve operation maintains number node tree deletion maximal tncb remove formed node figure deletion site new undetermined node created formed destination site movement conjunction adjunction new formed node created ancestor new formed node formed movement verify case maximal tncbs conjoined node dominating new node formed undetermined evaluated remain formed formed adjoin maximal tncb tncb node dominating new formed node disrupted dominance monotonicity node disrupted adjunction formed evaluation node dominating disrupted node formed formed evaluation rewriting evaluating improve tncb consider contrived worst case starting point provided figure test phase discover single interior node formed scan tncb left right looking maximal tncb case past bark conjunction figure test phase fails provide formed tncb repeat rewrite phase time finding dog conjoin figure show state second pas test phase testing enter rewrite phase time note brown inserted maximal tncb dog barked adjoined dog figure combining dog parent sign reflects correct orthography correct linear precedence finding big conjoined brown dog try adjoin combine brown dog adjunction lower tncb attempted final result tncb figure orthography big brown dog barked generation formed basic constituent dog refined adjoining modifier place heart approach formed constituent grow",
        "considering algorithm described note number rewrite necessary repair initial guess number formed tncbs exceed number interior node tncb formed lexical sign formed initial tncb generator fewer number rewrite required complete generation section illustrated initial guess bad possible section consider heuristic producing motivated guess initial tncb tncbs figure interpret subject object verb observe equivalence structure bracketings implication equivalence translating language head final language isomorphic dominance structure source target par mirroring source parse structure initial target tncb provide correct initial guess example english sentence corresponding japanese equivalent mirror japanese bracketing structure english form initial tncb obtain book red produce correct answer test phase generation need rewrite exact isomorphism source target commutative bracketings guess reasonable majority child commutative bracketings target language isomorphic equivalent source language french sentence tncb implied bracketing equivalent figure requires rewrite order formed tncbs mirror dominance information source language parse order furnish generator good initial guess hand matter structure differ algorithm operate polynomial complexity transfer incorporated improve efficiency generation necessary correctness tractability",
        "theoretical complexity generator size input informal argument complexity test phase number evaluation node tested worst case precedence monotonicity try combine child direction according grammar rule non leaf node complexity test phase complexity rewrite phase locating tncbs combined worst case imagine picking arbitrary child tncb trying find combine complexity phase product picking combining complexity combined complexity test rewrite cycle section argued rewrite necessary overall complexity generation solution found case complexity dependent quality guess tncb structure improved extent tncb evaluated rewriting slemat system poznanski tried form good initial guess mirroring source structure target tncb allowing local structural modification bilingual equivalence transfer operation affect efficiency functionality generation specification refined tested efficiency complete specification transfer operation required correct generation grammatical target text version shake bake translation presented maintains advantage traditional transfer model respect monotonicity constraint hand constitute dilution shake bake ideal independent grammar instance precedence monotonicity requires status clause lexical head main subordinate transferred german transfer information compromise ideal information appear transfer entry avoid grammatical incorrect translation great man translated homme grand problem justifying main subordinate distinction language wish translate german distinction justified language treat english french japanese constraint require monolingual grammar enriched unmotivated feature clear translation coverage extended new language pair added",
        "word grouping useful language processing task available thesaurus appear line distributional technique widespread bensch savitch brill brown grefenstette mckeown hatzivassiloglou pereira schuetze task interested relationship word sens word example cluster containing attorney counsel trial court judge brown illustrate sticky group word case sense ambiguity involved reader impose coherent interpretation word group aware computational system choice consider awkward possibility example cluster capturing distributional relationship advice sense counsel royalty sense court mistake application query expansion information retrieval surfeit false connection outweigh benefit obtained lexical knowledge obvious solution problem extend distributional grouping method word sens example construct vector representation sens basis co occurrence word sens corpus annotated word sense information computing reliable statistic word sens word require data available example large sense tagged corpus wordnet group annotated subset brown corpus illustrates difficulty obtaining suitable data small current corpus standard order hundred thousand word million ten million direct annotation methodology create labor intensive marcus found direct annotation take long automatic tagging correction speech annotation output quality reflects difficulty task inter annotator disagreement order contrasted error rate reported speech annotation marcus attempt capture behavior semantic category distributional setting despite unavailability sense annotated corpus example hearst schuetze step distributional treatment wordnet based class schuetze approach constructing vector representation large co occurrence matrix algorithm sense disambiguation thought way determining roget thesaurus category behave respect contextual feature treatment selectional constraint resnik provides way describe plausibility co occurrence term wordnet semantic category co occurrence relationship mediated syntactic structure case begin known semantic category wordnet synset roget numbered class non annotated text proceeds distributional characterization semantic category behavior co occurrence relationship paper begin different starting point cited work presupposition sense annotated text available assumption word grouping obtained black box procedure analysis unannotated text goal annotate word grouping post hoc knowledge based catalogue sens successful approach obvious benefit use source good word grouping available unsupervised word clustering method line thesaurus like folding complexity dealing word sens time resulting sense grouping useful variety purpose work motivated goal sense",
        "let state problem follows given set word word having associated set possible sens assume exists set representing set word sens ideal human judge conclude belong group sens corresponding word grouping goal define membership function take argument computes value representing confidence state sense belongs sense grouping principle precludes possibility multiple sens word included following word group restricting attention noun sens wordnet lookout crate polysemous word group expect assign value unique sens monosemous word assign high value lookout sense lower value expected sens lookout correspond observation tower activity watching wordnet sens correspond physical object quantity crateful crateful orange intuition included second receive higher value individual constitutes ideal human judge core disambiguation algorithm computation semantic similarity wordnet taxonomy topic investigated number people leacock chodorow resnik sussna paper restrict attention wordnet taxonomy noun approach semantic similarity evaluated basis information content shared item compared intuition approach simple similar word informative specific concept subsumes upper bound taxonomy concept corresponds wordnet synset traditional method evaluating similarity semantic network measuring path length node lee rada capture albeit semantic network hierarchy minimal path link node mean necessary high taxonomy concept order find upper bound problem simple path length definition semantic similarity experiment wordnet measure semantic similarity employed provide better match human similarity judgment simple path length resnik word semantic similarity calculated set wordnet synset subsume ancestor sense word concept maximizes expression referred informative subsumer way associate probability taxonomic class reasonable require concept probability non decreasing move higher taxonomy implies guarantee mean informative defining informativeness traditional way term log likelihood estimate derived corpus computingwhere set noun having sense subsumed concept probability computed relative frequency total number noun instance observed plural form counted noun noun covered wordnet ignored wordnet noun taxonomy multiple root node single virtual root node assumed exist original root node child",
        "equation sens virtual root node upper bound similarity value following table show semantic similarity computed word pair case shown informative subsumer estimated penn treebank version brown corpus pair come example given church hank illustrating word human subject judged associated word doctor word sick appeared list excluded noun doctor similar medicine hospital thing instance having concrete existence living nonliving wordnet class entity similar lawyer kind professional people similar nurse professional people working health profession similarity specialized notion association relatedness doctor sickness associated judge similar disambiguation algorithm noun group inspired observation polysemous word similar informative subsumer provides information sense word relevant table example doctor nurse polysemous wordnet record doctor kind health professional hold ph nurse mean health professional nanny word considered shared element meaning relevant sens emerges form informative subsumer pairing possible sens share element meaning example doctor ph nurse nanny descendant person individual case illustrated specific informative shared ancestor suggests sens come mind word considered working hypothesis paper hold true general observation algorithm requires thing way assign credit word sens based similarity co occurring word tractable way generalize case polysemous word involved algorithm given figure algorithm considers word pairwise avoiding tractability problem considering possible combination sens group word sens pair considered informative subsumer identified pair considered supporting evidence sens descendant concept equation support sum log probability preferring sens high support equivalent optimizing product probability considering word pairwise algorithm reflects probabilistic independence assumption informative subsumer doctor nurse health professional pairing contributes support sense doctor ph contributes support sense nurse health professional nanny support contributed pairwise comparison proportional informative informative subsumer evidence sens word influenced similar word similar word time process completed pair sense word group",
        "potential receiving supporting evidence pairing word group value assigned sense proportion support receive support possible kept track array normalization pseudocode intuition algorithm intuition exploited lesk sussna plausible assignment sens multiple co occurring word maximizes relatedness meaning sens chosen explicit comparison sussna approach similar previous work give example problem solving following paragraph corpus time magazine article information retrieval research time corpus lowercase readability punctuation appears original corpus sussna extract following noun grouping disambiguate non stopword noun paragraph appear wordnet version description sussna algorithm disambiguating noun grouping similar proposed number way relatedness characterized term semantic network wordnet focus noun evaluation semantic similarity sussna case semantic distance basis sense selection important difference sussna proposal algorithm aim disambiguate grouping noun established clustering manual effort related opposed grouping noun happen appear running text reflect relatedness based meaning provides justification restricting attention similarity reflected scaffolding link taxonomy opposed general notion association difference reflected fact sussna us link wordnet link sussna algorithm semantic similarity distance computation based path length information content choice argued resnik resnik combinatorics handled sussna explores analyzing sense combination living exponential complexity alternative freezing single sense choice assumed correct basis disambiguating algorithm presented fall alternative final important difference algorithm previous algorithm sense disambiguation offer possibility assigning higher level wordnet category lowest level sense label simple modification algorithm assign value synset containing word ancestor synset need let list synset associated word problem statement section include synset ancestor synset containing word num sens sense reinterpreted algorithm compute synset including word higher level abstraction word group doctor nurse lawyer include subsuming concept word synset member concept non value follows doctor nurse lawyer given assignment level abstraction obvious method semantic annotation assign highest level concept large sense specific value instance previous example assign annotation health professional doctor",
        "let state problem follows given set word word having associated set possible sens assume exists set representing set word sens ideal human judge conclude belong group sens corresponding word grouping goal define membership function take argument computes value representing confidence state sense belongs sense grouping principle precludes possibility multiple sens word included following word group restricting attention noun sens wordnet lookout crate polysemous word group expect assign value unique sens monosemous word assign high value lookout sense lower value expected sens lookout correspond observation tower activity watching wordnet sens correspond physical object quantity crateful crateful orange intuition included second receive higher value individual constitutes ideal human judge",
        "core disambiguation algorithm computation semantic similarity wordnet taxonomy topic investigated number people leacock chodorow resnik sussna paper restrict attention wordnet taxonomy noun approach semantic similarity evaluated basis information content shared item compared intuition approach simple similar word informative specific concept subsumes upper bound taxonomy concept corresponds wordnet synset traditional method evaluating similarity semantic network measuring path length node lee rada capture albeit semantic network hierarchy minimal path link node mean necessary high taxonomy concept order find upper bound problem simple path length definition semantic similarity experiment wordnet measure semantic similarity employed provide better match human similarity judgment simple path length resnik word semantic similarity calculated set wordnet synset subsume ancestor sense word concept maximizes expression referred informative subsumer way associate probability taxonomic class reasonable require concept probability non decreasing move higher taxonomy implies guarantee mean informative defining informativeness traditional way term log likelihood estimate derived corpus computingwhere set noun having sense subsumed concept probability computed relative frequency total number noun instance observed plural form counted noun noun covered wordnet ignored wordnet noun taxonomy multiple root node single virtual root node assumed exist original root node child equation sens virtual root node upper bound similarity value following table show semantic similarity computed word pair case shown informative subsumer estimated penn treebank version brown corpus pair come example given church hank illustrating word human subject judged associated word doctor word sick appeared list excluded noun doctor similar medicine hospital thing instance having concrete existence living nonliving wordnet class entity similar lawyer kind professional people similar nurse professional people working health profession similarity specialized notion association relatedness doctor sickness associated judge similar",
        "disambiguation algorithm noun group inspired observation polysemous word similar informative subsumer provides information sense word relevant table example doctor nurse polysemous wordnet record doctor kind health professional hold ph nurse mean health professional nanny word considered shared element meaning relevant sens emerges form informative subsumer pairing possible sens share element meaning example doctor ph nurse nanny descendant person individual case illustrated specific informative shared ancestor suggests sens come mind word considered working hypothesis paper hold true general observation algorithm requires thing way assign credit word sens based similarity co occurring word tractable way generalize case polysemous word involved algorithm given figure algorithm considers word pairwise avoiding tractability problem considering possible combination sens group word sens pair considered informative subsumer identified pair considered supporting evidence sens descendant concept equation support sum log probability preferring sens high support equivalent optimizing product probability considering word pairwise algorithm reflects probabilistic independence assumption informative subsumer doctor nurse health professional pairing contributes support sense doctor ph contributes support sense nurse health professional nanny support contributed pairwise comparison proportional informative informative subsumer evidence sens word influenced similar word similar word time process completed pair sense word group potential receiving supporting evidence pairing word group value assigned sense proportion support receive support possible kept track array normalization pseudocode intuition algorithm intuition exploited lesk sussna plausible assignment sens multiple co occurring word maximizes relatedness meaning sens chosen explicit comparison sussna approach similar previous work give example problem solving following paragraph corpus time magazine article information retrieval research time corpus lowercase readability punctuation appears original corpus sussna extract following noun grouping disambiguate non stopword noun paragraph appear wordnet version description sussna algorithm disambiguating noun grouping similar proposed number way relatedness characterized term semantic network",
        "wordnet focus noun evaluation semantic similarity sussna case semantic distance basis sense selection important difference sussna proposal algorithm aim disambiguate grouping noun established clustering manual effort related opposed grouping noun happen appear running text reflect relatedness based meaning provides justification restricting attention similarity reflected scaffolding link taxonomy opposed general notion association difference reflected fact sussna us link wordnet link sussna algorithm semantic similarity distance computation based path length information content choice argued resnik resnik combinatorics handled sussna explores analyzing sense combination living exponential complexity alternative freezing single sense choice assumed correct basis disambiguating algorithm presented fall alternative final important difference algorithm previous algorithm sense disambiguation offer possibility assigning higher level wordnet category lowest level sense label simple modification algorithm assign value synset containing word ancestor synset need let list synset associated word problem statement section include synset ancestor synset containing word num sens sense reinterpreted algorithm compute synset including word higher level abstraction word group doctor nurse lawyer include subsuming concept word synset member concept non value follows doctor nurse lawyer given assignment level abstraction obvious method semantic annotation assign highest level concept large sense specific value instance previous example assign annotation health professional doctor nurse capturing generalization presence word group appropriate level abstraction annotation professional lawyer",
        "intuition algorithm intuition exploited lesk sussna plausible assignment sens multiple co occurring word maximizes relatedness meaning sens chosen explicit comparison sussna approach similar previous work give example problem solving following paragraph corpus time magazine article information retrieval research time corpus lowercase readability punctuation appears original corpus sussna extract following noun grouping disambiguate non stopword noun paragraph appear wordnet version description sussna algorithm disambiguating noun grouping similar proposed number way relatedness characterized term semantic network wordnet focus noun evaluation semantic similarity sussna case semantic distance basis sense selection important difference sussna proposal algorithm aim disambiguate grouping noun established clustering manual effort related opposed grouping noun happen appear running text reflect relatedness based meaning provides justification restricting attention similarity reflected scaffolding link taxonomy opposed general notion association difference reflected fact sussna us link wordnet link sussna algorithm semantic similarity distance computation based path length information content choice argued resnik resnik combinatorics handled sussna explores analyzing sense combination living exponential complexity alternative freezing single sense choice assumed correct basis disambiguating algorithm presented fall alternative final important difference algorithm previous algorithm sense disambiguation offer possibility assigning higher level wordnet category lowest level sense label simple modification algorithm assign value synset containing word ancestor synset need let list synset associated word problem statement section include synset ancestor synset containing word num sens sense reinterpreted algorithm compute synset including word higher level abstraction word group doctor nurse lawyer include subsuming concept word synset member concept non value follows doctor nurse lawyer given assignment level abstraction obvious method semantic annotation assign highest level concept large sense specific value instance previous example assign annotation health professional doctor nurse capturing generalization presence word group appropriate level abstraction annotation professional lawyer",
        "section present number example evaluation inspection case source noun grouping grouping word description word sens value cluster brown head body hand eye voice arm seat hair mouth group class hand selected brown interesting distributional cluster brown tie jacket suit cluster derived brown modification algorithm designed uncover sticky cluster cluster brown cost expense risk profitability deferral earmark capstone cardinality mintage reseller cluster presented brown selected class hand picked coherence hand selected group presentation distributional neighborhood schuetze burglar thief rob mugging stray robbing lookout chase crate noted section group represents set word similar burglar according schuetze method deriving vector representation corpus behavior case word rob robbing excluded noun wordnet word stray excluded appears list adjective stray bullet generated thesaurus entry grefenstette method test mean procedure technique chose grouping random thesaurus created grefenstette syntactico distributional method med corpus medical abstract source group come thesaurus entry word method mean mean tradition sense disambiguation taking ambiguous word evaluating system performance word look case word line goal sense algorithm chooses considering word context roget thesaurus class appears class includes noun numbered category following list provides brief description sens line wordnet line appears numbered category roget thesaurus description value large present paper showing noun numbered category space average noun identify numbered category wordnet sens line greatest algorithm good job category reader find interesting exercise try decide sens choose case algorithm category",
        "distributional cluster brown head body hand eye voice arm seat hair mouth group class hand selected brown interesting distributional cluster brown tie jacket suit cluster derived brown modification algorithm designed uncover sticky cluster cluster brown cost expense risk profitability deferral earmark capstone cardinality mintage reseller cluster presented brown selected class hand picked coherence hand selected group presentation distributional neighborhood schuetze burglar thief rob mugging stray robbing lookout chase crate noted section group represents set word similar burglar according schuetze method deriving vector representation corpus behavior case word rob robbing excluded noun wordnet word stray excluded appears list adjective stray bullet generated thesaurus entry grefenstette method test mean procedure technique chose grouping random thesaurus created grefenstette syntactico distributional method med corpus medical abstract source group come thesaurus entry word method mean mean",
        "tradition sense disambiguation taking ambiguous word evaluating system performance word look case word line goal sense algorithm chooses considering word context roget thesaurus class appears class includes noun numbered category following list provides brief description sens line wordnet line appears numbered category roget thesaurus description value large present paper showing noun numbered category space average noun identify numbered category wordnet sens line greatest algorithm good job category reader find interesting exercise try decide sens choose case algorithm category",
        "previous section provided illustrative example demonstrating performance algorithm interesting case section present experimental result rigorous evaluation methodology evaluation came numbered category roget instance consisted noun group noun numbered category single word group disambiguated use example previous section category writing contains following word phrase group appears noun taxonomy wordnet candidate test instance example line secret writing test set chosen random contained test case note random choice case test instance came numbered category human judge given test case disambiguate case given set noun numbered category shown description wordnet sens word disambiguated example list sens line given previous section word sens forced choice task judge required choose sense addition judgment judge required provide confidence value decision ranging confident confident presented judge purpose evaluation test instance judge low confidence confidence rating excluded judge test instance high confidence considered baseline run selecting sens random choice average percent correct standard deviation upper bound judge correct test instance disambiguation algorithm show considerable progress upper bound correct judge test instance high confidence considered baseline run selecting sens random choice average percent correct standard deviation upper bound judge correct test instance disambiguation algorithm performs correct",
        "result evaluation encouraging considering disambiguating word sens level fine grainedness found wordnet bit difficult disambiguation level homograph hearst cowie note worth adding clear exact match criterion evaluating algorithm percentage exact match sense selection human judged baseline right task particular task important avoid inappropriate sens select right case query expansion information retrieval example adding inappropriate word query degrade performance voorhees example presented section encouraging regard addition performing task assigning high score best sense good job assigning low score sens inappropriate criterion success algorithm need evaluation plan include larger scale version experiment presented involving thesaurus class designed evaluation algorithm fare presented noun group produced distributional clustering addition plan explore alternative measure semantic similarity example improved variant simple path length proposed leacock chodorow algorithm intended suite technique disambiguating word running text respect wordnet sens argue success task require combining knowledge kind wordnet provides relatedness meaning knowledge kind provided corpus usage context difficulty kind knowledge widespread success characterizing lexical behavior term distributional relationship applied level word word form opposed sens paper represents step getting leverage possible work paradigm help determine relationship word sens action",
        "semitic known computational linguist particular computational morphologists inflexional morphology root pattern phenomenon pose difficulty morphological system make error detection difficult task paper aim presenting morphographemic model cope issue following convention adopted represented brace surface phonological form solidus orthographic string acute bracket example grammar variable begin capital letter denote consonant denote vowel bar denotes complement asterisk indicates formed string difficulty morphological analysis error detection semitic arise following fact non linearitya semitic stem consists root vowel melody arranged according canonical pattern example arabic caused write perfect passive composed root morpheme ktb notion writing vowel melody morpheme perfect passive arranged according pattern morpheme cvccvc causative phenomenon analysed mccarthy line autosegmental phonology goldsmith analysis appears semitic text appear form consonantal text incorporate short vowel matres lectionis arabic ktb kaatb vocalised text incorporate short vowel clarify ambiguity kutb distinguish andvocalised text incorporate vocalisation tada diacritic shiftssemitic language employ large number diacritic represent enter alia short vowel doubled letter nunation editor allow user enter diacritic letter speed data entry user enters base character paragraph go enters diacritic common mistake place cursor extra position left entering diacritic result vowel shifted position wkatubi wakutib quality perfect imperfect vowel basic form semitic verb idiosyncratic example syriac root ktb take perfect vowel root take vowel common learner mistake syncopationa consonantal segment omitted phonetic surface form maintained orthographic surface example syriac mdnt city pronounced application morphographemic rule constraint lexical morpheme apply example glottal stop end stem followed relative adjective morpheme iyy arabic heavenly air issuesin broken plural diminutive deverbal noun user enter sound formed word shall discus detail section add language independent issue spell checking damerau transformation omission insertion transposition substitution damerau",
        "section present morphographemic model handle error detection non linear string present formalism subsection describes model order handle non linear phenomenon model adopts level formalism presented pulman hepple multi tape extension kiraz formalism appears special symbol wildcard matching context length restriction operator caters obligatory rule lexical string map surface string iff partitioned pair lexical surface subsequence pair licenced rule partition violates rule multi tape version lexical expression llc lex rlc n tuple regular expression form ith expression refers symbol ith tape nill slot indicated extension giving llc ability contain ellipsis indicates optional omission llc tuples provided tuples left appear left lex morphographemic model add similar formalism expressing error rule error rule capture correspondence error surface correct surface given surrounding partition surface lexical context utilise multi tape format integrate morphological analysis prc left right context lexical correct surface level error obligatory analysis called assumption word free error fails analysis attempted error restriction error rule considered ordinary morphological rule fail error rule succeed lead successful partition word analysis backtracks try error rule point word purpose simplicity likely word contain error damerau pollock zamora normal error analysis resume error rule succeeds exception occurs vowel shift error error rule succeeds expectation shifted vowel set error rule allowed subsequent partition reason rule marked occur error rule selected corrected surface substituted error surface normal analysis continues position substituted surface form variable ground normal analysis sequence lexical matching lexicon tree way lexical word considered variable letter instantiated letter branching current position lexicon tree prolog backtracking explore alternative rule lexical branch applies",
        "order handle non linear phenomenon model adopts level formalism presented pulman hepple multi tape extension kiraz formalism appears special symbol wildcard matching context length restriction operator caters obligatory rule lexical string map surface string iff partitioned pair lexical surface subsequence pair licenced rule partition violates rule multi tape version lexical expression llc lex rlc n tuple regular expression form ith expression refers symbol ith tape nill slot indicated extension giving llc ability contain ellipsis indicates optional omission llc tuples provided tuples left appear left lex morphographemic model add similar formalism expressing error rule error rule capture correspondence error surface correct surface given surrounding partition surface lexical context utilise multi tape format integrate morphological analysis prc left right context lexical correct surface level error obligatory",
        "morphological analysis called assumption word free error fails analysis attempted error restriction error rule considered ordinary morphological rule fail error rule succeed lead successful partition word analysis backtracks try error rule point word purpose simplicity likely word contain error damerau pollock zamora normal error analysis resume error rule succeeds exception occurs vowel shift error error rule succeeds expectation shifted vowel set error rule allowed subsequent partition reason rule marked occur error rule selected corrected surface substituted error surface normal analysis continues position substituted surface form variable ground normal analysis sequence lexical matching lexicon tree way lexical word considered variable letter instantiated letter branching current position lexicon tree prolog backtracking explore alternative rule lexical branch applies",
        "morphological analysis called assumption word free error fails analysis attempted error restriction error rule considered ordinary morphological rule fail error rule succeed lead successful partition word analysis backtracks try error rule point word purpose simplicity likely word contain error damerau pollock zamora normal error analysis resume error rule succeeds exception occurs vowel shift error error rule succeeds expectation shifted vowel set error rule allowed subsequent partition reason rule marked occur",
        "demonstrate model arabic verbal stem shown mccarthy classified according measure trilateral measure quadrilateral one table notice change vowel melody active passive remains invariant change canonical pattern occurs remains invariant present simple level grammar describes data present error checking lexical level maintains lexical tape kay kiraz pattern tape root tape vocalism tape tape scan lexical tree pattern morpheme root morpheme ktb vocalism morpheme active passive following level grammar handle data lexical expression triple lexical expression symbol assume remaining position general rule allows character lexical tape surface infix prefix suffix pattern tape second root tape transition vocalism tape corresponds surface tape rule sanction consonant state pattern tape vocalism tape transition root tape corresponds surface tape rule sanction vowel boundary rule non stem morpheme prefix suffix stem morpheme reading boundary symbol mark end stem llc ensures right boundary rule invoked right time embarking rest rule illustrated example order derivation unri passive morpheme suffix person illustrated number surface tape lexical tape indicate rule sanction move description grammar present spreading rule use ellipsis indicate tuples separating lex llc tuples llc nearest one lex spreading gemination consonant spreading vowel example appear following rule allow different possible orthographic vocalisation semitic text allow optional deletion short vowel non stem stem morpheme note lexical context sure long vowel deleted optional deletion short vowel cause spreading example rule sanction active passive interpretation ktb showin outlined error rule resulting peculiarly semitic problem rule constructed similar vein deal typographical damerau error care issue wrong vocalism vowel shift error rule tried partition short vowel expected lexical vowel position vowel omitted orthographic representation fact contributes problem vowel shift vowel considered shifted vowel omitted word rule deletes vowel surface pas normal analysis partition analysed legitimate omission expected vowel prepares shifted vowel treated way expectation reapplication allowed reap rule shifted vowel deleted surface",
        "partition contextual tuples consist rule surf lex lex element tuple pattern root vocalism shifted vowel analysed omitted stem vowel analysed omitted spread vowel surface lexical restriction context written detail rule use fact context analysed partition check meet condition omitted stem vowel omitted spread vowel example interpreted rule number line indicate vowel shift rule applied replace error surface vowel error surface vowel written italic resulting phonetic syncopation treated accidental omission consonant error result different fault deleted long vowel treated way deleted consonant current transcription practice long vowel written character represented single distinct character form interpreted deleted consonant geminated deleted long vowel type morphographemic error consonant substitution place appending suffix example heaven relative adjective surface given context common mistake write glottal change rule normal morphological spelling change rule incorporating contextual constraint morpheme boundary necessary",
        "lexical level maintains lexical tape kay kiraz pattern tape root tape vocalism tape tape scan lexical tree pattern morpheme root morpheme ktb vocalism morpheme active passive following level grammar handle data lexical expression triple lexical expression symbol assume remaining position general rule allows character lexical tape surface infix prefix suffix pattern tape second root tape transition vocalism tape corresponds surface tape rule sanction consonant state pattern tape vocalism tape transition root tape corresponds surface tape rule sanction vowel boundary rule non stem morpheme prefix suffix stem morpheme reading boundary symbol mark end stem llc ensures right boundary rule invoked right time embarking rest rule illustrated example order derivation unri passive morpheme suffix person illustrated number surface tape lexical tape indicate rule sanction move description grammar present spreading rule use ellipsis indicate tuples separating lex llc tuples llc nearest one lex spreading gemination consonant spreading vowel example appear following rule allow different possible orthographic vocalisation semitic text allow optional deletion short vowel non stem stem morpheme note lexical context sure long vowel deleted optional deletion short vowel cause spreading example rule sanction active passive interpretation ktb showin",
        "outlined error rule resulting peculiarly semitic problem rule constructed similar vein deal typographical damerau error care issue wrong vocalism vowel shift error rule tried partition short vowel expected lexical vowel position vowel omitted orthographic representation fact contributes problem vowel shift vowel considered shifted vowel omitted word rule deletes vowel surface pas normal analysis partition analysed legitimate omission expected vowel prepares shifted vowel treated way expectation reapplication allowed reap rule shifted vowel deleted surface partition contextual tuples consist rule surf lex lex element tuple pattern root vocalism shifted vowel analysed omitted stem vowel analysed omitted spread vowel surface lexical restriction context written detail rule use fact context analysed partition check meet condition omitted stem vowel omitted spread vowel example interpreted rule number line indicate vowel shift rule applied replace error surface vowel error surface vowel written italic resulting phonetic syncopation treated accidental omission consonant error result different fault deleted long vowel treated way deleted consonant current transcription practice long vowel written character represented single distinct character form interpreted deleted consonant geminated deleted long vowel type morphographemic error consonant substitution place appending suffix example heaven relative adjective surface given context common mistake write glottal change rule normal morphological spelling change rule incorporating contextual constraint morpheme boundary necessary",
        "vowel shift error rule tried partition short vowel expected lexical vowel position vowel omitted orthographic representation fact contributes problem vowel shift vowel considered shifted vowel omitted word rule deletes vowel surface pas normal analysis partition analysed legitimate omission expected vowel prepares shifted vowel treated way expectation reapplication allowed reap rule shifted vowel deleted surface partition contextual tuples consist rule surf lex lex element tuple pattern root vocalism shifted vowel analysed omitted stem vowel analysed omitted spread vowel surface lexical restriction context written detail rule use fact context analysed partition check meet condition omitted stem vowel omitted spread vowel example interpreted rule number line indicate vowel shift rule applied replace error surface vowel error surface vowel written italic",
        "section deal morphosyntactic error independent level analysis data described obtained daniel ponsford personal communication based wehr semitic stem consists root morpheme vocalism morpheme arranged according canonical pattern morpheme root occur vocalism pattern lexical entry associated feature structure indicates inter alia possible pattern vocalism particular root nominal data marked plausible occur cited noun common mistake choose wrong pattern case level model succeeds finding level analysis word question fails parsing word stage parser passed root vocalism pattern feature structure unify feature clash situation creates problem constituent preference langer vocalism indicates inflection broken plural preferance vocalism pattern type inflection belongs root example analysed broken plural vocalism pattern type vocalism clash broken plural pattern root expects correct morphological analyser executed generation mode generate broken plural form normal way procedure applied diminutive deverbal noun",
        "model presented corrects error resulting combining nonconcatenative string standard morphological spelling error cover semitic error relating vocalisation diacritic phonetic syncopation morphographemic idiosyncrasy issue broken plural diminutive deverbal noun handled complementary correction strategy depends morphological analysis economic factor important advantage combining morphological analysis error detection correction way lexical tree associated analysis determine correction possibility morphological analysis proceeds selecting rule hypothesise lexical string given surface string rule accepted rejected checking lexical string extend lexical tree current position introduced error rule surface string instantiated associating surface lexical matching lexical string lexicon tree system unable consider correction character lexical impossibility",
        "conversation people mixed initiative control conversation transferred person apply set rule transfer control set dialogue consisting total turn application control rule let derive domain independent discourse structure derived structure indicate initiative play role structuring discourse order explore relationship control initiative discourse process centering analyze distribution different class anaphora data set distribution indicates control segment related analysis suggests discourse participant agree change topic compared initiative task oriented advice giving dialogue found allocation control manner control transferred different dialogue type difference explained term collaborative planning principle",
        "conversation people number characteristic modeled human computer dialogue bidirectional way flow information participant exchanged mixed initiative participant occasion conversational lead partner respond feel free volunteer information requested ask question nickerson initiative pass discourse participant control conversation get transferred discourse participant computational linguist interested factor contribute interactivity discourse theoretical practical motivation wish extend formal account single utterance produced single speaker explain multi participant multi utterance discourse pollack cohen perrault study discourse structure multi participant dialogue factored role mixed initiative allocating control participant grosz cohen assuming passive listener mckeown cohen conversation collaborative process clark wilkes gibbs sack model conversation provide basis extending planning theory grosz sidner cohen situation requires negotiation collaborative plan theory account interacting belief intention multiple participant practical perspective ample evidence mixed initiative contributed lack system usability researcher noted absence mixed initiative give rise problem expert system allow user participate reasoning process ask question want answered pollack kidd frohlich luff addition question answering system fail account system role conversational partner example fragmentary utterance interpreted respect previous user input user reaction system previous response cohen perrault sidner paper focus interactive discourse model mixed initiative utterance type classification set rule transfer control discourse participant proposed whittaker stenton evaluate generality analysis applying control rule set dialogue including advisory dialogue ad task oriented dialogue tod analysed financial support ad financial ad radio talk harry gross speaking money support ad resulted client phoning expert help diagnose repair software fault tod construction plastic water pump telephone keyboard modality application control rule dialogue let derive domain independent discourse segment segment controlled discourse participant propose control segment correspond different subgoals evolving discourse plan addition argue linguistic device necessary conversational participant coordinate contribution dialogue agree mutual belief respect evolving plan example agree particular subgoal achieved final phenomenon concern shift control device achieve shift occur unusual single participant responsible coordinating achievement discourse plan different participant assumes control discourse subgoal control shift occurs participant mechanism achieving control framework distinguishes instance control",
        "shift negotiated participant instance participant seizes control paper objective explore phenomenon control relation attentional state grosz sidner grosz sidner predict shift attentional state shift control negotiated agreed participant control seized participant acceptance reflected different distribution anaphora case test prediction distribution control different type dialogue tod embody master slave assumption grosz sidner control allocated expert expectation control located participant tod contrast ad",
        "use framework allocation transfer control whittaker stenton analysis based classification utterance type assertion declarative utterance state fact response question classified assertion basis supplying information utterance intended instigate action imperative form indirect suggestion question utterance intended elicit information including indirect form wondering prompt utterance express propositional content yeah okay uh huh prompt direct contrast option participant available point discourse indicating speaker want floor prompt function number level including expression understanding agreement schegloff rule allocation control based utterance type classification allow dialogue divided segment correspond speaker controller segment definition controller seen correspond intuition term initiating conversational participant icp defined initiator given discourse segment grosz sidner conversational participant ocp speak utterance segment discourse segment purpose purpose icp control rule place segment boundary role participant icp ocp change example whittaker stenton performed post hoc analysis segment boundary defined control rule boundary fell type abdication repetition summary recommendation ensure integral set file abdication correspond case controller produce prompt utterance segment class repetition summary corresponds controller producing redundant utterance utterance exact repetition previous propositional content summary realizes proposition inferred came orderly control shift occur controller indicates wish relinquish control unifies abdication repetition summary controller supply new propositional content remaining class interruption characterize shift occurring noncontroller display initiative seizing control class general definition interruption contains cross speaker interruption involve topic shift similar true interruption grosz sidner clarification subdialogues sidner litman allen classification suggests transfer control collaborative phenomenon noncontroller ocp option seizing control juncture discourse controller icps control noncontroller allows observation address problem raised grosz sidner icps signal ocps recognize segment boundary claim shift control occur controller indicates end discourse segment abdicating producing repetition summary",
        "assertion declarative utterance state fact response question classified assertion basis supplying information utterance intended instigate action imperative form indirect suggestion question utterance intended elicit information including indirect form wondering prompt utterance express propositional content yeah okay uh huh prompt direct contrast option participant available point discourse indicating speaker want floor prompt function number level including expression understanding agreement schegloff rule allocation control based utterance type classification allow dialogue divided segment correspond speaker controller segment",
        "abdication repetition summary recommendation ensure integral set file abdication correspond case controller produce prompt utterance segment class repetition summary corresponds controller producing redundant utterance utterance exact repetition previous propositional content summary realizes proposition inferred came orderly control shift occur controller indicates wish relinquish control unifies abdication repetition summary controller supply new propositional content remaining class interruption characterize shift occurring noncontroller display initiative seizing control class general definition interruption contains cross speaker interruption involve topic shift similar true interruption grosz sidner clarification subdialogues sidner litman allen classification suggests transfer control collaborative phenomenon noncontroller ocp option seizing control juncture discourse controller icps control noncontroller allows observation address problem raised grosz sidner icps signal ocps recognize segment boundary claim shift control occur controller indicates end discourse segment abdicating producing repetition summary",
        "determine relationship derived control segment attentional state looked distribution anaphora respect control segment ad data analysed difference cited significant level looked anaphor excluding second person grouped class anaphor person hisone new somedeictic noun phrase npevent verb phrase sentence segment class deictic refers deictic reference material introduced noun phrase class event refers material introduced phenomenon noted anaphora distribution indicated segment related apparent case discourse participant interrupted passed control following example illustrates point control segment defined treat case composed different segment ignores fact utterance related propositional content example plural pronoun straddle central subsegment referent picked second example allowed hierarchical segment treating interruption subsegments utterance related part parent segment interruption treated embeddings way relationship segment interruption segment determined independent ground topic intentional structure extended control framework allow embedding interrupt coded anaphor respect antecedent lay current segment labelled cross segment boundary antecedent cross segment boundary figure addition break type control shift occurred previous segment boundary looked distribution anaphora support ad found similar result dialogue distribution anaphor varies according type control shift occurred previous segment boundary look different type anaphora find person anaphor cross boundary event anaphor deictic pronoun demonstrate different pattern mean fact anaphora cross segment boundary following interruption summary abdication consistent control principle summary abdication speaker give explicit signal wish relinquish control contrast interruption unprompted attempt listener seize control having problem controller utterance interruption likely topic deixis event anaphor behave anaphor deixis serf pick object selected use standard anaphora expect referent deixis immediate focus likely current segment webber picture complex event anaphora serve number different function dialogue talk past event lead current situation order place refer set proposition preceding discourse little background webber",
        "prevalent use refer future event action day task ad develop plan speaker use event anaphora concise reference plan negotiated discus status quality plan suggested frequent cross speaker reference future event action correspond phase plan negotiation pollack reference related control structure example illustrates clustering event anaphora segment boundary discourse participant us anaphor summarize plan participant evaluates plan control shift reference plan cross control boundary distribution event anaphora bear reference future action utterance segment boundary example instance event anaphora crossing segment boundary occurs speaker talking future event action looked tod instance anaphora describe future act way observed ad turn tod instance event anaphora main occasion necessary talk plan financial ad event anaphor utterance",
        "phenomenon noted anaphora distribution indicated segment related apparent case discourse participant interrupted passed control following example illustrates point control segment defined treat case composed different segment ignores fact utterance related propositional content example plural pronoun straddle central subsegment referent picked second example allowed hierarchical segment treating interruption subsegments utterance related part parent segment interruption treated embeddings way relationship segment interruption segment determined independent ground topic intentional structure",
        "extended control framework allow embedding interrupt coded anaphor respect antecedent lay current segment labelled cross segment boundary antecedent cross segment boundary figure addition break type control shift occurred previous segment boundary looked distribution anaphora support ad found similar result dialogue distribution anaphor varies according type control shift occurred previous segment boundary look different type anaphora find person anaphor cross boundary event anaphor deictic pronoun demonstrate different pattern mean fact anaphora cross segment boundary following interruption summary abdication consistent control principle summary abdication speaker give explicit signal wish relinquish control contrast interruption unprompted attempt listener seize control having problem controller utterance interruption likely topic deixis event anaphor behave anaphor deixis serf pick object selected use standard anaphora expect referent deixis immediate focus likely current segment webber picture complex event anaphora serve number different function dialogue talk past event lead current situation order place refer set proposition preceding discourse little background webber prevalent use refer future event action day task ad develop plan speaker use event anaphora concise reference plan negotiated discus status quality plan suggested frequent cross speaker reference future event action correspond phase plan negotiation pollack reference related control structure example illustrates clustering event anaphora segment boundary discourse participant us anaphor summarize plan participant evaluates plan control shift reference plan cross control boundary distribution event anaphora bear reference future action utterance segment boundary example instance event anaphora crossing segment boundary occurs speaker talking future event action looked tod instance anaphora describe future act way observed ad turn tod instance event anaphora main occasion necessary talk plan financial ad event anaphor utterance",
        "explore relationship control planning compare tod type ad financial support expect dialogue differ term initiative ad objective develop collaborative plan series conversational exchange discourse participant believe expert knowledge domain partial information situation believe advisee contribute problem description constraint problem solved information exchanged mutual belief necessary develop collaborative plan established conversation joshi situation different tod participant believe outset expert sufficient information situation complete correct knowledge execute task apprentice need assert information change expert belief ask question verify expert belief issue command expect apprentice control present execute action indicated knowledgeable participant difference belief knowledge state participant interpreted term collaborative planning principle whittaker stenton generalize principle information quality plan quality predict interrupt occur quality listener believe information speaker provided true unambiguous relevant mutual goal corresponds rule truth listener belief fact belief fact relevant belief speaker belief speaker know interrupt ambiguity listener belief speaker assertion relevant ambiguous interrupt quality listener believe action proposed speaker adequate plan achieve mutual goal action comprehensible listener rule express effectiveness listener belief belief present obstacle proposed plan belief proposed plan satisfied interrupt ambiguity listener belief assertion proposed plan ambiguous interrupt principle provide mean ensure mutual belief participant interrupt condition interrupt hold lack interruption signal discrepancy mutual belief discrepancy interruption necessary contribution collaborative plan distraction joint activity compare ad tod respect control exchanged calculating average number turn control shift investigate control shared participant percentage control shift represented abdication interrupt summary dialogue type figure thing striking data predicted distribution control expert client different ad tod expert control utterance tod control shared ad contrary expectation find instance shift tod distribution interruption summary differs dialogue type collaborative planning principle highlight difference observe reason shift occur tod interruption tod",
        "result apprentice seizing control indicate temporary problem plan execution delayed control exchanged execution task started awry problem physical situation indicates apprentice relevant belief shared instructor possession critical information current state apprentice pump necessitates information exchange resynchronize mutual belief rest plan executed control allocated instructor tod reason participant believe contribution fewer attempt instructor coordinate activity summary synchronize mutual belief apprentice need contribution interruption explaining interruption dialogue addition majority interruption initiated apprentice contrast ad produced client frequent ad ad participant believe plan constructed contribution summary device allow contribution coordinated participant use device set opportunity contribution ensure mutual belief increased frequency summary ad result fact participant start discrepant mutual belief situation establishing maintaining mutual belief key ad",
        "stated discourse collaborative process manifested certain phenomenon use anaphora cue word grosz sidner hirschberg litman cohen speaker make aspect discourse structure explicit found shift attentional state shift control negotiated agreed participant control seized participant acceptance reflected different distribution anaphora case found type anaphora behaved way anaphora clustered segment boundary refer preceding segment likely cross segment boundary function talking proposed plan found control distributed exchanged ad tod result provide support control rule analysis argued hierarchical organization control segment basis specific example interruption believe level structure discourse captured control rule control shift correspond task boundary topic shift change initiation change control topic shift whittaker stenton relationship cue word intonational contour pierrehumbert hirschberg use modal subordination robert segment derived control rule topic future research controversial question concern rhetorical relation extent detected listener grosz sidner applied coherence relation face face conversation mixed initiative displayed participant hobbs agar hobbs category rhetorical relation describes elaboration speaker repeat propositional content previous utterance difficulty determining function repetition maintain function follows general principle control rule speaker signal wish shift control supplying new propositional content repetition summary add new information function signal listener speaker listener recognize fact appear additional function synchronization allowing participant agree proposition believed point discussion work highlight aspect collaboration discourse integrated research collaborative planning grosz sidner cohen respect relation control shift coordination plan",
        "application natural language processing necessary determine likelihood given word combination example speech recognizer need determine word combination eat peach eat beach likely statistical nlp method determine likelihood word combination according frequency training corpus nature language word combination infrequent occur given corpus work propose method estimating probability unseen word combination available information similar word describe probabilistic word association model based distributional word similarity apply improving probability estimate unseen word bigram variant katz model similarity based method yield perplexity improvement prediction unseen bigram significant reduction speech recognition error",
        "data sparseness inherent problem statistical method natural language processing method use statistic relative frequency configuration element training corpus evaluate alternative analysis interpretation new sample text speech likely analysis taken contains frequent configuration problem data sparseness arises analysis contain configuration occurred training corpus possible estimate probability observed frequency estimation scheme focus particular kind configuration word cooccurrence cooccurrences include relationship head word syntactic construction verb object adjective noun example word sequence n gram model probability estimate unseen cooccurrence function probability estimate word cooccurrence example bigram model study probability conditioned word occurred training following conditioning word calculated probability estimated frequency corpus jelinek katz method depends independence assumption cooccurrence frequent higher estimate based similarity based model provide alternative independence assumption model relationship given word modeled analogy word sense similar given one suggest class based n gram model word similar cooccurrence distribution clustered word class cooccurrence probability given pair word estimated according averaged cooccurrence probability corresponding class propose soft clustering scheme certain grammatical cooccurrences membership word class probabilistic probability word modeled averaged cooccurrence probability word cluster argue reduction small number predetermined word class cluster cause substantial loss information similarity based model avoids clustering word modeled specific class set word similar nearest neighbor approach pattern recognition scheme predict unobserved cooccurrences likely model probabilistic provide probability estimate unobserved cooccurrences complete probabilistic framework n gram language model probabilistic lexicalized grammar schabes lafferty similarity based method estimating probability cooccurrences unseen training based estimation language modeling cooccurrence smoothing method essen steinbiss derived work acoustic model smoothing sugawara present different method take starting point scheme katz allocate appropriate probability mass unseen cooccurrences following method redistribute mass unseen cooccurrences according averaged cooccurrence distribution set similar conditioning word relative entropy similarity measure second step replaces use independence assumption original model applied method estimate unseen bigram probability wall street journal text compared standard model held sample similarity model achieved reduction perplexity unseen bigram constituted test sample leading overall reduction test set perplexity experimented application language modeling speech recognition yielded significant reduction recognition error remainder discussion presented term valid",
        "low probability bigram missing finite sample aggregate probability unseen bigram high new sample contain data sparseness use maximum likelihood estimator mle bigram probability mle probability bigram frequency training corpus total number bigram estimate probability unseen bigram undesirable proposal circumvent problem good jelinek katz church gale mle initial estimate adjust total probability seen bigram leaving probability mass unseen bigram adjustment involves interpolation new estimator weighted combination mle estimator guaranteed nonzero unseen bigram discounting mle decreased according model unreliability small frequency count leaving probability mass unseen bigram model katz provides clear separation frequent event observed frequency reliable probability estimator low frequency event prediction involve additional information source addition model require complex estimation interpolation parameter model requires method fordiscounting estimate observed event leave positive probability mass unseen event andredistributing unseen event probability mass freed discounting bigram resulting estimator general represents discounted estimate seen bigram model probability redistribution unseen bigram normalization factor overall mass left unseen bigram starting given bythe normalization factor required ensure second formulation normalization preferable total number possible bigram type exceeds number observed type modifies katz presentation include placeholder alternative model distribution unseen bigram us turing formula replace actual frequency bigram event general discounted frequency defined number different corpus frequency us discounted frequency conditional probability calculation bigram original good turing method good free probability mass redistributed unseen event katz scheme redistributes free probability mass non proportion frequency settingkatz assumes given conditioning word probability unseen following word proportional unconditional probability overall form model depend assumption investigate estimate derived averaging estimate conditional probability follows word similar",
        "scheme based assumption word similar provide good prediction distribution unseen bigram denote set word similar determined similarity metric define similarity based model conditional distribution weighted average conditional distribution word unnormalized weight given determined degree similarity scheme likely follow tends follow word similar complete scheme necessary define similarity metric pereira measure word similarity relative entropy kullback leibler distance corresponding conditional distribution distance increase distribution similar compute nonzero estimate necessary defined use estimate given standard model satisfy requirement application similarity model average standard estimate set similar conditioning word define set nearest word excluding satisfy parameter control content tuned defined asthe weight larger word similar closer parameter control relative contribution word different distance value increase nearest word weight decrease remote word larger effect tuned definition use scheme found smooth interpolating unigram probability recall katz linear interpolation getwhere determined interpolation parameter smoothing appears compensate inaccuracy infrequent conditioning word evaluation show good value small similarity based model play stronger role independence assumption summarize construct similarity based model interpolate interpolated model scheme obtain better estimate unseen bigram parameter tuned relevant process determine set similar word considered determines relative effect word determines overall importance similarity based model",
        "evaluated method comparing perplexity effect speech recognition accuracy baseline bigram model developed mit lincoln laboratory wall street journal wsj text dictation corpus provided arpa hlt program paul baseline model follows katz design compactness frequency bigram ignored count model obtained word wsj text year perplexity evaluation tuned similarity model parameter minimizing perplexity additional sample word wsj text drawn arpa hlt development test set best parameter value found value improvement perplexity unseen bigram held word sample bigram unseen improvement unseen bigram corresponds overall test set perplexity improvement show reduction training test perplexity sorted training reduction different choice number closest neighbor value best one found equation clear computational cost applying similarity model unseen bigram lower value preferable table reducing incurs penalty perplexity improvement low value appear sufficient achieve benefit similarity model table show best value increase decrease greater weight given conditioned word frequency suggests predictive power neighbor closest modeled overall frequency conditioned word bigram similarity model tested language model speech recognition test data experiment pruned word lattice wsj closed vocabulary test sentence score lattice sum acoustic score negative log likelihood language model score case negative log probability provided baseline bigram model given lattice constructed new lattice arc score modified use similarity model baseline model compared best sentence hypothesis original lattice modified counted word disagreement hypothesis correct total disagreement similarity model correct case model advantage similarity model significant level overall reduction error rate small number disagreement small compared overall number error current recognition setup show example speech recognition disagreement model hypothesis labeled similarity bold face word error similarity model able model better regularity semantic parallelism list avoiding tense form hand similarity model make mistake function word inserted place punctuation found written text",
        "cooccurrence smoothing technique essen steinbiss based earlier stochastic speech modeling work sugawara main previous attempt use similarity estimate probability unseen event language modeling addition original use language modeling speech recognition grishman sterling applied cooccurrence smoothing technique estimate likelihood selectional pattern outline main parallel difference method cooccurrence smoothing detailed analysis require empirical comparison method corpus task cooccurrence smoothing method baseline model combined similarity based model refines probability estimate similarity model cooccurrence smoothing based intuition similarity word measured confusion probability substituted arbitrary context training corpus baseline probability model taken mle confusion probability conditioning word defined asthe probability followed context word bigram estimate derived cooccurrence smoothing given bynotice formula form similarity model us confusion probability use normalized weight addition restrict summation similar word cooccurrence smoothing method sum word lexicon similarity measure symmetric sense identical frequency normalization contrast asymmetric weighs context proportion probability occurrence way comparable frequency sharper context distribution greater similarity model play stronger role estimating property motivated choice relative entropy similarity measure intuition word sharper distribution informative word word flat distribution similarity model missing bigram scheme essen steinbiss linear interpolation bigram combine cooccurrence smoothing model mle model bigram unigrams choice interpolation independent similarity model",
        "model provides basic scheme probabilistic similarity based estimation developed direction variation tried different similarity metric different weighting scheme simplification current model parameter possible respect parameter select nearest neighbor word substantial variation base model similarity conditioned word similarity conditioning word evidence combined similarity based estimate instance weigh estimate measure reliability similarity metric neighbor distribution second possibility account negative evidence frequent followed statistical evidence bound estimate require adjustment similarity based estimate line rosenfeld huang similarity based estimate smooth maximum likelihood estimate small nonzero frequency similarity based estimate high bigram receive higher estimate predicted uniform discounting method similarity based model applied configuration bigram trigram necessary measure similarity different conditioning measuring distance distribution form corresponding different bigram define similarity measure bigram function similarity corresponding word type conditional cooccurrence probability probabilistic parsing black configuration question includes word possible use model bigram configuration includes element necessary adjust method line discussed trigram",
        "similarity based model suggest appealing approach dealing data sparseness corpus statistic provide analogy word agree linguistic domain intuition paper presented new model implement similarity based approach provide estimate conditional probability unseen word cooccurrences method combine similarity based estimate katz scheme language modeling speech recognition scheme proposed preferred way implementing independence assumption suggest appropriate implementing similarity based model class based model enables rely direct maximum likelihood estimate reliable statistic available resort estimate indirect model improvement achieved bigram model significant modest overall effect small proportion unseen event bigram accessible platform develop test model substantial improvement obtainable informative configuration obvious case trigram sparse data problem severe longer term goal apply similarity technique motivated word cooccurrence configuration suggested lexicalized approach parsing schabes lafferty configuration verb object adjective noun evidence pereira sharper word cooccurrence distribution obtainable leading improved prediction similarity technique"
    ],
    "id": [
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        1,
        1,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        3,
        3,
        3,
        3,
        3,
        3,
        3,
        3,
        4,
        4,
        4,
        4,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        6,
        6,
        6,
        6,
        7,
        7,
        7,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        9,
        10,
        10,
        10,
        10,
        10,
        11,
        11,
        11,
        11,
        11,
        11,
        11,
        11,
        11,
        11,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        13,
        13,
        13,
        13,
        13,
        13,
        13,
        14,
        14,
        14,
        14,
        14,
        14,
        14,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        15,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        17,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        18,
        19,
        19,
        19,
        19,
        19,
        19,
        19,
        19,
        19,
        19,
        20,
        20,
        20,
        20,
        20,
        20,
        20,
        20,
        20,
        20,
        21,
        21,
        21,
        21,
        21,
        21,
        21,
        21,
        21,
        21,
        21,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        22,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        23,
        24,
        24,
        24,
        24,
        24,
        24,
        24,
        24,
        24,
        24,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        25,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        26,
        27,
        27,
        27,
        27,
        27,
        27,
        27,
        27,
        27,
        27,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        28,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        29,
        30,
        30,
        30,
        30,
        30,
        30,
        30,
        30,
        30,
        30,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        31,
        32,
        32,
        32,
        32,
        32,
        32,
        32,
        32,
        32,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        33,
        34,
        34,
        34,
        34,
        34,
        34,
        34,
        34,
        34,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        35,
        36,
        36,
        36,
        36,
        36,
        36,
        37,
        37,
        37,
        37,
        37,
        37,
        37,
        37,
        37,
        37,
        38,
        38,
        38,
        38,
        38,
        38,
        38,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        39,
        40,
        40,
        40,
        40,
        40,
        40,
        40,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        41,
        42,
        42,
        42,
        42,
        42,
        42,
        43,
        43,
        43,
        43,
        43,
        43,
        43,
        43,
        43,
        43,
        43,
        44,
        44,
        44,
        44,
        44,
        44,
        44,
        45,
        45,
        45,
        45,
        45,
        45,
        45,
        46,
        46,
        46,
        46,
        46,
        46,
        46,
        46,
        46,
        46,
        46,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        47,
        48,
        48,
        48,
        48,
        48,
        48,
        49,
        49,
        49,
        49,
        49,
        49,
        49,
        49,
        49,
        49,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        50,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        51,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        52,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        53,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        54,
        55,
        55,
        55,
        55,
        55,
        55,
        55,
        55,
        55,
        55,
        55,
        56,
        56,
        56,
        56,
        56,
        56,
        56,
        56,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        57,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        58,
        59,
        59,
        59,
        59,
        59,
        59,
        59,
        59,
        60,
        60,
        60,
        60,
        60,
        60,
        60,
        60,
        60,
        60,
        60,
        61,
        61,
        61,
        61,
        61,
        61,
        61,
        61,
        61,
        61,
        61,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        62,
        63,
        63,
        63,
        63,
        63,
        63,
        63,
        63,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        64,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        65,
        66,
        66,
        66,
        66,
        66,
        66,
        66,
        66,
        67,
        67,
        67,
        67,
        67,
        67,
        67,
        67,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        68,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        69,
        70,
        70,
        70,
        70,
        70,
        70,
        70,
        71,
        71,
        71,
        71,
        71,
        71,
        71,
        71,
        72,
        72,
        72,
        72,
        72,
        73,
        73,
        73,
        73,
        73,
        73,
        73,
        73,
        73,
        73,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        74,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        75,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        76,
        77,
        77,
        77,
        77,
        77,
        77,
        77,
        77
    ],
    "time": [
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1987,
        1987,
        1987,
        1987,
        1987,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1989,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1988,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1992,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1993,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1996,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1995,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1990,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994,
        1994
    ],
    "class": [
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample",
        "core_sample"
    ]
}